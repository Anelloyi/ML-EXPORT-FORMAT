{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-08 22:17:40.571141: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "scikit-learn version 1.3.2 is not supported. Minimum required version: 0.17. Maximum required version: 1.1.2. Disabling scikit-learn conversion API.\n",
      "XGBoost version 2.1.0 has not been tested with coremltools. You may run into unexpected errors. XGBoost 1.4.2 is the most recent version that has been tested.\n",
      "Torch version 2.2.2 has not been tested with coremltools. You may run into unexpected errors. Torch 2.2.0 is the most recent version that has been tested.\n",
      "/opt/anaconda3/envs/env_name/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import time\n",
    "import subprocess\n",
    "import os\n",
    "import re\n",
    "import psutil\n",
    "import torch\n",
    "import tempfile\n",
    "import tf2onnx\n",
    "import tensorflow as tf\n",
    "import coremltools as ct\n",
    "import coremltools as ct\n",
    "import onnx\n",
    "import numpy as np\n",
    "import onnxruntime as ort\n",
    "\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载 MNIST 数据集\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "# 调整输入数据的形状和范围\n",
    "X_train = X_train.reshape((X_train.shape[0], 28, 28, 1))\n",
    "X_test = X_test.reshape((X_test.shape[0], 28, 28, 1))\n",
    "X_train = X_train.astype('float32') / 255\n",
    "X_test = X_test.astype('float32') / 255\n",
    "# 将标签转换为 one-hot 编码\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 创建 CNN 模型\n",
    "model = Sequential([\n",
    "    Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# 编译模型\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "# 训练模型\n",
    "model.fit(X_train, y_train, batch_size=128, epochs=2, verbose=1, validation_data=(X_test, y_test))\n",
    "# 评估模型\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "\n",
    "#model.save('./mnist_models/mnist_cnn_model.h5')\n",
    "model.save('./models_train/mnist_cnn_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model...\n",
      "Subprocess started.\n",
      "Time taken to save TensorFlow SavedModel: 0.2993950843811035 seconds\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "Time taken for inference on 100 samples 1 times: 0.3005 seconds\n",
      "Subprocess finished.\n",
      "Resource monitoring finished.\n",
      "Content saved to ./mnist_models/output_cnn/output-mnist-cnn-h5.txt\n",
      "0\n",
      "[]\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 137\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[39mif\u001b[39;00m match:\n\u001b[1;32m    135\u001b[0m         numbers\u001b[39m.\u001b[39mappend(\u001b[39mfloat\u001b[39m(match\u001b[39m.\u001b[39mgroup())) \n\u001b[0;32m--> 137\u001b[0m delta_time \u001b[39m=\u001b[39m duration \u001b[39m*\u001b[39;49m \u001b[39m2\u001b[39;49m \u001b[39m/\u001b[39;49m filtered_lines_count\n\u001b[1;32m    138\u001b[0m \u001b[39m# print(delta_time)\u001b[39;00m\n\u001b[1;32m    140\u001b[0m numbers_scaled \u001b[39m=\u001b[39m [num \u001b[39m*\u001b[39m delta_time \u001b[39mfor\u001b[39;00m num \u001b[39min\u001b[39;00m numbers]\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: float division by zero"
     ]
    }
   ],
   "source": [
    "thread_output = {}\n",
    "duration = 0\n",
    "# 定义监控保存模型时的资源使用率的线程函数\n",
    "def monitor_resources_during_save(stop_event):\n",
    "    cpu_usage = []\n",
    "    gpu_usage = []\n",
    "\n",
    "    while not stop_event.is_set():\n",
    "        cpu_usage.append(psutil.cpu_percent(interval=0.1))\n",
    "        try:\n",
    "            gpu_output = subprocess.check_output(['nvidia-smi', \n",
    "                                                  '--query-gpu=utilization.gpu', '--format=csv,noheader,nounits'])\n",
    "            gpu_usage.append(int(gpu_output.strip()))\n",
    "        except Exception as e:\n",
    "            gpu_usage.append(None)  # 如果没有GPU或nvidia-smi命令失败，则记录None\n",
    "        # time.sleep(0.1)\n",
    "\n",
    "    # 保存监测结果\n",
    "    thread_output['cpu_usage'] = cpu_usage\n",
    "    thread_output['gpu_usage'] = gpu_usage\n",
    "    print(\"Resource monitoring finished.\")\n",
    "\n",
    "# 定义保存模型的函数\n",
    "def save_model(stop_event, model):\n",
    "    global duration\n",
    "    global inference_duration\n",
    "    print(\"Saving model...\")\n",
    "    \n",
    "    # 测量保存模型的时间\n",
    "    start_time = time.time()\n",
    "    # 保存 Keras 模型为 HDF5 格式\n",
    "    for i in range(0, 1):\n",
    "        model.save(\"mnist_cnn_model.h5\")\n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    print(f'Time taken to save TensorFlow SavedModel: {duration} seconds')\n",
    "\n",
    "    # 测量推理时间\n",
    "    X_test_sample = X_test[:100]  # 选择前10个样本进行推理batch_size=10000\n",
    "    start_time_inference = time.time()\n",
    "    n_time = 1 # 计数器\n",
    "    for i in range(n_time): # 重复操作n遍\n",
    "        model.predict(X_test_sample)\n",
    "    end_time_inference = time.time()\n",
    "    inference_duration = end_time_inference - start_time_inference\n",
    "    print(f'Time taken for inference on {len(X_test_sample)} samples {n_time} times: {inference_duration:.4f} seconds')\n",
    "\n",
    "    stop_event.set()  # 触发停止其他线程\n",
    "\n",
    "# 运行外部脚本并捕获输出\n",
    "def run_script(stop_event):\n",
    "    try:\n",
    "        # 使用临时文件来存储子进程输出\n",
    "        with tempfile.NamedTemporaryFile(delete=False) as tmp_file:\n",
    "            process = subprocess.Popen(['/Users/anelloyi/Desktop/run_powermetrics.sh'], \n",
    "                                       stdout=tmp_file, stderr=subprocess.STDOUT, text=True)\n",
    "            print(\"Subprocess started.\")\n",
    "            while not stop_event.is_set():\n",
    "                if process.poll() is not None:  # 检查进程是否已经结束\n",
    "                    break\n",
    "                # time.sleep(0.1)\n",
    "            \n",
    "            if process.poll() is None:\n",
    "                # 尝试终止进程\n",
    "                process.terminate()\n",
    "                try:\n",
    "                    process.wait(timeout=0.1)\n",
    "                except subprocess.TimeoutExpired:\n",
    "                    process.kill()  # 如果超时，则强制终止\n",
    "                    process.wait()\n",
    "        \n",
    "        # 读取临时文件的内容\n",
    "        with open(tmp_file.name, 'r') as f:\n",
    "            thread_output['powermetrics'] = f.read()\n",
    "        \n",
    "        os.remove(tmp_file.name)  # 删除临时文件\n",
    "        print(\"Subprocess finished.\")\n",
    "    except Exception as e:\n",
    "        thread_output['powermetrics'] = str(e)\n",
    "        print(\"Exception in subprocess:\", str(e))\n",
    "\n",
    "# 载入数据和模型\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X_train = X_train.reshape((X_train.shape[0], 28, 28, 1))\n",
    "X_test = X_test.reshape((X_test.shape[0], 28, 28, 1))\n",
    "X_train = X_train.astype('float32') / 255\n",
    "X_test = X_test.astype('float32') / 255\n",
    "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "stop_event = threading.Event()\n",
    "model = tf.keras.models.load_model(\"./mnist_models/mnist_cnn_model.h5\")\n",
    "\n",
    "# 创建和启动线程\n",
    "thread1 = threading.Thread(target=save_model, args=(stop_event, model))\n",
    "thread2 = threading.Thread(target=monitor_resources_during_save, args=(stop_event,))\n",
    "thread3 = threading.Thread(target=run_script, args=(stop_event,))\n",
    "thread3.start()\n",
    "thread1.start()\n",
    "thread2.start()\n",
    "# 等待线程完成\n",
    "thread1.join()\n",
    "thread2.join()\n",
    "thread3.join()\n",
    "\n",
    "# 输出从线程收集的数据\n",
    "content = thread_output.get('powermetrics', 'No output captured')\n",
    "# 将内容按行拆分\n",
    "lines = content.split('\\n')\n",
    "# 筛选出以 \"CPU consume\" 开头的行\n",
    "filtered_lines = [line for line in lines if line.startswith('GPU Power:') or \n",
    "                  line.startswith('CPU Power:')] # or\n",
    "                  # line.startswith('Combined Power (CPU + GPU + ANE):')]\n",
    "# 将筛选后的行合并为一个字符串，每行之间用换行符分隔\n",
    "filtered_content = '\\n'.join(filtered_lines)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "file_name = f'./mnist_models/output_cnn/output-mnist-cnn-h5.txt'\n",
    "# ----------------------------------------------------------------------------\n",
    "with open(file_name, 'w') as file:\n",
    "    file.write(filtered_content)\n",
    "    file.write(f'\\nTotal Duration(s): {duration:.2f}')\n",
    "    file.write(f'\\nInference Duration(s): {inference_duration:.4f}')\n",
    "print(f\"Content saved to {file_name}\")\n",
    "\n",
    "filtered_lines_count = len(filtered_lines)\n",
    "print(filtered_lines_count)\n",
    "print(filtered_lines)\n",
    "\n",
    "# 提取每一个采样点的数字，即CPU和GPU的具体mV\n",
    "numbers = []\n",
    "for line in filtered_lines:\n",
    "    match = re.search(r'[\\d.]+', line)\n",
    "    if match:\n",
    "        numbers.append(float(match.group())) \n",
    "\n",
    "delta_time = duration * 2 / filtered_lines_count\n",
    "# print(delta_time)\n",
    "\n",
    "numbers_scaled = [num * delta_time for num in numbers]\n",
    "total_energy_consumption = sum(numbers_scaled)\n",
    "print(f\"Total energy consumption: {total_energy_consumption} mV\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model...\n",
      "Subprocess started.\n",
      "Time taken to save TensorFlow SavedModel: 0.05698871612548828 seconds\n",
      "4/4 [==============================] - 0s 16ms/step\n",
      "4/4 [==============================] - 0s 14ms/step\n",
      "4/4 [==============================] - 0s 7ms/step\n",
      "4/4 [==============================] - 0s 7ms/step\n",
      "4/4 [==============================] - 0s 7ms/step\n",
      "4/4 [==============================] - 0s 14ms/step\n",
      "4/4 [==============================] - 0s 6ms/step\n",
      "4/4 [==============================] - 0s 6ms/step\n",
      "4/4 [==============================] - 0s 6ms/step\n",
      "4/4 [==============================] - 0s 15ms/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "4/4 [==============================] - 0s 6ms/step\n",
      "4/4 [==============================] - 0s 14ms/step\n",
      "4/4 [==============================] - 0s 6ms/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "4/4 [==============================] - 0s 14ms/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "4/4 [==============================] - 0s 6ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 6ms/step\n",
      "4/4 [==============================] - 0s 6ms/step\n",
      "4/4 [==============================] - 0s 10ms/step\n",
      "4/4 [==============================] - 0s 6ms/step\n",
      "4/4 [==============================] - 0s 7ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "4/4 [==============================] - 0s 7ms/step\n",
      "4/4 [==============================] - 0s 15ms/step\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "4/4 [==============================] - 0s 6ms/step\n",
      "4/4 [==============================] - 0s 19ms/step\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "4/4 [==============================] - 0s 7ms/step\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "4/4 [==============================] - 0s 6ms/step\n",
      "4/4 [==============================] - 0s 6ms/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "4/4 [==============================] - 0s 6ms/step\n",
      "4/4 [==============================] - 0s 6ms/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "4/4 [==============================] - 0s 6ms/step\n",
      "4/4 [==============================] - 0s 6ms/step\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "4/4 [==============================] - 0s 6ms/step\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "4/4 [==============================] - 0s 6ms/step\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "4/4 [==============================] - 0s 6ms/step\n",
      "4/4 [==============================] - 0s 6ms/step\n",
      "4/4 [==============================] - 0s 6ms/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "4/4 [==============================] - 0s 6ms/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "4/4 [==============================] - 0s 6ms/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "4/4 [==============================] - 0s 16ms/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "4/4 [==============================] - 0s 6ms/step\n",
      "4/4 [==============================] - 0s 6ms/step\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "4/4 [==============================] - 0s 6ms/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "4/4 [==============================] - 0s 14ms/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "4/4 [==============================] - 0s 6ms/step\n",
      "4/4 [==============================] - 0s 16ms/step\n",
      "4/4 [==============================] - 0s 5ms/step\n",
      "4/4 [==============================] - 0s 6ms/step\n",
      "4/4 [==============================] - 0s 6ms/step\n",
      "4/4 [==============================] - 0s 15ms/step\n",
      "4/4 [==============================] - 0s 4ms/step\n",
      "Time taken for inference on 10000 samples 1 times: 7.1372 seconds\n",
      "Subprocess finished.\n",
      "Resource monitoring finished.\n",
      "Content saved to ./mnist_models/output_cnn/output-mnist-cnn-h5.txt\n",
      "Total energy consumption: 65512.37684197979 mV\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import threading\n",
    "import psutil\n",
    "import subprocess\n",
    "import tempfile\n",
    "import os\n",
    "import re\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "thread_output = {}\n",
    "duration = 0\n",
    "inference_duration = 0\n",
    "\n",
    "# 定义监控保存模型时的资源使用率的线程函数\n",
    "def monitor_resources_during_save(stop_event):\n",
    "    cpu_usage = []\n",
    "    gpu_usage = []\n",
    "\n",
    "    while not stop_event.is_set():\n",
    "        cpu_usage.append(psutil.cpu_percent(interval=0.1))\n",
    "        try:\n",
    "            gpu_output = subprocess.check_output(['nvidia-smi', \n",
    "                                                  '--query-gpu=utilization.gpu', '--format=csv,noheader,nounits'])\n",
    "            gpu_usage.append(int(gpu_output.strip()))\n",
    "        except Exception as e:\n",
    "            gpu_usage.append(None)  # 如果没有GPU或nvidia-smi命令失败，则记录None\n",
    "\n",
    "    # 保存监测结果\n",
    "    thread_output['cpu_usage'] = cpu_usage\n",
    "    thread_output['gpu_usage'] = gpu_usage\n",
    "    print(\"Resource monitoring finished.\")\n",
    "\n",
    "# 定义保存模型的函数\n",
    "def save_model(stop_event, model):\n",
    "    global duration\n",
    "    global inference_duration\n",
    "    print(\"Saving model...\")\n",
    "    \n",
    "    # 测量保存模型的时间\n",
    "    start_time = time.time()\n",
    "    # 保存 Keras 模型为 HDF5 格式\n",
    "    for i in range(1):  # 修改为2次保存，以提高可靠性\n",
    "        model.save(\"mnist_cnn_model.h5\")\n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    print(f'Time taken to save TensorFlow SavedModel: {duration} seconds')\n",
    "\n",
    "    # 测量推理时间\n",
    "    X_test_sample = X_test[:10000]  # 选择前10000个样本进行推理\n",
    "    batch_size = 128 # 批量大小 32 16 8 4 \n",
    "    num_batches = len(X_test_sample) // batch_size  # 计算批次数\n",
    "\n",
    "    start_time_inference = time.time()\n",
    "\n",
    "    # 批量推理\n",
    "    for _ in range(1):  # 重复1次进行批量推理\n",
    "        for batch_idx in range(num_batches):\n",
    "            batch_start = batch_idx * batch_size\n",
    "            batch_end = (batch_idx + 1) * batch_size\n",
    "            batch = X_test_sample[batch_start:batch_end]\n",
    "            \n",
    "            # 扩展批次数据的维度\n",
    "            batch_reshaped = batch\n",
    "            \n",
    "            model.predict(batch_reshaped)\n",
    "    \n",
    "    end_time_inference = time.time()\n",
    "    inference_duration = end_time_inference - start_time_inference\n",
    "    print(f'Time taken for inference on {len(X_test_sample)} samples {1} times: {inference_duration:.4f} seconds')\n",
    "\n",
    "    stop_event.set()  # 触发停止其他线程\n",
    "\n",
    "# 运行外部脚本并捕获输出\n",
    "def run_script(stop_event):\n",
    "    try:\n",
    "        # 使用临时文件来存储子进程输出\n",
    "        with tempfile.NamedTemporaryFile(delete=False) as tmp_file:\n",
    "            process = subprocess.Popen(['/Users/anelloyi/Desktop/run_powermetrics.sh'], \n",
    "                                       stdout=tmp_file, stderr=subprocess.STDOUT, text=True)\n",
    "            print(\"Subprocess started.\")\n",
    "            while not stop_event.is_set():\n",
    "                if process.poll() is not None:  # 检查进程是否已经结束\n",
    "                    break\n",
    "            \n",
    "            if process.poll() is None:\n",
    "                # 尝试终止进程\n",
    "                process.terminate()\n",
    "                try:\n",
    "                    process.wait(timeout=0.1)\n",
    "                except subprocess.TimeoutExpired:\n",
    "                    process.kill()  # 如果超时，则强制终止\n",
    "                    process.wait()\n",
    "        \n",
    "        # 读取临时文件的内容\n",
    "        with open(tmp_file.name, 'r') as f:\n",
    "            thread_output['powermetrics'] = f.read()\n",
    "        \n",
    "        os.remove(tmp_file.name)  # 删除临时文件\n",
    "        print(\"Subprocess finished.\")\n",
    "    except Exception as e:\n",
    "        thread_output['powermetrics'] = str(e)\n",
    "        print(\"Exception in subprocess:\", str(e))\n",
    "\n",
    "# # 载入数据和模型\n",
    "# (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "# X_train = X_train.reshape((X_train.shape[0], 28, 28, 1))\n",
    "# X_test = X_test.reshape((X_test.shape[0], 28, 28, 1))\n",
    "# X_train = X_train.astype('float32') / 255\n",
    "# X_test = X_test.astype('float32') / 255\n",
    "# y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
    "# y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "stop_event = threading.Event()\n",
    "model = tf.keras.models.load_model(\"./mnist_models/mnist_cnn_model.h5\")\n",
    "\n",
    "# 创建和启动线程\n",
    "thread1 = threading.Thread(target=save_model, args=(stop_event, model))\n",
    "thread2 = threading.Thread(target=monitor_resources_during_save, args=(stop_event,))\n",
    "thread3 = threading.Thread(target=run_script, args=(stop_event,))\n",
    "thread1.start()\n",
    "thread2.start()\n",
    "thread3.start()\n",
    "\n",
    "# 等待线程完成\n",
    "thread1.join()\n",
    "thread2.join()\n",
    "thread3.join()\n",
    "\n",
    "# 输出从线程收集的数据\n",
    "content = thread_output.get('powermetrics', 'No output captured')\n",
    "# 将内容按行拆分\n",
    "lines = content.split('\\n')\n",
    "# 筛选出以 \"GPU Power:\" 或 \"CPU Power:\" 开头的行\n",
    "filtered_lines = [line for line in lines if line.startswith('GPU Power:') or \n",
    "                  line.startswith('CPU Power:')] \n",
    "\n",
    "# 将筛选后的行合并为一个字符串，每行之间用换行符分隔\n",
    "filtered_content = '\\n'.join(filtered_lines)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "file_name = f'./mnist_models/output_cnn/output-mnist-cnn-h5.txt'\n",
    "# ----------------------------------------------------------------------------\n",
    "with open(file_name, 'w') as file:\n",
    "    file.write(filtered_content)\n",
    "    file.write(f'\\nTotal Duration(s): {duration:.2f}')\n",
    "    file.write(f'\\nInference Duration(s): {inference_duration:.4f}')\n",
    "print(f\"Content saved to {file_name}\")\n",
    "\n",
    "filtered_lines_count = len(filtered_lines)\n",
    "#print(filtered_lines_count)\n",
    "#print(filtered_lines)\n",
    "\n",
    "duration = inference_duration\n",
    "# 提取每一个采样点的数字，即CPU和GPU的具体mV\n",
    "numbers = []\n",
    "for line in filtered_lines:\n",
    "    match = re.search(r'[\\d.]+', line)\n",
    "    if match:\n",
    "        numbers.append(float(match.group())) \n",
    "\n",
    "delta_time = duration * 2 / filtered_lines_count\n",
    "\n",
    "numbers_scaled = [num * delta_time for num in numbers]\n",
    "total_energy_consumption = sum(numbers_scaled)\n",
    "print(f\"Total energy consumption: {total_energy_consumption} mV\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cnn H5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model...\n",
      "Subprocess started.\n",
      "Time taken to save TensorFlow SavedModel: 0.1022939682006836 seconds\n",
      "1/1 [==============================] - 0s 184ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "Time taken for inference on 1000 samples 1 times: 69.7560 seconds\n",
      "Subprocess finished.\n",
      "Resource monitoring finished.\n",
      "Content saved to ./mnist_models/output_cnn/output-mnist-cnn-h5.txt\n",
      "Total energy consumption: 633522.4598342732 mV\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import threading\n",
    "import psutil\n",
    "import subprocess\n",
    "import tempfile\n",
    "import os\n",
    "import re\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "thread_output = {}\n",
    "duration = 0\n",
    "inference_duration = 0\n",
    "\n",
    "# 定义监控保存模型时的资源使用率的线程函数\n",
    "def monitor_resources_during_save(stop_event):\n",
    "    cpu_usage = []\n",
    "    gpu_usage = []\n",
    "\n",
    "    while not stop_event.is_set():\n",
    "        cpu_usage.append(psutil.cpu_percent(interval=0.1))\n",
    "        try:\n",
    "            gpu_output = subprocess.check_output(['nvidia-smi', \n",
    "                                                  '--query-gpu=utilization.gpu', '--format=csv,noheader,nounits'])\n",
    "            gpu_usage.append(int(gpu_output.strip()))\n",
    "        except Exception as e:\n",
    "            gpu_usage.append(None)  # 如果没有GPU或nvidia-smi命令失败，则记录None\n",
    "\n",
    "    # 保存监测结果\n",
    "    thread_output['cpu_usage'] = cpu_usage\n",
    "    thread_output['gpu_usage'] = gpu_usage\n",
    "    print(\"Resource monitoring finished.\")\n",
    "\n",
    "# 定义保存模型的函数\n",
    "def save_model(stop_event, model):\n",
    "    global duration\n",
    "    global inference_duration\n",
    "    print(\"Saving model...\")\n",
    "    \n",
    "    # 测量保存模型的时间\n",
    "    start_time = time.time()\n",
    "    # 保存 Keras 模型为 HDF5 格式\n",
    "    for i in range(1):  # 修改为2次保存，以提高可靠性\n",
    "        model.save(\"mnist_cnn_model.h5\")\n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    print(f'Time taken to save TensorFlow SavedModel: {duration} seconds')\n",
    "\n",
    "    # 测量推理时间\n",
    "    X_test_sample = X_test[:1000]  # 选择前10000个样本进行推理\n",
    "    batch_size = 1  # 批量大小\n",
    "    num_batches = len(X_test_sample) // batch_size  # 计算批次数\n",
    "\n",
    "    start_time_inference = time.time()\n",
    "\n",
    "    # 批量推理\n",
    "    for _ in range(1):  # 重复32次进行批量推理\n",
    "        for batch_idx in range(num_batches):\n",
    "            batch_start = batch_idx * batch_size\n",
    "            batch_end = (batch_idx + 1) * batch_size\n",
    "            batch = X_test_sample[batch_start:batch_end]\n",
    "            \n",
    "            # 扩展批次数据的维度\n",
    "            batch_reshaped = batch\n",
    "            \n",
    "            model.predict(batch_reshaped)\n",
    "    \n",
    "    end_time_inference = time.time()\n",
    "    inference_duration = end_time_inference - start_time_inference\n",
    "    print(f'Time taken for inference on {len(X_test_sample)} samples {1} times: {inference_duration:.4f} seconds')\n",
    "\n",
    "    stop_event.set()  # 触发停止其他线程\n",
    "\n",
    "# 运行外部脚本并捕获输出\n",
    "def run_script(stop_event):\n",
    "    try:\n",
    "        # 使用临时文件来存储子进程输出\n",
    "        with tempfile.NamedTemporaryFile(delete=False) as tmp_file:\n",
    "            process = subprocess.Popen(['/Users/anelloyi/Desktop/run_powermetrics.sh'], \n",
    "                                       stdout=tmp_file, stderr=subprocess.STDOUT, text=True)\n",
    "            print(\"Subprocess started.\")\n",
    "            while not stop_event.is_set():\n",
    "                if process.poll() is not None:  # 检查进程是否已经结束\n",
    "                    break\n",
    "            \n",
    "            if process.poll() is None:\n",
    "                # 尝试终止进程\n",
    "                process.terminate()\n",
    "                try:\n",
    "                    process.wait(timeout=0.1)\n",
    "                except subprocess.TimeoutExpired:\n",
    "                    process.kill()  # 如果超时，则强制终止\n",
    "                    process.wait()\n",
    "        \n",
    "        # 读取临时文件的内容\n",
    "        with open(tmp_file.name, 'r') as f:\n",
    "            thread_output['powermetrics'] = f.read()\n",
    "        \n",
    "        os.remove(tmp_file.name)  # 删除临时文件\n",
    "        print(\"Subprocess finished.\")\n",
    "    except Exception as e:\n",
    "        thread_output['powermetrics'] = str(e)\n",
    "        print(\"Exception in subprocess:\", str(e))\n",
    "\n",
    "# # 载入数据和模型\n",
    "# (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "# X_train = X_train.reshape((X_train.shape[0], 28, 28, 1))\n",
    "# X_test = X_test.reshape((X_test.shape[0], 28, 28, 1))\n",
    "# X_train = X_train.astype('float32') / 255\n",
    "# X_test = X_test.astype('float32') / 255\n",
    "# y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
    "# y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "stop_event = threading.Event()\n",
    "model = tf.keras.models.load_model(\"./mnist_models/mnist_cnn_model.h5\")\n",
    "\n",
    "# 创建和启动线程\n",
    "thread1 = threading.Thread(target=save_model, args=(stop_event, model))\n",
    "thread2 = threading.Thread(target=monitor_resources_during_save, args=(stop_event,))\n",
    "thread3 = threading.Thread(target=run_script, args=(stop_event,))\n",
    "thread1.start()\n",
    "thread2.start()\n",
    "thread3.start()\n",
    "\n",
    "# 等待线程完成\n",
    "thread1.join()\n",
    "thread2.join()\n",
    "thread3.join()\n",
    "\n",
    "# 输出从线程收集的数据\n",
    "content = thread_output.get('powermetrics', 'No output captured')\n",
    "# 将内容按行拆分\n",
    "lines = content.split('\\n')\n",
    "# 筛选出以 \"GPU Power:\" 或 \"CPU Power:\" 开头的行\n",
    "filtered_lines = [line for line in lines if line.startswith('GPU Power:') or \n",
    "                  line.startswith('CPU Power:')] \n",
    "\n",
    "# 将筛选后的行合并为一个字符串，每行之间用换行符分隔\n",
    "filtered_content = '\\n'.join(filtered_lines)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "file_name = f'./mnist_models/output_cnn/output-mnist-cnn-h5.txt'\n",
    "# ----------------------------------------------------------------------------\n",
    "with open(file_name, 'w') as file:\n",
    "    file.write(filtered_content)\n",
    "    file.write(f'\\nTotal Duration(s): {duration:.2f}')\n",
    "    file.write(f'\\nInference Duration(s): {inference_duration:.4f}')\n",
    "print(f\"Content saved to {file_name}\")\n",
    "\n",
    "filtered_lines_count = len(filtered_lines)\n",
    "# print(filtered_lines_count)\n",
    "# print(filtered_lines)\n",
    "\n",
    "duration = inference_duration\n",
    "# 提取每一个采样点的数字，即CPU和GPU的具体mV\n",
    "numbers = []\n",
    "for line in filtered_lines:\n",
    "    match = re.search(r'[\\d.]+', line)\n",
    "    if match:\n",
    "        numbers.append(float(match.group())) \n",
    "\n",
    "delta_time = duration * 2 / filtered_lines_count\n",
    "\n",
    "numbers_scaled = [num * delta_time for num in numbers]\n",
    "total_energy_consumption = sum(numbers_scaled)\n",
    "print(f\"Total energy consumption: {total_energy_consumption} mV\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model...\n",
      "Subprocess started.\n",
      "Time taken to save ONNX model: 1.1493628025054932 seconds\n",
      "Time taken for inference on 10000 samples: 0.3822 seconds\n",
      "Subprocess finished.\n",
      "Resource monitoring finished.\n",
      "Content saved to ./mnist_models/output_cnn/output-mnist-cnn-onnx.txt\n",
      "Total energy consumption: 4980.994953481356 mV\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import onnxruntime as ort\n",
    "import tf2onnx\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import threading\n",
    "import subprocess\n",
    "import tempfile\n",
    "import os\n",
    "import psutil\n",
    "import re\n",
    "\n",
    "# 全局变量\n",
    "thread_output = {}\n",
    "duration = 0\n",
    "inference_duration = 0\n",
    "\n",
    "# 定义监控保存模型时的资源使用率的线程函数\n",
    "def monitor_resources_during_save(stop_event):\n",
    "    cpu_usage = []\n",
    "    gpu_usage = []\n",
    "\n",
    "    while not stop_event.is_set():\n",
    "        cpu_usage.append(psutil.cpu_percent(interval=0.1))\n",
    "        try:\n",
    "            gpu_output = subprocess.check_output(['nvidia-smi', \n",
    "                                                  '--query-gpu=utilization.gpu', '--format=csv,noheader,nounits'])\n",
    "            gpu_usage.append(int(gpu_output.strip()))\n",
    "        except Exception as e:\n",
    "            gpu_usage.append(None)  # 如果没有GPU或nvidia-smi命令失败，则记录None\n",
    "\n",
    "    # 保存监测结果\n",
    "    thread_output['cpu_usage'] = cpu_usage\n",
    "    thread_output['gpu_usage'] = gpu_usage\n",
    "    print(\"Resource monitoring finished.\")\n",
    "\n",
    "# 定义保存模型的函数\n",
    "def save_model(stop_event, model):\n",
    "    global duration\n",
    "    global inference_duration\n",
    "    global X_test  # 确保引用全局变量\n",
    "\n",
    "    print(\"Saving model...\")\n",
    "    # --------------------------------------------------------------------------------\n",
    "    start_time = time.time()\n",
    "    # 使用 tf2onnx 保存模型为ONNX格式\n",
    "    spec = (tf.TensorSpec((None, 28, 28, 1), tf.float32, name=\"input\"),)\n",
    "    output_path = \"mnist_cnn_model.onnx\"\n",
    "    \n",
    "    for _ in range(5):  # 保存 ONNX 模型的循环次数为 5\n",
    "        model_proto, _ = tf2onnx.convert.from_keras(model, input_signature=spec, output_path=output_path)\n",
    "\n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    print(f'Time taken to save ONNX model: {duration} seconds')\n",
    "    # --------------------------------------------------------------------------------\n",
    "\n",
    "    # 测量推理时间\n",
    "    # 使用 ONNX Runtime 加载模型\n",
    "    ort_session = ort.InferenceSession(output_path)\n",
    "    \n",
    "    # 准备 ONNX 输入\n",
    "    X_test_sample = X_test[:10000].astype(np.float32)  # 选择前5000个样本进行推理\n",
    "    \n",
    "    # 测量推理时间\n",
    "    start_time_inference = time.time()\n",
    "    \n",
    "    # 获取模型输入名称\n",
    "    input_name = ort_session.get_inputs()[0].name\n",
    "    \n",
    "    # 批量推理\n",
    "    batch_size = 128\n",
    "    num_batches = len(X_test_sample) // batch_size\n",
    "    for _ in range(1):  # 推理循环次数为 1\n",
    "        for batch_idx in range(num_batches):\n",
    "            batch_start = batch_idx * batch_size\n",
    "            batch_end = batch_start + batch_size\n",
    "            batch = X_test_sample[batch_start:batch_end]\n",
    "            ort_outs = ort_session.run(None, {input_name: batch})\n",
    "    \n",
    "    end_time_inference = time.time()\n",
    "    inference_duration = end_time_inference - start_time_inference\n",
    "    print(f'Time taken for inference on {len(X_test_sample)} samples: {inference_duration:.4f} seconds')\n",
    "\n",
    "    stop_event.set()  # 触发停止其他线程\n",
    "\n",
    "# 运行外部脚本并捕获输出\n",
    "def run_script(stop_event):\n",
    "    try:\n",
    "        # 使用临时文件来存储子进程输出\n",
    "        with tempfile.NamedTemporaryFile(delete=False) as tmp_file:\n",
    "            process = subprocess.Popen(['/Users/anelloyi/Desktop/run_powermetrics.sh'], \n",
    "                                       stdout=tmp_file, stderr=subprocess.STDOUT, text=True)\n",
    "            print(\"Subprocess started.\")\n",
    "            while not stop_event.is_set():\n",
    "                if process.poll() is not None:  # 检查进程是否已经结束\n",
    "                    break\n",
    "            \n",
    "            if process.poll() is None:\n",
    "                # 尝试终止进程\n",
    "                process.terminate()\n",
    "                try:\n",
    "                    process.wait(timeout=0.1)\n",
    "                except subprocess.TimeoutExpired:\n",
    "                    process.kill()  # 如果超时，则强制终止\n",
    "                    process.wait()\n",
    "        \n",
    "        # 读取临时文件的内容\n",
    "        with open(tmp_file.name, 'r') as f:\n",
    "            thread_output['powermetrics'] = f.read()\n",
    "        \n",
    "        os.remove(tmp_file.name)  # 删除临时文件\n",
    "        print(\"Subprocess finished.\")\n",
    "    except Exception as e:\n",
    "        thread_output['powermetrics'] = str(e)\n",
    "        print(\"Exception in subprocess:\", str(e))\n",
    "\n",
    "# 载入数据和模型\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X_train = X_train.reshape((X_train.shape[0], 28, 28, 1))\n",
    "X_test = X_test.reshape((X_test.shape[0], 28, 28, 1))\n",
    "X_train = X_train.astype('float32') / 255\n",
    "X_test = X_test.astype('float32') / 255\n",
    "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "stop_event = threading.Event()\n",
    "model = tf.keras.models.load_model(\"./mnist_models/mnist_cnn_model.h5\")\n",
    "\n",
    "# 创建和启动线程\n",
    "thread1 = threading.Thread(target=save_model, args=(stop_event, model))\n",
    "thread2 = threading.Thread(target=monitor_resources_during_save, args=(stop_event,))\n",
    "thread3 = threading.Thread(target=run_script, args=(stop_event,))\n",
    "thread3.start()\n",
    "thread1.start()\n",
    "thread2.start()\n",
    "# 等待线程完成\n",
    "thread1.join()\n",
    "thread2.join()\n",
    "thread3.join()\n",
    "\n",
    "# 输出从线程收集的数据\n",
    "content = thread_output.get('powermetrics', 'No output captured')\n",
    "# 将内容按行拆分\n",
    "lines = content.split('\\n')\n",
    "# 筛选出以 \"CPU consume\" 开头的行\n",
    "filtered_lines = [line for line in lines if line.startswith('GPU Power:') or \n",
    "                  line.startswith('CPU Power:')] \n",
    "\n",
    "# 将筛选后的行合并为一个字符串，每行之间用换行符分隔\n",
    "filtered_content = '\\n'.join(filtered_lines)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "file_name = f'./mnist_models/output_cnn/output-mnist-cnn-onnx.txt'\n",
    "# ----------------------------------------------------------------------------\n",
    "with open(file_name, 'w') as file:\n",
    "    file.write(filtered_content)\n",
    "    file.write(f'\\nTotal Duration(s): {duration:.2f}')\n",
    "    file.write(f'\\nInference Duration(s): {inference_duration:.4f}')\n",
    "print(f\"Content saved to {file_name}\")\n",
    "\n",
    "filtered_lines_count = len(filtered_lines)\n",
    "# print(filtered_lines_count)\n",
    "# print(filtered_lines)\n",
    "\n",
    "duration = inference_duration\n",
    "# 提取每一个采样点的数字，即CPU和GPU的具体mV\n",
    "numbers = []\n",
    "for line in filtered_lines:\n",
    "    match = re.search(r'[\\d.]+', line)\n",
    "    if match:\n",
    "        numbers.append(float(match.group())) \n",
    "\n",
    "delta_time = duration * 2 / filtered_lines_count\n",
    "\n",
    "numbers_scaled = [num * delta_time for num in numbers]\n",
    "total_energy_consumption = sum(numbers_scaled)\n",
    "print(f\"Total energy consumption: {total_energy_consumption} mV\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN core ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "When both 'convert_to' and 'minimum_deployment_target' not specified, 'convert_to' is set to \"mlprogram\" and 'minimum_deployment_target' is set to ct.target.iOS15 (which is same as ct.target.macOS12). Note: the model will not run on systems older than iOS15/macOS12/watchOS8/tvOS15. In order to make your model run on older system, please set the 'minimum_deployment_target' to iOS14/iOS13. Details please see the link: https://apple.github.io/coremltools/docs-guides/source/target-conversion-formats.html\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model...\n",
      "Subprocess started.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running TensorFlow Graph Passes: 100%|██████████| 6/6 [00:00<00:00, 25.08 passes/s]\n",
      "Converting TF Frontend ==> MIL Ops: 100%|██████████| 26/26 [00:00<00:00, 2796.85 ops/s]\n",
      "Running MIL frontend_tensorflow2 pipeline: 100%|██████████| 7/7 [00:00<00:00, 1517.16 passes/s]\n",
      "Running MIL default pipeline: 100%|██████████| 78/78 [00:00<00:00, 508.17 passes/s]\n",
      "Running MIL backend_mlprogram pipeline: 100%|██████████| 12/12 [00:00<00:00, 1697.07 passes/s]\n",
      "When both 'convert_to' and 'minimum_deployment_target' not specified, 'convert_to' is set to \"mlprogram\" and 'minimum_deployment_target' is set to ct.target.iOS15 (which is same as ct.target.macOS12). Note: the model will not run on systems older than iOS15/macOS12/watchOS8/tvOS15. In order to make your model run on older system, please set the 'minimum_deployment_target' to iOS14/iOS13. Details please see the link: https://apple.github.io/coremltools/docs-guides/source/target-conversion-formats.html\n",
      "Running TensorFlow Graph Passes: 100%|██████████| 6/6 [00:00<00:00, 23.52 passes/s]\n",
      "Converting TF Frontend ==> MIL Ops: 100%|██████████| 26/26 [00:00<00:00, 2711.99 ops/s]\n",
      "Running MIL frontend_tensorflow2 pipeline: 100%|██████████| 7/7 [00:00<00:00, 1430.04 passes/s]\n",
      "Running MIL default pipeline: 100%|██████████| 78/78 [00:00<00:00, 500.65 passes/s]\n",
      "Running MIL backend_mlprogram pipeline: 100%|██████████| 12/12 [00:00<00:00, 1569.68 passes/s]\n",
      "When both 'convert_to' and 'minimum_deployment_target' not specified, 'convert_to' is set to \"mlprogram\" and 'minimum_deployment_target' is set to ct.target.iOS15 (which is same as ct.target.macOS12). Note: the model will not run on systems older than iOS15/macOS12/watchOS8/tvOS15. In order to make your model run on older system, please set the 'minimum_deployment_target' to iOS14/iOS13. Details please see the link: https://apple.github.io/coremltools/docs-guides/source/target-conversion-formats.html\n",
      "Running TensorFlow Graph Passes: 100%|██████████| 6/6 [00:00<00:00, 25.53 passes/s]\n",
      "Converting TF Frontend ==> MIL Ops: 100%|██████████| 26/26 [00:00<00:00, 2550.21 ops/s]\n",
      "Running MIL frontend_tensorflow2 pipeline: 100%|██████████| 7/7 [00:00<00:00, 1466.91 passes/s]\n",
      "Running MIL default pipeline: 100%|██████████| 78/78 [00:00<00:00, 513.59 passes/s]\n",
      "Running MIL backend_mlprogram pipeline: 100%|██████████| 12/12 [00:00<00:00, 1662.76 passes/s]\n",
      "When both 'convert_to' and 'minimum_deployment_target' not specified, 'convert_to' is set to \"mlprogram\" and 'minimum_deployment_target' is set to ct.target.iOS15 (which is same as ct.target.macOS12). Note: the model will not run on systems older than iOS15/macOS12/watchOS8/tvOS15. In order to make your model run on older system, please set the 'minimum_deployment_target' to iOS14/iOS13. Details please see the link: https://apple.github.io/coremltools/docs-guides/source/target-conversion-formats.html\n",
      "Running TensorFlow Graph Passes: 100%|██████████| 6/6 [00:00<00:00, 25.57 passes/s]\n",
      "Converting TF Frontend ==> MIL Ops: 100%|██████████| 26/26 [00:00<00:00, 2501.65 ops/s]\n",
      "Running MIL frontend_tensorflow2 pipeline: 100%|██████████| 7/7 [00:00<00:00, 1335.40 passes/s]\n",
      "Running MIL default pipeline: 100%|██████████| 78/78 [00:00<00:00, 507.57 passes/s]\n",
      "Running MIL backend_mlprogram pipeline: 100%|██████████| 12/12 [00:00<00:00, 1820.12 passes/s]\n",
      "When both 'convert_to' and 'minimum_deployment_target' not specified, 'convert_to' is set to \"mlprogram\" and 'minimum_deployment_target' is set to ct.target.iOS15 (which is same as ct.target.macOS12). Note: the model will not run on systems older than iOS15/macOS12/watchOS8/tvOS15. In order to make your model run on older system, please set the 'minimum_deployment_target' to iOS14/iOS13. Details please see the link: https://apple.github.io/coremltools/docs-guides/source/target-conversion-formats.html\n",
      "Running TensorFlow Graph Passes: 100%|██████████| 6/6 [00:00<00:00, 26.30 passes/s]\n",
      "Converting TF Frontend ==> MIL Ops: 100%|██████████| 26/26 [00:00<00:00, 2619.61 ops/s]\n",
      "Running MIL frontend_tensorflow2 pipeline: 100%|██████████| 7/7 [00:00<00:00, 1388.12 passes/s]\n",
      "Running MIL default pipeline: 100%|██████████| 78/78 [00:00<00:00, 503.40 passes/s]\n",
      "Running MIL backend_mlprogram pipeline: 100%|██████████| 12/12 [00:00<00:00, 1714.06 passes/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to save Core ML model: 6.624547719955444 seconds\n",
      "Time taken for inference on 10000 samples: 4.8873 seconds\n",
      "Subprocess finished.\n",
      "Resource monitoring finished.\n",
      "Content saved to ./mnist_models/output_cnn/output-mnist-cnn-coreml.txt\n",
      "Total energy consumption: 36148.15653216859 mV\n"
     ]
    }
   ],
   "source": [
    "thread_output = {}\n",
    "duration = 0\n",
    "inference_duration = 0  # 添加推理时间变量\n",
    "\n",
    "# 定义监控保存模型时的资源使用率的线程函数\n",
    "def monitor_resources_during_save(stop_event):\n",
    "    cpu_usage = []\n",
    "    gpu_usage = []\n",
    "\n",
    "    while not stop_event.is_set():\n",
    "        cpu_usage.append(psutil.cpu_percent(interval=0.1))\n",
    "        try:\n",
    "            gpu_output = subprocess.check_output(['nvidia-smi', \n",
    "                                                  '--query-gpu=utilization.gpu', '--format=csv,noheader,nounits'])\n",
    "            gpu_usage.append(int(gpu_output.strip()))\n",
    "        except Exception as e:\n",
    "            gpu_usage.append(None)  # 如果没有GPU或nvidia-smi命令失败，则记录None\n",
    "\n",
    "    # 保存监测结果\n",
    "    thread_output['cpu_usage'] = cpu_usage\n",
    "    thread_output['gpu_usage'] = gpu_usage\n",
    "    print(\"Resource monitoring finished.\")\n",
    "\n",
    "# 定义保存模型的函数\n",
    "def save_model(stop_event, model):\n",
    "    global duration\n",
    "    global inference_duration\n",
    "    print(\"Saving model...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    # 将 TensorFlow Keras 模型转换为 Core ML 模型\n",
    "    for i in range(5):\n",
    "        try:\n",
    "            input_shape = ct.Shape(shape=(1, 28, 28, 1))\n",
    "            input_feature = ct.TensorType(name=\"conv2d_input\", shape=input_shape)\n",
    "            mlmodel = ct.convert(model, inputs=[input_feature])\n",
    "            mlmodel.save(f\"mnist_cnn_model_coreML.mlpackage\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving model {i}: {e}\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    print(f'Time taken to save Core ML model: {duration} seconds')\n",
    "\n",
    "    \n",
    "    \n",
    "    # 测量推理时间\n",
    "    X_test_sample = X_test[:10000].astype(np.float32)  # 选择前100个样本进行推理\n",
    "    start_time_inference = time.time()\n",
    "\n",
    "    batch_size =  1 # 批量大小\n",
    "    num_batches = len(X_test_sample) // batch_size  # 计算批次数\n",
    "    predictions = []\n",
    "\n",
    "    # 进行批量推理\n",
    "    for batch_idx in range(num_batches):\n",
    "        batch_start = batch_idx * batch_size\n",
    "        batch_end = (batch_idx + 1) * batch_size\n",
    "        batch = X_test_sample[batch_start:batch_end]\n",
    "\n",
    "        # 去除多余的批次维度添加\n",
    "        batch_reshaped = batch.reshape((batch_size, 28, 28, 1))  # 保持数据的秩为 4\n",
    "\n",
    "        try:\n",
    "            batch_predictions = mlmodel.predict({\"conv2d_input\": batch_reshaped})\n",
    "            predictions.append(batch_predictions)\n",
    "        except Exception as e:\n",
    "            print(f\"Error during batch inference {batch_idx}: {e}\")\n",
    "\n",
    "    end_time_inference = time.time()\n",
    "    inference_duration = end_time_inference - start_time_inference\n",
    "    print(f'Time taken for inference on {len(X_test_sample)} samples: {inference_duration:.4f} seconds')\n",
    "\n",
    "    stop_event.set()  # 触发停止其他线程\n",
    "\n",
    "\n",
    "\n",
    "# 运行外部脚本并捕获输出\n",
    "def run_script(stop_event):\n",
    "    try:\n",
    "        # 使用临时文件来存储子进程输出\n",
    "        with tempfile.NamedTemporaryFile(delete=False) as tmp_file:\n",
    "            process = subprocess.Popen(['/Users/anelloyi/Desktop/run_powermetrics.sh'], \n",
    "                                       stdout=tmp_file, stderr=subprocess.STDOUT, text=True)\n",
    "            print(\"Subprocess started.\")\n",
    "            while not stop_event.is_set():\n",
    "                if process.poll() is not None:  # 检查进程是否已经结束\n",
    "                    break\n",
    "            \n",
    "            if process.poll() is None:\n",
    "                # 尝试终止进程\n",
    "                process.terminate()\n",
    "                try:\n",
    "                    process.wait(timeout=0.1)\n",
    "                except subprocess.TimeoutExpired:\n",
    "                    process.kill()  # 如果超时，则强制终止\n",
    "                    process.wait()\n",
    "        \n",
    "        # 读取临时文件的内容\n",
    "        with open(tmp_file.name, 'r') as f:\n",
    "            thread_output['powermetrics'] = f.read()\n",
    "        \n",
    "        os.remove(tmp_file.name)  # 删除临时文件\n",
    "        print(\"Subprocess finished.\")\n",
    "    except Exception as e:\n",
    "        thread_output['powermetrics'] = str(e)\n",
    "        print(\"Exception in subprocess:\", str(e))\n",
    "\n",
    "\n",
    "stop_event = threading.Event()\n",
    "# model = tf.keras.models.load_model(\"./mnist_models/mnist_cnn_model.h5\")\n",
    "model = tf.keras.models.load_model(\"./models_train/mnist_cnn_model.h5\")\n",
    "\n",
    "\n",
    "# 创建和启动线程\n",
    "thread1 = threading.Thread(target=save_model, args=(stop_event, model))\n",
    "thread2 = threading.Thread(target=monitor_resources_during_save, args=(stop_event,))\n",
    "thread3 = threading.Thread(target=run_script, args=(stop_event,))\n",
    "\n",
    "thread3.start()\n",
    "thread1.start()\n",
    "thread2.start()\n",
    "\n",
    "# 等待线程完成\n",
    "thread1.join()\n",
    "thread2.join()\n",
    "thread3.join()\n",
    "\n",
    "# 输出从线程收集的数据\n",
    "content = thread_output.get('powermetrics', 'No output captured')\n",
    "# 将内容按行拆分\n",
    "lines = content.split('\\n')\n",
    "# 筛选出以 \"CPU consume\" 开头的行\n",
    "filtered_lines = [line for line in lines if line.startswith('GPU Power:') or \n",
    "                  line.startswith('CPU Power:')]\n",
    "# 将筛选后的行合并为一个字符串，每行之间用换行符分隔\n",
    "filtered_content = '\\n'.join(filtered_lines)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "file_name = f'./mnist_models/output_cnn/output-mnist-cnn-coreml.txt'\n",
    "# ----------------------------------------------------------------------------\n",
    "with open(file_name, 'w') as file:\n",
    "    file.write(filtered_content)\n",
    "    file.write(f'\\nTotal Duration(s): {duration:.2f}')\n",
    "print(f\"Content saved to {file_name}\")\n",
    "\n",
    "filtered_lines_count = len(filtered_lines)\n",
    "# print(filtered_lines_count)\n",
    "# print(filtered_lines)\n",
    "\n",
    "duration = inference_duration\n",
    "# 提取每一个采样点的数字，即CPU和GPU的具体mV\n",
    "numbers = []\n",
    "for line in filtered_lines:\n",
    "    match = re.search(r'[\\d.]+', line)\n",
    "    if match:\n",
    "        numbers.append(float(match.group())) \n",
    "# print(\"Extracted numbers:\", numbers)\n",
    "\n",
    "delta_time = duration * 2 / filtered_lines_count\n",
    "# print(delta_time)\n",
    "\n",
    "numbers_scaled = [num * delta_time for num in numbers]\n",
    "total_energy_consumption = sum(numbers_scaled)\n",
    "print(f\"Total energy consumption: {total_energy_consumption} mV\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "When both 'convert_to' and 'minimum_deployment_target' not specified, 'convert_to' is set to \"mlprogram\" and 'minimum_deployment_target' is set to ct.target.iOS15 (which is same as ct.target.macOS12). Note: the model will not run on systems older than iOS15/macOS12/watchOS8/tvOS15. In order to make your model run on older system, please set the 'minimum_deployment_target' to iOS14/iOS13. Details please see the link: https://apple.github.io/coremltools/docs-guides/source/target-conversion-formats.html\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model...\n",
      "Subprocess started.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running TensorFlow Graph Passes: 100%|██████████| 6/6 [00:00<00:00, 22.73 passes/s]\n",
      "Converting TF Frontend ==> MIL Ops: 100%|██████████| 26/26 [00:00<00:00, 2387.51 ops/s]\n",
      "Running MIL frontend_tensorflow2 pipeline: 100%|██████████| 7/7 [00:00<00:00, 1562.12 passes/s]\n",
      "Running MIL default pipeline: 100%|██████████| 78/78 [00:00<00:00, 496.44 passes/s]\n",
      "Running MIL backend_mlprogram pipeline: 100%|██████████| 12/12 [00:00<00:00, 1701.89 passes/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to save Core ML model: 1.6163678169250488 seconds\n",
      "Time taken for inference on 10000 samples: 4.9429 seconds\n",
      "Subprocess finished.\n",
      "Resource monitoring finished.\n",
      "Content saved to ./mnist_models/output_cnn/output-mnist-cnn-coreml.txt\n",
      "Total energy consumption: 14268.401959651219 mV\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import time\n",
    "import numpy as np\n",
    "import psutil\n",
    "import subprocess\n",
    "import os\n",
    "import tempfile\n",
    "import tensorflow as tf\n",
    "import coremltools as ct\n",
    "import re\n",
    "\n",
    "thread_output = {}\n",
    "duration = 0\n",
    "inference_duration = 0  # 添加推理时间变量\n",
    "\n",
    "def monitor_resources_during_save(stop_event):\n",
    "    cpu_usage = []\n",
    "    gpu_usage = []\n",
    "    while not stop_event.is_set():\n",
    "        cpu_usage.append(psutil.cpu_percent(interval=0.1))\n",
    "        try:\n",
    "            gpu_output = subprocess.check_output(['nvidia-smi', '--query-gpu=utilization.gpu', '--format=csv,noheader,nounits'])\n",
    "            gpu_usage.append(int(gpu_output.strip()))\n",
    "        except Exception as e:\n",
    "            gpu_usage.append(None)  # 如果没有GPU或nvidia-smi命令失败，则记录None\n",
    "    thread_output['cpu_usage'] = cpu_usage\n",
    "    thread_output['gpu_usage'] = gpu_usage\n",
    "    print(\"Resource monitoring finished.\")\n",
    "\n",
    "def save_model(stop_event, model, batch_size=128):\n",
    "    global duration\n",
    "    global inference_duration\n",
    "    print(\"Saving model...\")\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        input_shape = ct.Shape(shape=(1, 28, 28, 1))\n",
    "        input_feature = ct.TensorType(name=\"conv2d_input\", shape=input_shape)\n",
    "        mlmodel = ct.convert(model, inputs=[input_feature])\n",
    "        mlmodel.save(f\"mnist_cnn_model_coreML.mlpackage\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving model: {e}\")\n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    print(f'Time taken to save Core ML model: {duration} seconds')\n",
    "\n",
    "    # 测量推理时间\n",
    "    X_test_sample = X_test[:10000].astype(np.float32)  # 选择前1000个样本进行推理\n",
    "    start_time_inference = time.time()\n",
    "    predictions = []\n",
    "    for idx in range(len(X_test_sample)):\n",
    "        batch = X_test_sample[idx]\n",
    "        batch_reshaped = batch.reshape((1, 28, 28, 1))  # 一次只处理一个样本，匹配模型输入维度\n",
    "        try:\n",
    "            batch_predictions = mlmodel.predict({\"conv2d_input\": batch_reshaped})\n",
    "            predictions.append(batch_predictions)\n",
    "        except Exception as e:\n",
    "            print(f\"Error during inference for sample {idx}: {e}\")\n",
    "    end_time_inference = time.time()\n",
    "    inference_duration = end_time_inference - start_time_inference\n",
    "    print(f'Time taken for inference on {len(X_test_sample)} samples: {inference_duration:.4f} seconds')\n",
    "\n",
    "    stop_event.set()\n",
    "\n",
    "\n",
    "def run_script(stop_event):\n",
    "    try:\n",
    "        with tempfile.NamedTemporaryFile(delete=False) as tmp_file:\n",
    "            process = subprocess.Popen(['/Users/anelloyi/Desktop/run_powermetrics.sh'], stdout=tmp_file, stderr=subprocess.STDOUT, text=True)\n",
    "            print(\"Subprocess started.\")\n",
    "            while not stop_event.is_set():\n",
    "                if process.poll() is not None:\n",
    "                    break\n",
    "            if process.poll() is None:\n",
    "                process.terminate()\n",
    "                try:\n",
    "                    process.wait(timeout=0.1)\n",
    "                except subprocess.TimeoutExpired:\n",
    "                    process.kill()\n",
    "                    process.wait()\n",
    "        with open(tmp_file.name, 'r') as f:\n",
    "            thread_output['powermetrics'] = f.read()\n",
    "        os.remove(tmp_file.name)\n",
    "        print(\"Subprocess finished.\")\n",
    "    except Exception as e:\n",
    "        thread_output['powermetrics'] = str(e)\n",
    "        print(\"Exception in subprocess:\", str(e))\n",
    "\n",
    "stop_event = threading.Event()\n",
    "model = tf.keras.models.load_model(\"./models_train/mnist_cnn_model.h5\")\n",
    "\n",
    "thread1 = threading.Thread(target=save_model, args=(stop_event, model, 128))  # 可修改批量大小为 128 或其他值\n",
    "thread2 = threading.Thread(target=monitor_resources_during_save, args=(stop_event,))\n",
    "thread3 = threading.Thread(target=run_script, args=(stop_event,))\n",
    "\n",
    "thread1.start()\n",
    "thread2.start()\n",
    "thread3.start()\n",
    "thread1.join()\n",
    "thread2.join()\n",
    "thread3.join()\n",
    "\n",
    "content = thread_output.get('powermetrics', 'No output captured')\n",
    "lines = content.split('\\n')\n",
    "filtered_lines = [line for line in lines if line.startswith('GPU Power:') or line.startswith('CPU Power:')]\n",
    "filtered_content = '\\n'.join(filtered_lines)\n",
    "\n",
    "file_name = f'./mnist_models/output_cnn/output-mnist-cnn-coreml.txt'\n",
    "with open(file_name, 'w') as file:\n",
    "    file.write(filtered_content)\n",
    "    file.write(f'\\nTotal Duration(s): {duration:.2f}')\n",
    "print(f\"Content saved to {file_name}\")\n",
    "\n",
    "filtered_lines_count = len(filtered_lines)\n",
    "numbers = []\n",
    "for line in filtered_lines:\n",
    "    match = re.search(r'[\\d.]+', line)\n",
    "    if match:\n",
    "        numbers.append(float(match.group()))\n",
    "delta_time = duration * 2 / filtered_lines_count\n",
    "numbers_scaled = [num * delta_time for num in numbers]\n",
    "total_energy_consumption = sum(numbers_scaled)\n",
    "print(f\"Total energy consumption: {total_energy_consumption} mV\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import numpy as np\n",
    "\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(64*5*5, 128)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# pytorch_model = CNNModel()\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "# 设置要选择的测试集数量\n",
    "test_sample_size = 10000  # 例如，选择10000个测试样本\n",
    "\n",
    "# 加载MNIST测试数据\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "test_dataset = datasets.MNIST('.', train=False, transform=transform, download=True)\n",
    "\n",
    "# 从测试集中选择指定数量的样本\n",
    "indices = list(range(len(test_dataset)))\n",
    "np.random.shuffle(indices)\n",
    "selected_indices = indices[:test_sample_size]\n",
    "subset_test_dataset = Subset(test_dataset, selected_indices)\n",
    "\n",
    "test_loader = DataLoader(subset_test_dataset, batch_size=128, shuffle=False)  # 使用更大的batch_size进行批量推理\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model...\n",
      "Subprocess started.\n",
      "Time taken to save PyTorch model: 0.039903879165649414 seconds\n",
      "Test Accuracy on 10000 samples: 15.97%\n",
      "Time taken for inference on 10000 samples, 1 times: 6.0333 seconds\n",
      "Subprocess finished.\n",
      "Resource monitoring finished.\n",
      "Content saved to ./mnist_models/output_cnn/output-mnist-cnn-pth.txt\n",
      "Total energy consumption: 40215.5893334991 mV\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import threading\n",
    "import psutil\n",
    "import subprocess\n",
    "import tempfile\n",
    "import os\n",
    "import re\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "\n",
    "thread_output = {}\n",
    "duration = 0\n",
    "inference_duration = 0  # 添加用于存储推理时间的变量\n",
    "\n",
    "# 定义监控保存模型时的资源使用率的线程函数\n",
    "def monitor_resources_during_save(stop_event):\n",
    "    cpu_usage = []\n",
    "    gpu_usage = []\n",
    "\n",
    "    while not stop_event.is_set():\n",
    "        cpu_usage.append(psutil.cpu_percent(interval=0.1))\n",
    "        try:\n",
    "            gpu_output = subprocess.check_output(['nvidia-smi', \n",
    "                                                  '--query-gpu=utilization.gpu', '--format=csv,noheader,nounits'])\n",
    "            gpu_usage.append(int(gpu_output.strip()))\n",
    "        except Exception as e:\n",
    "            gpu_usage.append(None)  # 如果没有GPU或nvidia-smi命令失败，则记录None\n",
    "\n",
    "    # 保存监测结果\n",
    "    thread_output['cpu_usage'] = cpu_usage\n",
    "    thread_output['gpu_usage'] = gpu_usage\n",
    "    print(\"Resource monitoring finished.\")\n",
    "\n",
    "# 定义保存模型的函数\n",
    "def save_model(stop_event, model):\n",
    "    global duration\n",
    "    global inference_duration\n",
    "    print(\"Saving model...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    # 提取权重并分别保存为.npy文件\n",
    "    for i, layer in enumerate(model.layers):\n",
    "        weights = layer.get_weights()\n",
    "        for j, weight in enumerate(weights):\n",
    "            np.save(f'layer_{i}_weight_{j}.npy', weight)\n",
    "\n",
    "    pytorch_model = CNNModel()\n",
    "\n",
    "    # 为PyTorch模型赋值\n",
    "    with torch.no_grad():\n",
    "        pytorch_model.conv1.weight = nn.Parameter(torch.tensor(np.load('layer_0_weight_0.npy')).permute(3, 2, 0, 1))\n",
    "        pytorch_model.conv1.bias = nn.Parameter(torch.tensor(np.load('layer_0_weight_1.npy')))\n",
    "\n",
    "        pytorch_model.conv2.weight = nn.Parameter(torch.tensor(np.load('layer_2_weight_0.npy')).permute(3, 2, 0, 1))\n",
    "        pytorch_model.conv2.bias = nn.Parameter(torch.tensor(np.load('layer_2_weight_1.npy')))\n",
    "\n",
    "        pytorch_model.fc1.weight = nn.Parameter(torch.tensor(np.load('layer_5_weight_0.npy').T))\n",
    "        pytorch_model.fc1.bias = nn.Parameter(torch.tensor(np.load('layer_5_weight_1.npy')))\n",
    "\n",
    "        pytorch_model.fc2.weight = nn.Parameter(torch.tensor(np.load('layer_7_weight_0.npy').T))\n",
    "        pytorch_model.fc2.bias = nn.Parameter(torch.tensor(np.load('layer_7_weight_1.npy')))\n",
    "\n",
    "    # 保存整个模型\n",
    "    for i in range(1):\n",
    "        torch.save(pytorch_model, 'mnist_cnn_model.pth')\n",
    "\n",
    "\n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    print(f'Time taken to save PyTorch model: {duration} seconds')\n",
    "\n",
    "    #################\n",
    "\n",
    "    # 测量推理时间\n",
    "    start_time = time.time()\n",
    "\n",
    "    # 切换模型到评估模式\n",
    "    pytorch_model.eval()\n",
    "\n",
    "    n_time = 1\n",
    "    for i in range(n_time):\n",
    "        # 进行推理\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for data, target in test_loader:\n",
    "                output = pytorch_model(data)\n",
    "                pred = output.argmax(dim=1, keepdim=True)\n",
    "                correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "        print(f'Test Accuracy on {test_sample_size} samples: {correct / test_sample_size * 100:.2f}%')\n",
    "\n",
    "\n",
    "    end_time = time.time()\n",
    "    inference_duration = end_time - start_time\n",
    "    print(f'Time taken for inference on {test_sample_size} samples, {n_time} times: {inference_duration:.4f} seconds')\n",
    "\n",
    "    stop_event.set()  # 触发停止其他线程\n",
    "\n",
    "# 运行外部脚本并捕获输出\n",
    "def run_script(stop_event):\n",
    "    try:\n",
    "        # 使用临时文件来存储子进程输出\n",
    "        with tempfile.NamedTemporaryFile(delete=False) as tmp_file:\n",
    "            process = subprocess.Popen(['/Users/anelloyi/Desktop/run_powermetrics.sh'], \n",
    "                                       stdout=tmp_file, stderr=subprocess.STDOUT, text=True)\n",
    "            print(\"Subprocess started.\")\n",
    "            while not stop_event.is_set():\n",
    "                if process.poll() is not None:  # 检查进程是否已经结束\n",
    "                    break\n",
    "            \n",
    "            if process.poll() is None:\n",
    "                # 尝试终止进程\n",
    "                process.terminate()\n",
    "                try:\n",
    "                    process.wait(timeout=0.1)\n",
    "                except subprocess.TimeoutExpired:\n",
    "                    process.kill()  # 如果超时，则强制终止\n",
    "                    process.wait()\n",
    "        \n",
    "        # 读取临时文件的内容\n",
    "        with open(tmp_file.name, 'r') as f:\n",
    "            thread_output['powermetrics'] = f.read()\n",
    "        \n",
    "        os.remove(tmp_file.name)  # 删除临时文件\n",
    "        print(\"Subprocess finished.\")\n",
    "    except Exception as e:\n",
    "        thread_output['powermetrics'] = str(e)\n",
    "        print(\"Exception in subprocess:\", str(e))\n",
    "\n",
    "# 定义推理测试的线程函数\n",
    "def test_inference(stop_event, model):\n",
    "    global inference_duration\n",
    "    print(\"Testing inference...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    # 示例推理代码：假设输入数据为随机生成的\n",
    "    input_data = tf.random.normal([1, 28, 28, 1])  # 根据你的模型调整输入形状\n",
    "    model.predict(input_data)\n",
    "\n",
    "    end_time = time.time()\n",
    "    inference_duration = end_time - start_time\n",
    "    print(f'Time taken for inference: {inference_duration} seconds')\n",
    "    stop_event.set()  # 触发停止其他线程\n",
    "\n",
    "stop_event = threading.Event()\n",
    "model = tf.keras.models.load_model(\"./mnist_models/mnist_cnn_model.h5\")\n",
    "\n",
    "# 创建和启动线程\n",
    "thread1 = threading.Thread(target=save_model, args=(stop_event, model))\n",
    "thread2 = threading.Thread(target=monitor_resources_during_save, args=(stop_event,))\n",
    "thread3 = threading.Thread(target=run_script, args=(stop_event,))\n",
    "# thread4 = threading.Thread(target=test_inference, args=(stop_event, model))\n",
    "\n",
    "thread3.start()\n",
    "thread1.start()\n",
    "thread2.start()\n",
    "\n",
    "# 等待线程完成\n",
    "thread1.join()\n",
    "thread2.join()\n",
    "thread3.join()\n",
    "\n",
    "# 最后再做推理\n",
    "# thread4.start()\n",
    "# thread4.join()\n",
    "\n",
    "# 输出从线程收集的数据\n",
    "content = thread_output.get('powermetrics', 'No output captured')\n",
    "lines = content.split('\\n')\n",
    "filtered_lines = [line for line in lines if line.startswith('GPU Power:') or \n",
    "                  line.startswith('CPU Power:')]\n",
    "\n",
    "filtered_content = '\\n'.join(filtered_lines)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "file_name = f'./mnist_models/output_cnn/output-mnist-cnn-pth.txt'\n",
    "# ----------------------------------------------------------------------------\n",
    "with open(file_name, 'w') as file:\n",
    "    file.write(filtered_content)\n",
    "    file.write(f'\\nTotal Duration(s): {duration:.2f}')\n",
    "    file.write(f'\\nInference Duration(s): {inference_duration:.2f}')\n",
    "print(f\"Content saved to {file_name}\")\n",
    "\n",
    "filtered_lines_count = len(filtered_lines)\n",
    "#print(filtered_lines_count)\n",
    "#print(filtered_lines)\n",
    "\n",
    "duration = inference_duration\n",
    "# 确保 filtered_lines_count 不为零\n",
    "if filtered_lines_count > 0:\n",
    "    # 提取每一个采样点的数字，即CPU和GPU的具体mV\n",
    "    numbers = []\n",
    "    for line in filtered_lines:\n",
    "        match = re.search(r'[\\d.]+', line)\n",
    "        if match:\n",
    "            numbers.append(float(match.group()))\n",
    "\n",
    "    delta_time = duration * 2 / filtered_lines_count\n",
    "    numbers_scaled = [num * delta_time for num in numbers]\n",
    "    total_energy_consumption = sum(numbers_scaled)\n",
    "    print(f\"Total energy consumption: {total_energy_consumption} mV\")\n",
    "else:\n",
    "    print(\"No filtered lines to process.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.18 ('env_name')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "37cc1b05901c8571bea9fc4b42e3f528ca4394d6e6719f1b5d30d208c4a3cfc1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
