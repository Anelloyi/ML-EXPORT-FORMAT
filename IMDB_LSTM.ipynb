{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-08 12:55:52.017932: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import numpy as np\n",
    "import time\n",
    "import subprocess\n",
    "import os\n",
    "import psutil\n",
    "import tensorflow as tf\n",
    "import tempfile\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.callbacks import LambdaCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "scikit-learn version 1.3.2 is not supported. Minimum required version: 0.17. Maximum required version: 1.1.2. Disabling scikit-learn conversion API.\n",
      "XGBoost version 2.1.0 has not been tested with coremltools. You may run into unexpected errors. XGBoost 1.4.2 is the most recent version that has been tested.\n",
      "Torch version 2.2.2 has not been tested with coremltools. You may run into unexpected errors. Torch 2.2.0 is the most recent version that has been tested.\n",
      "/opt/anaconda3/envs/env_name/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import time\n",
    "import subprocess\n",
    "import os\n",
    "import psutil\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.callbacks import LambdaCallback\n",
    "import tempfile\n",
    "import re\n",
    "import tf2onnx\n",
    "import coremltools as ct\n",
    "import onnxruntime as ort\n",
    "import coremltools as ct\n",
    "from coremltools.models import MLModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 全局变量用于存储子线程的输出\n",
    "thread_output = {}\n",
    "\n",
    "# 加载 IMDB 数据集\n",
    "max_features = 20000  # 使用的单词数量\n",
    "maxlen = 100  # 每条评论的最大长度\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "\n",
    "# 数据预处理和填充\n",
    "X_train = pad_sequences(X_train, maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "Memory usage: 90.4%\n",
      "CPU usage: 9.8%\n",
      "195/195 - 42s - loss: 0.4247 - accuracy: 0.7954 - 42s/epoch - 215ms/step\n",
      "Epoch 2/2\n",
      "Memory usage: 90.2%\n",
      "CPU usage: 7.3%\n",
      "195/195 - 39s - loss: 0.2283 - accuracy: 0.9107 - 39s/epoch - 200ms/step\n",
      "Test accuracy: 0.8289999961853027\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# 构建 LSTM 模型\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=max_features, output_dim=128, input_length=maxlen),\n",
    "    LSTM(units=128, return_sequences=True),  # 第一层LSTM，返回序列以叠加另一层LSTM\n",
    "    LSTM(units=128),  # 第二层LSTM\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')  # 二分类问题，使用sigmoid激活函数\n",
    "])\n",
    "\n",
    "# 编译模型\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 自定义批次生成器\n",
    "def batch_generator(X, y, batch_size):\n",
    "    while True:\n",
    "        for start in range(0, len(X), batch_size):\n",
    "            end = min(start + batch_size, len(X))\n",
    "            yield X[start:end], y[start:end]\n",
    "\n",
    "batch_size = 128\n",
    "train_gen = batch_generator(X_train, y_train, batch_size)\n",
    "\n",
    "# 监控资源使用情况\n",
    "def monitor_resources():\n",
    "    memory_info = psutil.virtual_memory()\n",
    "    print(f\"Memory usage: {memory_info.percent}%\")\n",
    "    cpu_usage = psutil.cpu_percent(interval=1)\n",
    "    print(f\"CPU usage: {cpu_usage}%\")\n",
    "\n",
    "# 训练模型并监控资源使用情况\n",
    "steps_per_epoch = len(X_train) // batch_size\n",
    "model.fit(train_gen, epochs=2, steps_per_epoch=steps_per_epoch, verbose=2, \n",
    "          callbacks=[LambdaCallback(on_epoch_end=lambda epoch, logs: monitor_resources())])\n",
    "\n",
    "# 评估模型\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f'Test accuracy: {score[1]}')\n",
    "\n",
    "model.save('./models_train/imdb_lstm_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMDB-LSTM-H5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### H5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subprocess started.\n",
      "Time taken to save TensorFlow SavedModel: 0.20819616317749023 seconds\n",
      "4/4 [==============================] - 1s 80ms/step\n",
      "4/4 [==============================] - 0s 86ms/step\n",
      "4/4 [==============================] - 0s 65ms/step\n",
      "4/4 [==============================] - 0s 64ms/step\n",
      "4/4 [==============================] - 0s 80ms/step\n",
      "4/4 [==============================] - 0s 72ms/step\n",
      "4/4 [==============================] - 0s 81ms/step\n",
      "4/4 [==============================] - 0s 66ms/step\n",
      "4/4 [==============================] - 0s 65ms/step\n",
      "4/4 [==============================] - 0s 61ms/step\n",
      "4/4 [==============================] - 0s 81ms/step\n",
      "4/4 [==============================] - 0s 63ms/step\n",
      "4/4 [==============================] - 0s 60ms/step\n",
      "4/4 [==============================] - 0s 62ms/step\n",
      "4/4 [==============================] - 0s 82ms/step\n",
      "4/4 [==============================] - 0s 66ms/step\n",
      "4/4 [==============================] - 0s 65ms/step\n",
      "4/4 [==============================] - 0s 79ms/step\n",
      "4/4 [==============================] - 0s 79ms/step\n",
      "4/4 [==============================] - 0s 65ms/step\n",
      "4/4 [==============================] - 0s 67ms/step\n",
      "4/4 [==============================] - 0s 80ms/step\n",
      "4/4 [==============================] - 0s 66ms/step\n",
      "4/4 [==============================] - 0s 62ms/step\n",
      "4/4 [==============================] - 0s 87ms/step\n",
      "4/4 [==============================] - 0s 63ms/step\n",
      "4/4 [==============================] - 0s 61ms/step\n",
      "4/4 [==============================] - 0s 83ms/step\n",
      "4/4 [==============================] - 0s 80ms/step\n",
      "4/4 [==============================] - 0s 64ms/step\n",
      "4/4 [==============================] - 0s 62ms/step\n",
      "4/4 [==============================] - 0s 81ms/step\n",
      "4/4 [==============================] - 0s 65ms/step\n",
      "4/4 [==============================] - 0s 65ms/step\n",
      "4/4 [==============================] - 0s 80ms/step\n",
      "4/4 [==============================] - 0s 64ms/step\n",
      "4/4 [==============================] - 0s 64ms/step\n",
      "4/4 [==============================] - 0s 78ms/step\n",
      "4/4 [==============================] - 0s 80ms/step\n",
      "4/4 [==============================] - 0s 61ms/step\n",
      "4/4 [==============================] - 0s 61ms/step\n",
      "4/4 [==============================] - 0s 62ms/step\n",
      "4/4 [==============================] - 0s 81ms/step\n",
      "4/4 [==============================] - 0s 66ms/step\n",
      "4/4 [==============================] - 0s 68ms/step\n",
      "4/4 [==============================] - 0s 82ms/step\n",
      "4/4 [==============================] - 0s 80ms/step\n",
      "4/4 [==============================] - 0s 65ms/step\n",
      "4/4 [==============================] - 0s 63ms/step\n",
      "4/4 [==============================] - 0s 87ms/step\n",
      "4/4 [==============================] - 0s 64ms/step\n",
      "4/4 [==============================] - 0s 65ms/step\n",
      "4/4 [==============================] - 0s 81ms/step\n",
      "4/4 [==============================] - 0s 63ms/step\n",
      "4/4 [==============================] - 0s 64ms/step\n",
      "4/4 [==============================] - 0s 81ms/step\n",
      "4/4 [==============================] - 0s 80ms/step\n",
      "4/4 [==============================] - 0s 66ms/step\n",
      "4/4 [==============================] - 0s 61ms/step\n",
      "4/4 [==============================] - 0s 82ms/step\n",
      "4/4 [==============================] - 0s 80ms/step\n",
      "4/4 [==============================] - 0s 63ms/step\n",
      "4/4 [==============================] - 0s 62ms/step\n",
      "4/4 [==============================] - 0s 79ms/step\n",
      "4/4 [==============================] - 0s 67ms/step\n",
      "4/4 [==============================] - 0s 62ms/step\n",
      "4/4 [==============================] - 0s 78ms/step\n",
      "4/4 [==============================] - 0s 68ms/step\n",
      "4/4 [==============================] - 0s 82ms/step\n",
      "4/4 [==============================] - 0s 65ms/step\n",
      "4/4 [==============================] - 0s 97ms/step\n",
      "4/4 [==============================] - 0s 67ms/step\n",
      "4/4 [==============================] - 0s 63ms/step\n",
      "4/4 [==============================] - 0s 81ms/step\n",
      "4/4 [==============================] - 0s 65ms/step\n",
      "4/4 [==============================] - 0s 63ms/step\n",
      "4/4 [==============================] - 0s 65ms/step\n",
      "4/4 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 1s 951ms/step\n",
      "Time taken for inference on 10000 samples: 30.1386 seconds\n",
      "Subprocess finished.\n",
      "Resource monitoring finished.\n",
      "Content saved to ./imbd_models/output_lstm/output-imdb-lstm-h5.txt\n",
      "Total energy consumption: 2128.41 mV\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import time\n",
    "import subprocess\n",
    "import tempfile\n",
    "import os\n",
    "import re\n",
    "import psutil\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# 初始化全局变量\n",
    "thread_output = {}\n",
    "duration = 0\n",
    "inference_duration = 0\n",
    "\n",
    "# 定义监控保存模型时的资源使用率的线程函数\n",
    "def monitor_resources_during_save(stop_event):\n",
    "    cpu_usage = []\n",
    "    gpu_usage = []\n",
    "\n",
    "    while not stop_event.is_set():\n",
    "        cpu_usage.append(psutil.cpu_percent(interval=0.1))\n",
    "        try:\n",
    "            gpu_output = subprocess.check_output(['nvidia-smi', '--query-gpu=utilization.gpu', '--format=csv,noheader,nounits'])\n",
    "            gpu_usage.append(int(gpu_output.strip()))\n",
    "        except Exception as e:\n",
    "            gpu_usage.append(None)  # 如果没有GPU或nvidia-smi命令失败，则记录None\n",
    "\n",
    "    # 保存监测结果\n",
    "    thread_output['cpu_usage'] = cpu_usage\n",
    "    thread_output['gpu_usage'] = gpu_usage\n",
    "    print(\"Resource monitoring finished.\")\n",
    "\n",
    "# 定义保存模型的函数\n",
    "def save_model(stop_event, model):\n",
    "    global duration\n",
    "    global inference_duration\n",
    "    start_time = time.time()\n",
    "\n",
    "    for i in range(0, 1):\n",
    "        model.save(\"imdb_lstm_model.h5\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    print(f'Time taken to save TensorFlow SavedModel: {duration} seconds')\n",
    "    \n",
    "    # 测量推理时间\n",
    "    X_test_sample = X_test[:10000]  # 选择前1000个样本进行推理\n",
    "    batch_size = 128\n",
    "    num_batches = (len(X_test_sample) + batch_size - 1) // batch_size\n",
    "\n",
    "    start_time_inference = time.time()\n",
    "    for _ in range(1):  # 推理循环次数为1\n",
    "        for batch_idx in range(num_batches):\n",
    "            batch_start = batch_idx * batch_size\n",
    "            batch_end = min(batch_start + batch_size, len(X_test_sample))\n",
    "            batch = X_test_sample[batch_start:batch_end]\n",
    "            model.predict(batch)\n",
    "    end_time_inference = time.time()\n",
    "\n",
    "    inference_duration = end_time_inference - start_time_inference\n",
    "    print(f'Time taken for inference on {len(X_test_sample)} samples: {inference_duration:.4f} seconds')\n",
    "\n",
    "    stop_event.set()  # 触发停止其他线程\n",
    "\n",
    "# 运行外部脚本并捕获输出\n",
    "def run_script(stop_event):\n",
    "    try:\n",
    "        with tempfile.NamedTemporaryFile(delete=False) as tmp_file:\n",
    "            process = subprocess.Popen(['/Users/anelloyi/Desktop/run_powermetrics.sh'], stdout=tmp_file, stderr=subprocess.STDOUT, text=True)\n",
    "            print(\"Subprocess started.\")\n",
    "            while not stop_event.is_set():\n",
    "                if process.poll() is not None:  # 检查进程是否已经结束\n",
    "                    break\n",
    "            \n",
    "            if process.poll() is None:\n",
    "                process.terminate()\n",
    "                try:\n",
    "                    process.wait(timeout=0.1)\n",
    "                except subprocess.TimeoutExpired:\n",
    "                    process.kill()\n",
    "                    process.wait()\n",
    "        \n",
    "        with open(tmp_file.name, 'r') as f:\n",
    "            thread_output['powermetrics'] = f.read()\n",
    "        \n",
    "        os.remove(tmp_file.name)  # 删除临时文件\n",
    "        print(\"Subprocess finished.\")\n",
    "    except Exception as e:\n",
    "        thread_output['powermetrics'] = str(e)\n",
    "        print(\"Exception in subprocess:\", str(e))\n",
    "\n",
    "# 创建和启动线程\n",
    "stop_event = threading.Event()\n",
    "model = tf.keras.models.load_model(\"./models_train/imdb_lstm_model.h5\")  # 重新加载模型以进行保存性能监控\n",
    "\n",
    "# # 加载IMDB数据集进行处理\n",
    "# max_features = 20000\n",
    "# maxlen = 100\n",
    "# (X_train, y_train), (X_test, y_test) = tf.keras.datasets.imdb.load_data(num_words=max_features)\n",
    "# X_test = tf.keras.preprocessing.sequence.pad_sequences(X_test, maxlen=maxlen)\n",
    "\n",
    "thread1 = threading.Thread(target=save_model, args=(stop_event, model))\n",
    "thread2 = threading.Thread(target=monitor_resources_during_save, args=(stop_event,))\n",
    "thread3 = threading.Thread(target=run_script, args=(stop_event,))\n",
    "thread1.start()\n",
    "thread2.start()\n",
    "thread3.start()\n",
    "\n",
    "# 等待线程完成\n",
    "thread1.join()\n",
    "thread2.join()\n",
    "thread3.join()\n",
    "\n",
    "# 输出从线程收集的数据\n",
    "content = thread_output.get('powermetrics', 'No output captured')\n",
    "# 将内容按行拆分\n",
    "lines = content.split('\\n')\n",
    "# 筛选出以 \"CPU consume\" 和 \"GPU Power\" 开头的行\n",
    "filtered_lines = [line for line in lines if line.startswith('GPU Power:') or line.startswith('CPU Power:')]\n",
    "# 将筛选后的行合并为一个字符串，每行之间用换行符分隔\n",
    "filtered_content = '\\n'.join(filtered_lines)\n",
    "\n",
    "# 保存过滤后的内容到文件\n",
    "output_file_name = './imbd_models/output_lstm/output-imdb-lstm-h5.txt'\n",
    "with open(output_file_name, 'w') as file:\n",
    "    file.write(filtered_content)\n",
    "    file.write(f'\\nTotal Duration(s): {duration:.2f}')\n",
    "    file.write(f'\\nInference Duration(s): {inference_duration:.4f}')\n",
    "print(f\"Content saved to {output_file_name}\")\n",
    "\n",
    "# 提取每一个采样点的数字，即CPU和GPU的具体mV\n",
    "numbers = []\n",
    "for line in filtered_lines:\n",
    "    match = re.search(r'[\\d.]+', line)\n",
    "    if match:\n",
    "        numbers.append(float(match.group()))\n",
    "\n",
    "delta_time = duration * 2 / len(filtered_lines)\n",
    "numbers_scaled = [num * delta_time for num in numbers]\n",
    "total_energy_consumption = sum(numbers_scaled)\n",
    "print(f\"Total energy consumption: {total_energy_consumption:.2f} mV\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subprocess started.\n",
      "Time taken to save ONNX model: 2.283402919769287 seconds\n",
      "Time taken for inference on 10000 samples: 4.3970 seconds\n",
      "Subprocess finished.\n",
      "Resource monitoring finished.\n",
      "Content saved to ./imbd_models/output_lstm/output-imdb-lstm-onnx.txt\n",
      "Total energy consumption: 35187.12 mV\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import time\n",
    "import subprocess\n",
    "import tempfile\n",
    "import os\n",
    "import re\n",
    "import psutil\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tf2onnx\n",
    "import onnxruntime as ort\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# 初始化全局变量\n",
    "thread_output = {}\n",
    "duration = 0\n",
    "inference_duration = 0\n",
    "\n",
    "# 定义监控资源使用率的线程函数\n",
    "def monitor_resources_during_save(stop_event):\n",
    "    cpu_usage = []\n",
    "    gpu_usage = []\n",
    "\n",
    "    while not stop_event.is_set():\n",
    "        cpu_usage.append(psutil.cpu_percent(interval=0.1))\n",
    "        try:\n",
    "            gpu_output = subprocess.check_output(['nvidia-smi', \n",
    "                                                  '--query-gpu=utilization.gpu', '--format=csv,noheader,nounits'])\n",
    "            gpu_usage.append(int(gpu_output.strip()))\n",
    "        except Exception as e:\n",
    "            gpu_usage.append(None)  # 如果没有GPU或nvidia-smi命令失败，则记录None\n",
    "\n",
    "    # 保存监测结果\n",
    "    thread_output['cpu_usage'] = cpu_usage\n",
    "    thread_output['gpu_usage'] = gpu_usage\n",
    "    print(\"Resource monitoring finished.\")\n",
    "\n",
    "# 定义保存模型的函数\n",
    "def save_model(stop_event, model):\n",
    "    global duration\n",
    "    global inference_duration\n",
    "    global X_test  # 确保引用全局变量\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # 使用 tf2onnx 保存模型为ONNX格式\n",
    "    spec = (tf.TensorSpec((None, 100), tf.float32, name=\"input\"),)\n",
    "    output_path = \"imdb_lstm_model.onnx\"\n",
    "    \n",
    "    for i in range(1):\n",
    "        model_proto, _ = tf2onnx.convert.from_keras(model, input_signature=spec, output_path=output_path)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    print(f'Time taken to save ONNX model: {duration} seconds')\n",
    "    \n",
    "    # 测量推理时间\n",
    "    # 使用 ONNX Runtime 加载模型\n",
    "    ort_session = ort.InferenceSession(output_path)\n",
    "    \n",
    "    # 准备 ONNX 输入\n",
    "    X_test_sample = X_test[:10000].astype(np.float32)  # 选择前1000个样本进行推理\n",
    "    \n",
    "    # 测量推理时间\n",
    "    start_time_inference = time.time()\n",
    "    \n",
    "    # 获取模型输入名称\n",
    "    input_name = ort_session.get_inputs()[0].name\n",
    "    \n",
    "    # 批量推理部分\n",
    "    batch_size = 128\n",
    "    num_samples = X_test_sample.shape[0]\n",
    "    num_batches = (num_samples + batch_size - 1) // batch_size\n",
    "\n",
    "    for _ in range(1):  # 推理循环次数为 1\n",
    "        for batch_idx in range(num_batches):\n",
    "            batch_start = batch_idx * batch_size\n",
    "            batch_end = min(batch_start + batch_size, num_samples)\n",
    "            batch = X_test_sample[batch_start:batch_end]\n",
    "            ort_outs = ort_session.run(None, {input_name: batch})\n",
    "    \n",
    "    end_time_inference = time.time()\n",
    "    inference_duration = end_time_inference - start_time_inference\n",
    "    print(f'Time taken for inference on {len(X_test_sample)} samples: {inference_duration:.4f} seconds')\n",
    "\n",
    "    stop_event.set()  # 触发停止其他线程\n",
    "\n",
    "# 运行外部脚本并捕获输出\n",
    "def run_script(stop_event):\n",
    "    try:\n",
    "        with tempfile.NamedTemporaryFile(delete=False) as tmp_file:\n",
    "            process = subprocess.Popen(['/Users/anelloyi/Desktop/run_powermetrics.sh'], \n",
    "                                       stdout=tmp_file, stderr=subprocess.STDOUT, text=True)\n",
    "            print(\"Subprocess started.\")\n",
    "            while not stop_event.is_set():\n",
    "                if process.poll() is not None:  # 检查进程是否已经结束\n",
    "                    break\n",
    "            \n",
    "            if process.poll() is None:\n",
    "                process.terminate()\n",
    "                try:\n",
    "                    process.wait(timeout=0.1)\n",
    "                except subprocess.TimeoutExpired:\n",
    "                    process.kill()\n",
    "                    process.wait()\n",
    "        \n",
    "        with open(tmp_file.name, 'r') as f:\n",
    "            thread_output['powermetrics'] = f.read()\n",
    "        \n",
    "        os.remove(tmp_file.name)  # 删除临时文件\n",
    "        print(\"Subprocess finished.\")\n",
    "    except Exception as e:\n",
    "        thread_output['powermetrics'] = str(e)\n",
    "        print(\"Exception in subprocess:\", str(e))\n",
    "\n",
    "# 创建和启动线程\n",
    "stop_event = threading.Event()\n",
    "model = tf.keras.models.load_model(\"./models_train/imdb_lstm_model.h5\")  # 重新加载模型以进行保存性能监控\n",
    "\n",
    "# 确保 X_test 被加载以便进行推理\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=20000)\n",
    "X_test = pad_sequences(X_test, maxlen=100)\n",
    "\n",
    "thread1 = threading.Thread(target=save_model, args=(stop_event, model))\n",
    "thread2 = threading.Thread(target=monitor_resources_during_save, args=(stop_event,))\n",
    "thread3 = threading.Thread(target=run_script, args=(stop_event,))\n",
    "thread1.start()\n",
    "thread2.start()\n",
    "thread3.start()\n",
    "\n",
    "# 等待线程完成\n",
    "thread1.join()\n",
    "thread2.join()\n",
    "thread3.join()\n",
    "\n",
    "# 输出从线程收集的数据\n",
    "content = thread_output.get('powermetrics', 'No output captured')\n",
    "# 将内容按行拆分\n",
    "lines = content.split('\\n')\n",
    "# 筛选出以 \"GPU Power\" 和 \"CPU Power\" 开头的行\n",
    "filtered_lines = [line for line in lines if line.startswith('GPU Power:') or \n",
    "                  line.startswith('CPU Power:')]\n",
    "# 将筛选后的行合并为一个字符串，每行之间用换行符分隔\n",
    "filtered_content = '\\n'.join(filtered_lines)\n",
    "\n",
    "# 保存过滤后的内容到文件\n",
    "output_file_name = './imbd_models/output_lstm/output-imdb-lstm-onnx.txt'\n",
    "with open(output_file_name, 'w') as file:\n",
    "    file.write(filtered_content)\n",
    "    file.write(f'\\nTotal Duration(s): {duration:.2f}')\n",
    "    file.write(f'\\nInference Duration(s): {inference_duration:.4f}')\n",
    "print(f\"Content saved to {output_file_name}\")\n",
    "\n",
    "# 提取每一个采样点的数字，即CPU和GPU的具体mV\n",
    "numbers = []\n",
    "for line in filtered_lines:\n",
    "    match = re.search(r'[\\d.]+', line)\n",
    "    if match:\n",
    "        numbers.append(float(match.group())) \n",
    "\n",
    "delta_time = duration * 2 / len(filtered_lines) if len(filtered_lines) > 0 else 0\n",
    "numbers_scaled = [num * delta_time for num in numbers]\n",
    "total_energy_consumption = sum(numbers_scaled)\n",
    "print(f\"Total energy consumption: {total_energy_consumption:.2f} mV\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CORE ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subprocess started.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "When both 'convert_to' and 'minimum_deployment_target' not specified, 'convert_to' is set to \"mlprogram\" and 'minimum_deployment_target' is set to ct.target.iOS15 (which is same as ct.target.macOS12). Note: the model will not run on systems older than iOS15/macOS12/watchOS8/tvOS15. In order to make your model run on older system, please set the 'minimum_deployment_target' to iOS14/iOS13. Details please see the link: https://apple.github.io/coremltools/docs-guides/source/target-conversion-formats.html\n",
      "Running TensorFlow Graph Passes: 100%|██████████| 6/6 [00:01<00:00,  4.63 passes/s]\n",
      "Converting TF Frontend ==> MIL Ops:   0%|          | 0/93 [00:00<?, ? ops/s]Saving value type of int64 into a builtin type of int32, might lose precision!\n",
      "Converting TF Frontend ==> MIL Ops: 100%|██████████| 14/14 [00:00<00:00, 49974.69 ops/s]\n",
      "Input ls elem type unknown. Override with <class 'coremltools.converters.mil.mil.types.type_tensor.tensor.<locals>.tensor'>\n",
      "Converting TF Frontend ==> MIL Ops: 100%|██████████| 40/40 [00:00<00:00, 8720.42 ops/s]\n",
      "Converting TF Frontend ==> MIL Ops: 100%|██████████| 14/14 [00:00<00:00, 31587.01 ops/s]\n",
      "Input ls elem type unknown. Override with <class 'coremltools.converters.mil.mil.types.type_tensor.tensor.<locals>.tensor'>\n",
      "Converting TF Frontend ==> MIL Ops: 100%|██████████| 40/40 [00:00<00:00, 11136.55 ops/s]\n",
      "Saving value type of int64 into a builtin type of int32, might lose precision!\n",
      "Saving value type of int64 into a builtin type of int32, might lose precision!\n",
      "Converting TF Frontend ==> MIL Ops: 100%|██████████| 14/14 [00:00<00:00, 60661.42 ops/s]\n",
      "Input ls elem type unknown. Override with <class 'coremltools.converters.mil.mil.types.type_tensor.tensor.<locals>.tensor'>\n",
      "Converting TF Frontend ==> MIL Ops: 100%|██████████| 41/41 [00:00<00:00, 10226.97 ops/s]\n",
      "Converting TF Frontend ==> MIL Ops: 100%|██████████| 14/14 [00:00<00:00, 23610.88 ops/s]\n",
      "Input ls elem type unknown. Override with <class 'coremltools.converters.mil.mil.types.type_tensor.tensor.<locals>.tensor'>\n",
      "Converting TF Frontend ==> MIL Ops: 100%|██████████| 41/41 [00:00<00:00, 11945.43 ops/s]\n",
      "Saving value type of int64 into a builtin type of int32, might lose precision!\n",
      "Converting TF Frontend ==> MIL Ops: 100%|██████████| 93/93 [00:00<00:00, 1311.21 ops/s]\n",
      "Running MIL frontend_tensorflow2 pipeline: 100%|██████████| 7/7 [00:00<00:00, 338.31 passes/s]\n",
      "Running MIL default pipeline: 100%|██████████| 78/78 [00:00<00:00, 199.23 passes/s]\n",
      "Running MIL backend_mlprogram pipeline: 100%|██████████| 12/12 [00:00<00:00, 185.63 passes/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to save Core ML model: 5.234626054763794 seconds\n",
      "Time taken for inference on 1 sample, repeated 32 times: 6.3541 seconds\n",
      "Resource monitoring finished.\n",
      "Subprocess finished.\n",
      "Content saved to ./imbd_models/output_lstm/output-imdb-lstm-coreml.txt\n",
      "Total energy consumption: 19735.98 mV\n"
     ]
    }
   ],
   "source": [
    "# 初始化全局变量\n",
    "thread_output = {}\n",
    "duration = 0\n",
    "inference_duration = 0\n",
    "X_test = None  # 确保在测试时能够使用到\n",
    "\n",
    "# 定义监控资源使用率的线程函数\n",
    "def monitor_resources_during_save(stop_event):\n",
    "    cpu_usage = []\n",
    "    gpu_usage = []\n",
    "\n",
    "    while not stop_event.is_set():\n",
    "        cpu_usage.append(psutil.cpu_percent(interval=0.1))\n",
    "        try:\n",
    "            gpu_output = subprocess.check_output(['nvidia-smi', \n",
    "                                                  '--query-gpu=utilization.gpu', '--format=csv,noheader,nounits'])\n",
    "            gpu_usage.append(int(gpu_output.strip()))\n",
    "        except Exception as e:\n",
    "            gpu_usage.append(None)  # 如果没有GPU或nvidia-smi命令失败，则记录None\n",
    "\n",
    "    # 保存监测结果\n",
    "    thread_output['cpu_usage'] = cpu_usage\n",
    "    thread_output['gpu_usage'] = gpu_usage\n",
    "    print(\"Resource monitoring finished.\")\n",
    "\n",
    "# 定义保存模型的函数\n",
    "def save_model(stop_event, model):\n",
    "    global duration\n",
    "    global inference_duration\n",
    "    global X_test  # 确保引用全局变量\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # 将TensorFlow Keras模型转换为Core ML模型\n",
    "    for i in range(1):\n",
    "        try:\n",
    "            # 使用实际的输入层名称和适当的形状\n",
    "            input_feature = ct.TensorType(name=\"embedding_input\", shape=(1, 100))\n",
    "            mlmodel = ct.convert(model, inputs=[input_feature])\n",
    "            mlmodel.save(\"imdb_lstm_model.mlpackage\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving model: {e}\")\n",
    "    \n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    print(f'Time taken to save Core ML model: {duration} seconds')\n",
    "    \n",
    "    # 测量推理时间\n",
    "    # 使用 Core ML 加载模型\n",
    "    coreml_model = MLModel(\"imdb_lstm_model.mlpackage\")\n",
    "    \n",
    "    # 准备 Core ML 输入\n",
    "    X_test_sample = X_test[0].astype(np.float32)  # 选择第一个样本进行推理\n",
    "\n",
    "    # 确保形状正确\n",
    "    X_test_sample = X_test_sample.reshape((1, 100))\n",
    "\n",
    "    # 测量推理时间\n",
    "    start_time_inference = time.time()\n",
    "\n",
    "    for i in range(1):\n",
    "        input_data = {\"embedding_input\": X_test_sample}\n",
    "        coreml_out = coreml_model.predict(input_data)\n",
    "\n",
    "    end_time_inference = time.time()\n",
    "    inference_duration = end_time_inference - start_time_inference\n",
    "    print(f'Time taken for inference on 1 sample, repeated 32 times: {inference_duration:.4f} seconds')\n",
    "\n",
    "    stop_event.set()  # 触发停止其他线程\n",
    "\n",
    "# 运行外部脚本并捕获输出\n",
    "def run_script(stop_event):\n",
    "    try:\n",
    "        with tempfile.NamedTemporaryFile(delete=False) as tmp_file:\n",
    "            process = subprocess.Popen(['/Users/anelloyi/Desktop/run_powermetrics.sh'], \n",
    "                                       stdout=tmp_file, stderr=subprocess.STDOUT, text=True)\n",
    "            print(\"Subprocess started.\")\n",
    "            while not stop_event.is_set():\n",
    "                if process.poll() is not None:  # 检查进程是否已经结束\n",
    "                    break\n",
    "            \n",
    "            if process.poll() is None:\n",
    "                process.terminate()\n",
    "                try:\n",
    "                    process.wait(timeout=0.1)\n",
    "                except subprocess.TimeoutExpired:\n",
    "                    process.kill()\n",
    "                    process.wait()\n",
    "        \n",
    "        with open(tmp_file.name, 'r') as f:\n",
    "            thread_output['powermetrics'] = f.read()\n",
    "        \n",
    "        os.remove(tmp_file.name)  # 删除临时文件\n",
    "        print(\"Subprocess finished.\")\n",
    "    except Exception as e:\n",
    "        thread_output['powermetrics'] = str(e)\n",
    "        print(\"Exception in subprocess:\", str(e))\n",
    "\n",
    "# 创建和启动线程\n",
    "stop_event = threading.Event()\n",
    "model = tf.keras.models.load_model(\"./models_train/imdb_lstm_model.h5\") \n",
    "\n",
    "# 确保 X_test 被加载以便进行推理\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=20000)\n",
    "X_test = pad_sequences(X_test, maxlen=100)\n",
    "\n",
    "thread1 = threading.Thread(target=save_model, args=(stop_event, model))\n",
    "thread2 = threading.Thread(target=monitor_resources_during_save, args=(stop_event,))\n",
    "thread3 = threading.Thread(target=run_script, args=(stop_event,))\n",
    "thread1.start()\n",
    "thread2.start()\n",
    "thread3.start()\n",
    "\n",
    "# 等待线程完成\n",
    "thread1.join()\n",
    "thread2.join()\n",
    "thread3.join()\n",
    "\n",
    "# 输出从线程收集的数据\n",
    "content = thread_output.get('powermetrics', 'No output captured')\n",
    "# 将内容按行拆分\n",
    "lines = content.split('\\n')\n",
    "# 筛选出以 \"GPU Power\" 和 \"CPU Power\" 开头的行\n",
    "filtered_lines = [line for line in lines if line.startswith('GPU Power:') or \n",
    "                  line.startswith('CPU Power:')]\n",
    "# 将筛选后的行合并为一个字符串，每行之间用换行符分隔\n",
    "filtered_content = '\\n'.join(filtered_lines)\n",
    "\n",
    "# 保存过滤后的内容到文件\n",
    "output_file_name = './imbd_models/output_lstm/output-imdb-lstm-coreml.txt'\n",
    "with open(output_file_name, 'w') as file:\n",
    "    file.write(filtered_content)\n",
    "    file.write(f'\\nTotal Duration(s): {duration:.2f}')\n",
    "    file.write(f'\\nInference Duration(s): {inference_duration:.4f}')\n",
    "print(f\"Content saved to {output_file_name}\")\n",
    "\n",
    "# 提取每一个采样点的数字，即CPU和GPU的具体mV\n",
    "numbers = []\n",
    "for line in filtered_lines:\n",
    "    match = re.search(r'[\\d.]+', line)\n",
    "    if match:\n",
    "        numbers.append(float(match.group())) \n",
    "\n",
    "delta_time = duration * 2 / len(filtered_lines) if len(filtered_lines) > 0 else 0\n",
    "numbers_scaled = [num * delta_time for num in numbers]\n",
    "total_energy_consumption = sum(numbers_scaled)\n",
    "print(f\"Total energy consumption: {total_energy_consumption:.2f} mV\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "When both 'convert_to' and 'minimum_deployment_target' not specified, 'convert_to' is set to \"mlprogram\" and 'minimum_deployment_target' is set to ct.target.iOS15 (which is same as ct.target.macOS12). Note: the model will not run on systems older than iOS15/macOS12/watchOS8/tvOS15. In order to make your model run on older system, please set the 'minimum_deployment_target' to iOS14/iOS13. Details please see the link: https://apple.github.io/coremltools/docs-guides/source/target-conversion-formats.html\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subprocess started.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running TensorFlow Graph Passes: 100%|██████████| 6/6 [00:01<00:00,  4.25 passes/s]\n",
      "Converting TF Frontend ==> MIL Ops:   0%|          | 0/93 [00:00<?, ? ops/s]Saving value type of int64 into a builtin type of int32, might lose precision!\n",
      "Converting TF Frontend ==> MIL Ops: 100%|██████████| 14/14 [00:00<00:00, 45484.32 ops/s]\n",
      "Input ls elem type unknown. Override with <class 'coremltools.converters.mil.mil.types.type_tensor.tensor.<locals>.tensor'>\n",
      "Converting TF Frontend ==> MIL Ops: 100%|██████████| 40/40 [00:00<00:00, 8169.66 ops/s]\n",
      "Converting TF Frontend ==> MIL Ops: 100%|██████████| 14/14 [00:00<00:00, 54878.74 ops/s]\n",
      "Input ls elem type unknown. Override with <class 'coremltools.converters.mil.mil.types.type_tensor.tensor.<locals>.tensor'>\n",
      "Converting TF Frontend ==> MIL Ops: 100%|██████████| 40/40 [00:00<00:00, 11107.80 ops/s]\n",
      "Saving value type of int64 into a builtin type of int32, might lose precision!\n",
      "Saving value type of int64 into a builtin type of int32, might lose precision!\n",
      "Converting TF Frontend ==> MIL Ops: 100%|██████████| 14/14 [00:00<00:00, 67339.74 ops/s]\n",
      "Input ls elem type unknown. Override with <class 'coremltools.converters.mil.mil.types.type_tensor.tensor.<locals>.tensor'>\n",
      "Converting TF Frontend ==> MIL Ops: 100%|██████████| 41/41 [00:00<00:00, 9524.06 ops/s]\n",
      "Converting TF Frontend ==> MIL Ops: 100%|██████████| 14/14 [00:00<00:00, 33573.62 ops/s]\n",
      "Input ls elem type unknown. Override with <class 'coremltools.converters.mil.mil.types.type_tensor.tensor.<locals>.tensor'>\n",
      "Converting TF Frontend ==> MIL Ops: 100%|██████████| 41/41 [00:00<00:00, 11171.73 ops/s]\n",
      "Saving value type of int64 into a builtin type of int32, might lose precision!\n",
      "Converting TF Frontend ==> MIL Ops: 100%|██████████| 93/93 [00:00<00:00, 1190.49 ops/s]\n",
      "Running MIL frontend_tensorflow2 pipeline: 100%|██████████| 7/7 [00:00<00:00, 86.32 passes/s]\n",
      "Running MIL default pipeline: 100%|██████████| 78/78 [00:00<00:00, 190.26 passes/s]\n",
      "Running MIL backend_mlprogram pipeline: 100%|██████████| 12/12 [00:00<00:00, 582.43 passes/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully.\n",
      "Time taken to save Core ML model: 4.58 seconds\n",
      "Time taken for inference on 1000 samples: 205.5911 seconds\n",
      "Subprocess finished.\n",
      "Resource monitoring finished.\n",
      "Content saved to ./imbd_models/output_lstm/output-imdb-lstm-coreml.txt\n",
      "Total energy consumption: 19788.38 mV\n"
     ]
    }
   ],
   "source": [
    "import coremltools as ct\n",
    "import psutil\n",
    "import subprocess\n",
    "import threading\n",
    "import time\n",
    "import tempfile\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from coremltools.models import MLModel\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.callbacks import LambdaCallback\n",
    "\n",
    "# 初始化全局变量\n",
    "thread_output = {}\n",
    "duration = 0\n",
    "inference_duration = 0\n",
    "\n",
    "# 定义监控资源使用率的线程函数\n",
    "def monitor_resources_during_save(stop_event):\n",
    "    cpu_usage = []\n",
    "    gpu_usage = []\n",
    "\n",
    "    while not stop_event.is_set():\n",
    "        cpu_usage.append(psutil.cpu_percent(interval=0.1))\n",
    "        try:\n",
    "            gpu_output = subprocess.check_output(['nvidia-smi', \n",
    "                                                  '--query-gpu=utilization.gpu', '--format=csv,noheader,nounits'])\n",
    "            gpu_usage.append(int(gpu_output.strip()))\n",
    "        except Exception as e:\n",
    "            gpu_usage.append(None)  # 如果没有GPU或nvidia-smi命令失败，则记录None\n",
    "\n",
    "    # 保存监测结果\n",
    "    thread_output['cpu_usage'] = cpu_usage\n",
    "    thread_output['gpu_usage'] = gpu_usage\n",
    "    print(\"Resource monitoring finished.\")\n",
    "\n",
    "# # 定义保存模型的函数\n",
    "# def save_model(stop_event, model):\n",
    "#     global duration\n",
    "#     global inference_duration\n",
    "#     global X_test  # 确保引用全局变量\n",
    "\n",
    "#     start_time = time.time()\n",
    "    \n",
    "#     # 将TensorFlow Keras模型转换为Core ML模型\n",
    "#     try:\n",
    "#         # 使用实际的输入层名称和适当的形状\n",
    "#         input_feature = ct.TensorType(name=\"embedding_input\", shape=(1, 100))\n",
    "#         mlmodel = ct.convert(model, inputs=[input_feature])\n",
    "#         mlmodel.save(\"imdb_lstm_model.mlpackage\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error saving model: {e}\")\n",
    "    \n",
    "#     end_time = time.time()\n",
    "#     duration = end_time - start_time\n",
    "#     print(f'Time taken to save Core ML model: {duration} seconds')\n",
    "    \n",
    "#     # 测量推理时间\n",
    "#     # 使用 Core ML 加载模型\n",
    "#     coreml_model = MLModel(\"imdb_lstm_model.mlpackage\")\n",
    "    \n",
    "#     # 准备 Core ML 输入\n",
    "#     X_test_sample = X_test[:1000].astype(np.float32)  # 选择前1000个样本进行推理\n",
    "\n",
    "#     # 批量推理部分，设置 batch size 为 32\n",
    "#     batch_size = 1\n",
    "#     num_samples = X_test_sample.shape[0]\n",
    "#     num_batches = (num_samples + batch_size - 1) // batch_size\n",
    "\n",
    "#     start_time_inference = time.time()\n",
    "\n",
    "#     # 执行批量推理\n",
    "#     for batch_idx in range(num_batches):\n",
    "#         batch_start = batch_idx * batch_size\n",
    "#         batch_end = min(batch_start + batch_size, num_samples)\n",
    "#         batch = X_test_sample[batch_start:batch_end].reshape((batch_end - batch_start, 100))\n",
    "#         input_data = {\"embedding_input\": batch}\n",
    "#         coreml_out = coreml_model.predict(input_data)\n",
    "\n",
    "#     end_time_inference = time.time()\n",
    "#     inference_duration = end_time_inference - start_time_inference\n",
    "#     print(f'Time taken for inference on {num_samples} samples: {inference_duration:.4f} seconds')\n",
    "\n",
    "#     stop_event.set()  # 触发停止其他线程\n",
    "\n",
    "def save_model(stop_event, model):\n",
    "    global duration\n",
    "    global inference_duration\n",
    "    global X_test  # 确保引用全局变量\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # 将 TensorFlow Keras 模型转换为 Core ML 模型时，确保输入尺寸正确\n",
    "    try:\n",
    "        # 如果模型确实需要处理变长输入，可以尝试指定具体的批量大小\n",
    "        input_feature = ct.TensorType(name=\"embedding_input\", shape=(1, 100))\n",
    "        mlmodel = ct.convert(model, inputs=[input_feature], source='tensorflow')\n",
    "        mlmodel.save(\"imdb_lstm_model.mlpackage\")\n",
    "        print(\"Model saved successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving model: {e}\")\n",
    "\n",
    "\n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    print(f\"Time taken to save Core ML model: {duration:.2f} seconds\")\n",
    "\n",
    "    \n",
    "    # 测量推理时间\n",
    "    # 使用 Core ML 加载模型\n",
    "    coreml_model = MLModel(\"imdb_lstm_model.mlpackage\")\n",
    "    \n",
    "    # 准备 Core ML 输入，这里假设X_test已经被适当地预处理为序列\n",
    "    X_test_sample = X_test[:1000].astype(np.float32)  # 选择前1000个样本进行推理\n",
    "\n",
    "    # 批量推理部分，设置 batch size 为 32\n",
    "    batch_size = 128\n",
    "    num_samples = X_test_sample.shape[0]\n",
    "    num_batches = (num_samples + batch_size - 1) // batch_size\n",
    "\n",
    "    start_time_inference = time.time()\n",
    "\n",
    "    # 执行批量推理\n",
    "    predictions = []\n",
    "    for batch_idx in range(num_batches):\n",
    "        batch_start = batch_idx * batch_size\n",
    "        batch_end = min(batch_start + batch_size, num_samples)\n",
    "        batch = X_test_sample[batch_start:batch_end]\n",
    "        for sample in batch:  # 处理每个样本，因为模型只接受单个样本\n",
    "            input_data = {\"embedding_input\": sample.reshape(1, 100)}  # 重塑每个样本为 (1, 100)\n",
    "            try:\n",
    "                batch_predictions = coreml_model.predict(input_data)\n",
    "                predictions.append(batch_predictions)\n",
    "            except Exception as e:\n",
    "                print(f\"Error during batch inference {batch_idx}: {e}\")\n",
    "\n",
    "\n",
    "    end_time_inference = time.time()\n",
    "    inference_duration = end_time_inference - start_time_inference\n",
    "    print(f'Time taken for inference on {num_samples} samples: {inference_duration:.4f} seconds')\n",
    "\n",
    "    stop_event.set()  # 触发停止其他线程\n",
    "\n",
    "\n",
    "# 运行外部脚本并捕获输出\n",
    "def run_script(stop_event):\n",
    "    try:\n",
    "        with tempfile.NamedTemporaryFile(delete=False) as tmp_file:\n",
    "            process = subprocess.Popen(['/Users/anelloyi/Desktop/run_powermetrics.sh'], \n",
    "                                       stdout=tmp_file, stderr=subprocess.STDOUT, text=True)\n",
    "            print(\"Subprocess started.\")\n",
    "            while not stop_event.is_set():\n",
    "                if process.poll() is not None:  # 检查进程是否已经结束\n",
    "                    break\n",
    "            \n",
    "            if process.poll() is None:\n",
    "                process.terminate()\n",
    "                try:\n",
    "                    process.wait(timeout=0.1)\n",
    "                except subprocess.TimeoutExpired:\n",
    "                    process.kill()\n",
    "                    process.wait()\n",
    "        \n",
    "        with open(tmp_file.name, 'r') as f:\n",
    "            thread_output['powermetrics'] = f.read()\n",
    "        \n",
    "        os.remove(tmp_file.name)  # 删除临时文件\n",
    "        print(\"Subprocess finished.\")\n",
    "    except Exception as e:\n",
    "        thread_output['powermetrics'] = str(e)\n",
    "        print(\"Exception in subprocess:\", str(e))\n",
    "\n",
    "# 创建和启动线程\n",
    "stop_event = threading.Event()\n",
    "model = tf.keras.models.load_model(\"./models_train/imdb_lstm_model.h5\")\n",
    "\n",
    "# 确保 X_test 被加载以便进行推理\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=20000)\n",
    "X_test = pad_sequences(X_test, maxlen=100)\n",
    "\n",
    "thread1 = threading.Thread(target=save_model, args=(stop_event, model))\n",
    "thread2 = threading.Thread(target=monitor_resources_during_save, args=(stop_event,))\n",
    "thread3 = threading.Thread(target=run_script, args=(stop_event,))\n",
    "thread1.start()\n",
    "thread2.start()\n",
    "thread3.start()\n",
    "\n",
    "# 等待线程完成\n",
    "thread1.join()\n",
    "thread2.join()\n",
    "thread3.join()\n",
    "\n",
    "# 输出从线程收集的数据\n",
    "content = thread_output.get('powermetrics', 'No output captured')\n",
    "# 将内容按行拆分\n",
    "lines = content.split('\\n')\n",
    "# 筛选出以 \"GPU Power\" 和 \"CPU Power\" 开头的行\n",
    "filtered_lines = [line for line in lines if line.startswith('GPU Power:') or \n",
    "                  line.startswith('CPU Power:')]\n",
    "# 将筛选后的行合并为一个字符串，每行之间用换行符分隔\n",
    "filtered_content = '\\n'.join(filtered_lines)\n",
    "\n",
    "# 保存过滤后的内容到文件\n",
    "output_file_name = './imbd_models/output_lstm/output-imdb-lstm-coreml.txt'\n",
    "with open(output_file_name, 'w') as file:\n",
    "    file.write(filtered_content)\n",
    "    file.write(f'\\nTotal Duration(s): {duration:.2f}')\n",
    "    file.write(f'\\nInference Duration(s): {inference_duration:.4f}')\n",
    "print(f\"Content saved to {output_file_name}\")\n",
    "\n",
    "# 提取每一个采样点的数字，即CPU和GPU的具体mV\n",
    "numbers = []\n",
    "for line in filtered_lines:\n",
    "    match = re.search(r'[\\d.]+', line)\n",
    "    if match:\n",
    "        numbers.append(float(match.group())) \n",
    "\n",
    "delta_time = duration * 2 / len(filtered_lines) if len(filtered_lines) > 0 else 0\n",
    "numbers_scaled = [num * delta_time for num in numbers]\n",
    "total_energy_consumption = sum(numbers_scaled)\n",
    "print(f\"Total energy consumption: {total_energy_consumption:.2f} mV\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lstm-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载 IMDB 数据集 （确保不重新训练模型也能推理）\n",
    "max_features = 20000  # 使用的单词数量\n",
    "maxlen = 100  # 每条评论的最大长度\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "\n",
    "# 数据预处理和填充\n",
    "X_train = pad_sequences(X_train, maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import h5py\n",
    "\n",
    "# 用pytorch定义LSTM模型\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm1 = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(hidden_dim, hidden_dim, batch_first=True)\n",
    "        self.fc1 = nn.Linear(hidden_dim, 64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(64, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, _ = self.lstm1(x)\n",
    "        x, (h_n, c_n) = self.lstm2(x)  # 仅获取最后一个时间步的输出\n",
    "        x = h_n[-1]  # 取最后一个层的隐藏状态\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "def save_keras_lstm_to_pytorch(modelpath):\n",
    "    # 加载已经训练的TensorFlow模型，保存参数\n",
    "    model = tf.keras.models.load_model(modelpath)\n",
    "    model.save_weights('imdb_lstm_weights.h5')\n",
    "\n",
    "    # 加载 Keras 权重\n",
    "    # 打开 HDF5 文件\n",
    "    weights = h5py.File('imdb_lstm_weights.h5', 'r')\n",
    "\n",
    "    # 创建 PyTorch 模型实例\n",
    "    pytorch_model = LSTMModel(max_features, 128, 128)\n",
    "\n",
    "    # 加载 Embedding 层权重\n",
    "    pytorch_model.embedding.weight.data.copy_(torch.from_numpy(weights['embedding/embedding/embeddings:0'][()]))\n",
    "\n",
    "    # 加载第一个 LSTM 层权重\n",
    "    # LSTM层包括kernel（即weights_ih）和recurrent_kernel（即weights_hh）以及bias\n",
    "    pytorch_model.lstm1.weight_ih_l0.data.copy_(torch.from_numpy(weights['lstm/lstm/lstm_cell/kernel:0'][()].transpose()))\n",
    "    pytorch_model.lstm1.weight_hh_l0.data.copy_(torch.from_numpy(weights['lstm/lstm/lstm_cell/recurrent_kernel:0'][()].transpose()))\n",
    "    pytorch_model.lstm1.bias_ih_l0.data.copy_(torch.from_numpy(weights['lstm/lstm/lstm_cell/bias:0'][()]))\n",
    "    pytorch_model.lstm1.bias_hh_l0.data.fill_(0)  # 清零，因为Keras中的偏置是合并的\n",
    "\n",
    "    # 加载第二个 LSTM 层权重\n",
    "    pytorch_model.lstm2.weight_ih_l0.data.copy_(torch.from_numpy(weights['lstm_1/lstm_1/lstm_cell_1/kernel:0'][()].transpose()))\n",
    "    pytorch_model.lstm2.weight_hh_l0.data.copy_(torch.from_numpy(weights['lstm_1/lstm_1/lstm_cell_1/recurrent_kernel:0'][()].transpose()))\n",
    "    pytorch_model.lstm2.bias_ih_l0.data.copy_(torch.from_numpy(weights['lstm_1/lstm_1/lstm_cell_1/bias:0'][()]))\n",
    "    pytorch_model.lstm2.bias_hh_l0.data.fill_(0)  # 清零\n",
    "\n",
    "    # 加载第一个 Dense 层权重和偏置\n",
    "    pytorch_model.fc1.weight.data.copy_(torch.from_numpy(weights['dense/dense/kernel:0'][()].transpose()))\n",
    "    pytorch_model.fc1.bias.data.copy_(torch.from_numpy(weights['dense/dense/bias:0'][()]))\n",
    "\n",
    "    # 加载第二个 Dense 层权重和偏置\n",
    "    pytorch_model.fc2.weight.data.copy_(torch.from_numpy(weights['dense_1/dense_1/kernel:0'][()].transpose()))\n",
    "    pytorch_model.fc2.bias.data.copy_(torch.from_numpy(weights['dense_1/dense_1/bias:0'][()]))\n",
    "\n",
    "    weights.close()  # 关闭文件\n",
    "\n",
    "    # 现在你的 PyTorch 模型已经加载了 Keras 模型的所有权重\n",
    "\n",
    "    return pytorch_model\n",
    "\n",
    "    # 将模型设置为评估模式\n",
    "\n",
    "# pytorch_model = save_keras_lstm_to_pytorch(\"./models_train/imdb_lstm_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model...\n",
      "Subprocess started.\n",
      "Time taken to save PyTorch model: 3.17 seconds\n",
      "Inference output: tensor([[0.0524],\n",
      "        [0.9960],\n",
      "        [0.9824],\n",
      "        [0.1588],\n",
      "        [0.9983],\n",
      "        [0.9150],\n",
      "        [0.9923],\n",
      "        [0.0162],\n",
      "        [0.7153],\n",
      "        [0.9792]])\n",
      "Inference output: tensor([[0.0524],\n",
      "        [0.9960],\n",
      "        [0.9824],\n",
      "        [0.1588],\n",
      "        [0.9983],\n",
      "        [0.9150],\n",
      "        [0.9923],\n",
      "        [0.0162],\n",
      "        [0.7153],\n",
      "        [0.9792]])\n",
      "Inference output: tensor([[0.0524],\n",
      "        [0.9960],\n",
      "        [0.9824],\n",
      "        [0.1588],\n",
      "        [0.9983],\n",
      "        [0.9150],\n",
      "        [0.9923],\n",
      "        [0.0162],\n",
      "        [0.7153],\n",
      "        [0.9792]])\n",
      "Inference output: tensor([[0.0524],\n",
      "        [0.9960],\n",
      "        [0.9824],\n",
      "        [0.1588],\n",
      "        [0.9983],\n",
      "        [0.9150],\n",
      "        [0.9923],\n",
      "        [0.0162],\n",
      "        [0.7153],\n",
      "        [0.9792]])\n",
      "Inference output: tensor([[0.0524],\n",
      "        [0.9960],\n",
      "        [0.9824],\n",
      "        [0.1588],\n",
      "        [0.9983],\n",
      "        [0.9150],\n",
      "        [0.9923],\n",
      "        [0.0162],\n",
      "        [0.7153],\n",
      "        [0.9792]])\n",
      "Inference output: tensor([[0.0524],\n",
      "        [0.9960],\n",
      "        [0.9824],\n",
      "        [0.1588],\n",
      "        [0.9983],\n",
      "        [0.9150],\n",
      "        [0.9923],\n",
      "        [0.0162],\n",
      "        [0.7153],\n",
      "        [0.9792]])\n",
      "Inference output: tensor([[0.0524],\n",
      "        [0.9960],\n",
      "        [0.9824],\n",
      "        [0.1588],\n",
      "        [0.9983],\n",
      "        [0.9150],\n",
      "        [0.9923],\n",
      "        [0.0162],\n",
      "        [0.7153],\n",
      "        [0.9792]])\n",
      "Inference output: tensor([[0.0524],\n",
      "        [0.9960],\n",
      "        [0.9824],\n",
      "        [0.1588],\n",
      "        [0.9983],\n",
      "        [0.9150],\n",
      "        [0.9923],\n",
      "        [0.0162],\n",
      "        [0.7153],\n",
      "        [0.9792]])\n",
      "Inference output: tensor([[0.0524],\n",
      "        [0.9960],\n",
      "        [0.9824],\n",
      "        [0.1588],\n",
      "        [0.9983],\n",
      "        [0.9150],\n",
      "        [0.9923],\n",
      "        [0.0162],\n",
      "        [0.7153],\n",
      "        [0.9792]])\n",
      "Inference output: tensor([[0.0524],\n",
      "        [0.9960],\n",
      "        [0.9824],\n",
      "        [0.1588],\n",
      "        [0.9983],\n",
      "        [0.9150],\n",
      "        [0.9923],\n",
      "        [0.0162],\n",
      "        [0.7153],\n",
      "        [0.9792]])\n",
      "Inference output: tensor([[0.0524],\n",
      "        [0.9960],\n",
      "        [0.9824],\n",
      "        [0.1588],\n",
      "        [0.9983],\n",
      "        [0.9150],\n",
      "        [0.9923],\n",
      "        [0.0162],\n",
      "        [0.7153],\n",
      "        [0.9792]])\n",
      "Inference output: tensor([[0.0524],\n",
      "        [0.9960],\n",
      "        [0.9824],\n",
      "        [0.1588],\n",
      "        [0.9983],\n",
      "        [0.9150],\n",
      "        [0.9923],\n",
      "        [0.0162],\n",
      "        [0.7153],\n",
      "        [0.9792]])\n",
      "Inference output: tensor([[0.0524],\n",
      "        [0.9960],\n",
      "        [0.9824],\n",
      "        [0.1588],\n",
      "        [0.9983],\n",
      "        [0.9150],\n",
      "        [0.9923],\n",
      "        [0.0162],\n",
      "        [0.7153],\n",
      "        [0.9792]])\n",
      "Inference output: tensor([[0.0524],\n",
      "        [0.9960],\n",
      "        [0.9824],\n",
      "        [0.1588],\n",
      "        [0.9983],\n",
      "        [0.9150],\n",
      "        [0.9923],\n",
      "        [0.0162],\n",
      "        [0.7153],\n",
      "        [0.9792]])\n",
      "Inference output: tensor([[0.0524],\n",
      "        [0.9960],\n",
      "        [0.9824],\n",
      "        [0.1588],\n",
      "        [0.9983],\n",
      "        [0.9150],\n",
      "        [0.9923],\n",
      "        [0.0162],\n",
      "        [0.7153],\n",
      "        [0.9792]])\n",
      "Inference output: tensor([[0.0524],\n",
      "        [0.9960],\n",
      "        [0.9824],\n",
      "        [0.1588],\n",
      "        [0.9983],\n",
      "        [0.9150],\n",
      "        [0.9923],\n",
      "        [0.0162],\n",
      "        [0.7153],\n",
      "        [0.9792]])\n",
      "Inference output: tensor([[0.0524],\n",
      "        [0.9960],\n",
      "        [0.9824],\n",
      "        [0.1588],\n",
      "        [0.9983],\n",
      "        [0.9150],\n",
      "        [0.9923],\n",
      "        [0.0162],\n",
      "        [0.7153],\n",
      "        [0.9792]])\n",
      "Inference output: tensor([[0.0524],\n",
      "        [0.9960],\n",
      "        [0.9824],\n",
      "        [0.1588],\n",
      "        [0.9983],\n",
      "        [0.9150],\n",
      "        [0.9923],\n",
      "        [0.0162],\n",
      "        [0.7153],\n",
      "        [0.9792]])\n",
      "Inference output: tensor([[0.0524],\n",
      "        [0.9960],\n",
      "        [0.9824],\n",
      "        [0.1588],\n",
      "        [0.9983],\n",
      "        [0.9150],\n",
      "        [0.9923],\n",
      "        [0.0162],\n",
      "        [0.7153],\n",
      "        [0.9792]])\n",
      "Inference output: tensor([[0.0524],\n",
      "        [0.9960],\n",
      "        [0.9824],\n",
      "        [0.1588],\n",
      "        [0.9983],\n",
      "        [0.9150],\n",
      "        [0.9923],\n",
      "        [0.0162],\n",
      "        [0.7153],\n",
      "        [0.9792]])\n",
      "Inference output: tensor([[0.0524],\n",
      "        [0.9960],\n",
      "        [0.9824],\n",
      "        [0.1588],\n",
      "        [0.9983],\n",
      "        [0.9150],\n",
      "        [0.9923],\n",
      "        [0.0162],\n",
      "        [0.7153],\n",
      "        [0.9792]])\n",
      "Inference output: tensor([[0.0524],\n",
      "        [0.9960],\n",
      "        [0.9824],\n",
      "        [0.1588],\n",
      "        [0.9983],\n",
      "        [0.9150],\n",
      "        [0.9923],\n",
      "        [0.0162],\n",
      "        [0.7153],\n",
      "        [0.9792]])\n",
      "Inference output: tensor([[0.0524],\n",
      "        [0.9960],\n",
      "        [0.9824],\n",
      "        [0.1588],\n",
      "        [0.9983],\n",
      "        [0.9150],\n",
      "        [0.9923],\n",
      "        [0.0162],\n",
      "        [0.7153],\n",
      "        [0.9792]])\n",
      "Inference output: tensor([[0.0524],\n",
      "        [0.9960],\n",
      "        [0.9824],\n",
      "        [0.1588],\n",
      "        [0.9983],\n",
      "        [0.9150],\n",
      "        [0.9923],\n",
      "        [0.0162],\n",
      "        [0.7153],\n",
      "        [0.9792]])\n",
      "Inference output: tensor([[0.0524],\n",
      "        [0.9960],\n",
      "        [0.9824],\n",
      "        [0.1588],\n",
      "        [0.9983],\n",
      "        [0.9150],\n",
      "        [0.9923],\n",
      "        [0.0162],\n",
      "        [0.7153],\n",
      "        [0.9792]])\n",
      "Inference output: tensor([[0.0524],\n",
      "        [0.9960],\n",
      "        [0.9824],\n",
      "        [0.1588],\n",
      "        [0.9983],\n",
      "        [0.9150],\n",
      "        [0.9923],\n",
      "        [0.0162],\n",
      "        [0.7153],\n",
      "        [0.9792]])\n",
      "Inference output: tensor([[0.0524],\n",
      "        [0.9960],\n",
      "        [0.9824],\n",
      "        [0.1588],\n",
      "        [0.9983],\n",
      "        [0.9150],\n",
      "        [0.9923],\n",
      "        [0.0162],\n",
      "        [0.7153],\n",
      "        [0.9792]])\n",
      "Inference output: tensor([[0.0524],\n",
      "        [0.9960],\n",
      "        [0.9824],\n",
      "        [0.1588],\n",
      "        [0.9983],\n",
      "        [0.9150],\n",
      "        [0.9923],\n",
      "        [0.0162],\n",
      "        [0.7153],\n",
      "        [0.9792]])\n",
      "Inference output: tensor([[0.0524],\n",
      "        [0.9960],\n",
      "        [0.9824],\n",
      "        [0.1588],\n",
      "        [0.9983],\n",
      "        [0.9150],\n",
      "        [0.9923],\n",
      "        [0.0162],\n",
      "        [0.7153],\n",
      "        [0.9792]])\n",
      "Inference output: tensor([[0.0524],\n",
      "        [0.9960],\n",
      "        [0.9824],\n",
      "        [0.1588],\n",
      "        [0.9983],\n",
      "        [0.9150],\n",
      "        [0.9923],\n",
      "        [0.0162],\n",
      "        [0.7153],\n",
      "        [0.9792]])\n",
      "Inference output: tensor([[0.0524],\n",
      "        [0.9960],\n",
      "        [0.9824],\n",
      "        [0.1588],\n",
      "        [0.9983],\n",
      "        [0.9150],\n",
      "        [0.9923],\n",
      "        [0.0162],\n",
      "        [0.7153],\n",
      "        [0.9792]])\n",
      "Inference output: tensor([[0.0524],\n",
      "        [0.9960],\n",
      "        [0.9824],\n",
      "        [0.1588],\n",
      "        [0.9983],\n",
      "        [0.9150],\n",
      "        [0.9923],\n",
      "        [0.0162],\n",
      "        [0.7153],\n",
      "        [0.9792]])\n",
      "Time taken for inference on 10 samples: 3.3513 seconds\n",
      "Subprocess finished.\n",
      "Resource monitoring finished.\n",
      "Content saved to ./imbd_models/output_lstm/output-imdb-lstm-pth.txt\n",
      "616\n",
      "['CPU Power: 8595 mW', 'GPU Power: 56 mW', 'CPU Power: 5465 mW', 'GPU Power: 45 mW', 'CPU Power: 6216 mW', 'GPU Power: 91 mW', 'CPU Power: 4674 mW', 'GPU Power: 45 mW', 'CPU Power: 4816 mW', 'GPU Power: 83 mW', 'CPU Power: 3777 mW', 'GPU Power: 39 mW', 'CPU Power: 5657 mW', 'GPU Power: 41 mW', 'CPU Power: 6945 mW', 'GPU Power: 100 mW', 'CPU Power: 7088 mW', 'GPU Power: 47 mW', 'CPU Power: 7156 mW', 'GPU Power: 45 mW', 'CPU Power: 6316 mW', 'GPU Power: 138 mW', 'CPU Power: 5434 mW', 'GPU Power: 254 mW', 'CPU Power: 5464 mW', 'GPU Power: 126 mW', 'CPU Power: 6594 mW', 'GPU Power: 177 mW', 'CPU Power: 6454 mW', 'GPU Power: 202 mW', 'CPU Power: 5253 mW', 'GPU Power: 86 mW', 'CPU Power: 4972 mW', 'GPU Power: 85 mW', 'CPU Power: 6230 mW', 'GPU Power: 87 mW', 'CPU Power: 6804 mW', 'GPU Power: 258 mW', 'CPU Power: 5215 mW', 'GPU Power: 42 mW', 'CPU Power: 6654 mW', 'GPU Power: 47 mW', 'CPU Power: 6436 mW', 'GPU Power: 47 mW', 'CPU Power: 3912 mW', 'GPU Power: 94 mW', 'CPU Power: 4333 mW', 'GPU Power: 46 mW', 'CPU Power: 4223 mW', 'GPU Power: 47 mW', 'CPU Power: 5634 mW', 'GPU Power: 136 mW', 'CPU Power: 7701 mW', 'GPU Power: 510 mW', 'CPU Power: 5105 mW', 'GPU Power: 255 mW', 'CPU Power: 7258 mW', 'GPU Power: 105 mW', 'CPU Power: 6914 mW', 'GPU Power: 46 mW', 'CPU Power: 7499 mW', 'GPU Power: 311 mW', 'CPU Power: 6695 mW', 'GPU Power: 95 mW', 'CPU Power: 4304 mW', 'GPU Power: 47 mW', 'CPU Power: 5111 mW', 'GPU Power: 48 mW', 'CPU Power: 4610 mW', 'GPU Power: 99 mW', 'CPU Power: 3807 mW', 'GPU Power: 48 mW', 'CPU Power: 5096 mW', 'GPU Power: 64 mW', 'CPU Power: 4494 mW', 'GPU Power: 57 mW', 'CPU Power: 6320 mW', 'GPU Power: 44 mW', 'CPU Power: 5799 mW', 'GPU Power: 85 mW', 'CPU Power: 5865 mW', 'GPU Power: 43 mW', 'CPU Power: 6903 mW', 'GPU Power: 44 mW', 'CPU Power: 5776 mW', 'GPU Power: 51 mW', 'CPU Power: 4719 mW', 'GPU Power: 58 mW', 'CPU Power: 3739 mW', 'GPU Power: 61 mW', 'CPU Power: 5535 mW', 'GPU Power: 61 mW', 'CPU Power: 7672 mW', 'GPU Power: 58 mW', 'CPU Power: 6829 mW', 'GPU Power: 204 mW', 'CPU Power: 8841 mW', 'GPU Power: 323 mW', 'CPU Power: 4590 mW', 'GPU Power: 262 mW', 'CPU Power: 6368 mW', 'GPU Power: 259 mW', 'CPU Power: 6286 mW', 'GPU Power: 277 mW', 'CPU Power: 7888 mW', 'GPU Power: 194 mW', 'CPU Power: 7046 mW', 'GPU Power: 233 mW', 'CPU Power: 5325 mW', 'GPU Power: 245 mW', 'CPU Power: 4820 mW', 'GPU Power: 94 mW', 'CPU Power: 3586 mW', 'GPU Power: 85 mW', 'CPU Power: 6151 mW', 'GPU Power: 43 mW', 'CPU Power: 4183 mW', 'GPU Power: 141 mW', 'CPU Power: 4293 mW', 'GPU Power: 106 mW', 'CPU Power: 6440 mW', 'GPU Power: 118 mW', 'CPU Power: 5087 mW', 'GPU Power: 64 mW', 'CPU Power: 5304 mW', 'GPU Power: 54 mW', 'CPU Power: 6492 mW', 'GPU Power: 101 mW', 'CPU Power: 6278 mW', 'GPU Power: 45 mW', 'CPU Power: 4910 mW', 'GPU Power: 106 mW', 'CPU Power: 3515 mW', 'GPU Power: 55 mW', 'CPU Power: 4952 mW', 'GPU Power: 59 mW', 'CPU Power: 4253 mW', 'GPU Power: 59 mW', 'CPU Power: 5170 mW', 'GPU Power: 56 mW', 'CPU Power: 4511 mW', 'GPU Power: 53 mW', 'CPU Power: 3697 mW', 'GPU Power: 103 mW', 'CPU Power: 6832 mW', 'GPU Power: 56 mW', 'CPU Power: 7521 mW', 'GPU Power: 118 mW', 'CPU Power: 6657 mW', 'GPU Power: 54 mW', 'CPU Power: 4191 mW', 'GPU Power: 52 mW', 'CPU Power: 5019 mW', 'GPU Power: 102 mW', 'CPU Power: 5460 mW', 'GPU Power: 54 mW', 'CPU Power: 3656 mW', 'GPU Power: 57 mW', 'CPU Power: 4943 mW', 'GPU Power: 55 mW', 'CPU Power: 3991 mW', 'GPU Power: 57 mW', 'CPU Power: 5278 mW', 'GPU Power: 112 mW', 'CPU Power: 3491 mW', 'GPU Power: 150 mW', 'CPU Power: 4353 mW', 'GPU Power: 51 mW', 'CPU Power: 4833 mW', 'GPU Power: 54 mW', 'CPU Power: 6299 mW', 'GPU Power: 58 mW', 'CPU Power: 6002 mW', 'GPU Power: 61 mW', 'CPU Power: 6072 mW', 'GPU Power: 52 mW', 'CPU Power: 5468 mW', 'GPU Power: 47 mW', 'CPU Power: 3618 mW', 'GPU Power: 55 mW', 'CPU Power: 4478 mW', 'GPU Power: 105 mW', 'CPU Power: 5011 mW', 'GPU Power: 57 mW', 'CPU Power: 5098 mW', 'GPU Power: 55 mW', 'CPU Power: 5252 mW', 'GPU Power: 114 mW', 'CPU Power: 4490 mW', 'GPU Power: 53 mW', 'CPU Power: 5152 mW', 'GPU Power: 0 mW', 'CPU Power: 7181 mW', 'GPU Power: 56 mW', 'CPU Power: 7201 mW', 'GPU Power: 61 mW', 'CPU Power: 7628 mW', 'GPU Power: 62 mW', 'CPU Power: 6981 mW', 'GPU Power: 121 mW', 'CPU Power: 6706 mW', 'GPU Power: 49 mW', 'CPU Power: 5574 mW', 'GPU Power: 51 mW', 'CPU Power: 4102 mW', 'GPU Power: 55 mW', 'CPU Power: 6074 mW', 'GPU Power: 60 mW', 'CPU Power: 5737 mW', 'GPU Power: 61 mW', 'CPU Power: 6697 mW', 'GPU Power: 166 mW', 'CPU Power: 6926 mW', 'GPU Power: 49 mW', 'CPU Power: 9348 mW', 'GPU Power: 44 mW', 'CPU Power: 8513 mW', 'GPU Power: 85 mW', 'CPU Power: 10979 mW', 'GPU Power: 43 mW', 'CPU Power: 8783 mW', 'GPU Power: 87 mW', 'CPU Power: 8773 mW', 'GPU Power: 49 mW', 'CPU Power: 8004 mW', 'GPU Power: 54 mW', 'CPU Power: 10775 mW', 'GPU Power: 125 mW', 'CPU Power: 10126 mW', 'GPU Power: 114 mW', 'CPU Power: 3716 mW', 'GPU Power: 49 mW', 'CPU Power: 4119 mW', 'GPU Power: 49 mW', 'CPU Power: 3556 mW', 'GPU Power: 49 mW', 'CPU Power: 4253 mW', 'GPU Power: 49 mW', 'CPU Power: 4735 mW', 'GPU Power: 160 mW', 'CPU Power: 5070 mW', 'GPU Power: 61 mW', 'CPU Power: 6330 mW', 'GPU Power: 60 mW', 'CPU Power: 6266 mW', 'GPU Power: 52 mW', 'CPU Power: 10661 mW', 'GPU Power: 46 mW', 'CPU Power: 11757 mW', 'GPU Power: 50 mW', 'CPU Power: 5095 mW', 'GPU Power: 147 mW', 'CPU Power: 4384 mW', 'GPU Power: 47 mW', 'CPU Power: 3459 mW', 'GPU Power: 47 mW', 'CPU Power: 5288 mW', 'GPU Power: 49 mW', 'CPU Power: 3915 mW', 'GPU Power: 52 mW', 'CPU Power: 5348 mW', 'GPU Power: 126 mW', 'CPU Power: 6073 mW', 'GPU Power: 110 mW', 'CPU Power: 10241 mW', 'GPU Power: 52 mW', 'CPU Power: 10304 mW', 'GPU Power: 55 mW', 'CPU Power: 11162 mW', 'GPU Power: 53 mW', 'CPU Power: 7738 mW', 'GPU Power: 55 mW', 'CPU Power: 10853 mW', 'GPU Power: 62 mW', 'CPU Power: 11497 mW', 'GPU Power: 61 mW', 'CPU Power: 7720 mW', 'GPU Power: 60 mW', 'CPU Power: 6396 mW', 'GPU Power: 56 mW', 'CPU Power: 4701 mW', 'GPU Power: 51 mW', 'CPU Power: 6014 mW', 'GPU Power: 102 mW', 'CPU Power: 5242 mW', 'GPU Power: 62 mW', 'CPU Power: 8195 mW', 'GPU Power: 57 mW', 'CPU Power: 7164 mW', 'GPU Power: 105 mW', 'CPU Power: 13561 mW', 'GPU Power: 54 mW', 'CPU Power: 11380 mW', 'GPU Power: 97 mW', 'CPU Power: 11189 mW', 'GPU Power: 56 mW', 'CPU Power: 10317 mW', 'GPU Power: 112 mW', 'CPU Power: 4195 mW', 'GPU Power: 166 mW', 'CPU Power: 6176 mW', 'GPU Power: 55 mW', 'CPU Power: 4226 mW', 'GPU Power: 56 mW', 'CPU Power: 6440 mW', 'GPU Power: 115 mW', 'CPU Power: 4236 mW', 'GPU Power: 54 mW', 'CPU Power: 5760 mW', 'GPU Power: 54 mW', 'CPU Power: 5274 mW', 'GPU Power: 105 mW', 'CPU Power: 10716 mW', 'GPU Power: 59 mW', 'CPU Power: 11612 mW', 'GPU Power: 122 mW', 'CPU Power: 12298 mW', 'GPU Power: 54 mW', 'CPU Power: 8259 mW', 'GPU Power: 53 mW', 'CPU Power: 3340 mW', 'GPU Power: 53 mW', 'CPU Power: 4706 mW', 'GPU Power: 54 mW', 'CPU Power: 4472 mW', 'GPU Power: 62 mW', 'CPU Power: 5195 mW', 'GPU Power: 122 mW', 'CPU Power: 3852 mW', 'GPU Power: 61 mW', 'CPU Power: 5184 mW', 'GPU Power: 59 mW', 'CPU Power: 3642 mW', 'GPU Power: 54 mW', 'CPU Power: 8429 mW', 'GPU Power: 53 mW', 'CPU Power: 9827 mW', 'GPU Power: 53 mW', 'CPU Power: 11430 mW', 'GPU Power: 126 mW', 'CPU Power: 12302 mW', 'GPU Power: 195 mW', 'CPU Power: 4162 mW', 'GPU Power: 248 mW', 'CPU Power: 4741 mW', 'GPU Power: 53 mW', 'CPU Power: 3852 mW', 'GPU Power: 45 mW', 'CPU Power: 4180 mW', 'GPU Power: 92 mW', 'CPU Power: 5368 mW', 'GPU Power: 48 mW', 'CPU Power: 3439 mW', 'GPU Power: 104 mW', 'CPU Power: 7305 mW', 'GPU Power: 55 mW', 'CPU Power: 9940 mW', 'GPU Power: 54 mW', 'CPU Power: 9359 mW', 'GPU Power: 119 mW', 'CPU Power: 11948 mW', 'GPU Power: 61 mW', 'CPU Power: 11871 mW', 'GPU Power: 172 mW', 'CPU Power: 10074 mW', 'GPU Power: 217 mW', 'CPU Power: 4702 mW', 'GPU Power: 52 mW', 'CPU Power: 4867 mW', 'GPU Power: 52 mW', 'CPU Power: 5627 mW', 'GPU Power: 155 mW', 'CPU Power: 4394 mW', 'GPU Power: 50 mW', 'CPU Power: 4458 mW', 'GPU Power: 47 mW', 'CPU Power: 4282 mW', 'GPU Power: 45 mW', 'CPU Power: 3819 mW', 'GPU Power: 98 mW', 'CPU Power: 8431 mW', 'GPU Power: 47 mW', 'CPU Power: 10965 mW', 'GPU Power: 56 mW', 'CPU Power: 11747 mW', 'GPU Power: 64 mW', 'CPU Power: 11754 mW', 'GPU Power: 228 mW', 'CPU Power: 3496 mW', 'GPU Power: 142 mW', 'CPU Power: 4751 mW', 'GPU Power: 196 mW', 'CPU Power: 4633 mW', 'GPU Power: 45 mW', 'CPU Power: 3415 mW', 'GPU Power: 42 mW', 'CPU Power: 4008 mW', 'GPU Power: 49 mW', 'CPU Power: 3750 mW', 'GPU Power: 52 mW', 'CPU Power: 7941 mW', 'GPU Power: 61 mW', 'CPU Power: 12084 mW', 'GPU Power: 174 mW', 'CPU Power: 5643 mW', 'GPU Power: 48 mW', 'CPU Power: 3670 mW', 'GPU Power: 40 mW', 'CPU Power: 4258 mW', 'GPU Power: 101 mW', 'CPU Power: 4751 mW', 'GPU Power: 96 mW', 'CPU Power: 5542 mW', 'GPU Power: 44 mW', 'CPU Power: 4792 mW', 'GPU Power: 177 mW', 'CPU Power: 5222 mW', 'GPU Power: 251 mW', 'CPU Power: 8254 mW', 'GPU Power: 48 mW', 'CPU Power: 8947 mW', 'GPU Power: 115 mW', 'CPU Power: 4205 mW', 'GPU Power: 59 mW', 'CPU Power: 5911 mW', 'GPU Power: 51 mW', 'CPU Power: 3686 mW', 'GPU Power: 87 mW', 'CPU Power: 4868 mW', 'GPU Power: 43 mW', 'CPU Power: 4440 mW', 'GPU Power: 44 mW', 'CPU Power: 3569 mW', 'GPU Power: 88 mW', 'CPU Power: 4603 mW', 'GPU Power: 46 mW', 'CPU Power: 4848 mW', 'GPU Power: 45 mW', 'CPU Power: 12062 mW', 'GPU Power: 52 mW', 'CPU Power: 7877 mW', 'GPU Power: 290 mW', 'CPU Power: 3331 mW', 'GPU Power: 202 mW', 'CPU Power: 6879 mW', 'GPU Power: 112 mW', 'CPU Power: 7610 mW', 'GPU Power: 67 mW', 'CPU Power: 5305 mW', 'GPU Power: 57 mW', 'CPU Power: 3780 mW', 'GPU Power: 108 mW', 'CPU Power: 4623 mW', 'GPU Power: 52 mW', 'CPU Power: 4479 mW', 'GPU Power: 96 mW', 'CPU Power: 9232 mW', 'GPU Power: 45 mW', 'CPU Power: 7390 mW', 'GPU Power: 47 mW', 'CPU Power: 5559 mW', 'GPU Power: 104 mW', 'CPU Power: 5199 mW', 'GPU Power: 231 mW', 'CPU Power: 10887 mW', 'GPU Power: 186 mW', 'CPU Power: 8437 mW', 'GPU Power: 60 mW', 'CPU Power: 4027 mW', 'GPU Power: 52 mW', 'CPU Power: 4559 mW', 'GPU Power: 52 mW', 'CPU Power: 4297 mW', 'GPU Power: 163 mW', 'CPU Power: 4257 mW', 'GPU Power: 52 mW', 'CPU Power: 5581 mW', 'GPU Power: 53 mW', 'CPU Power: 4238 mW', 'GPU Power: 43 mW', 'CPU Power: 8171 mW', 'GPU Power: 63 mW', 'CPU Power: 13231 mW', 'GPU Power: 60 mW', 'CPU Power: 9341 mW', 'GPU Power: 114 mW', 'CPU Power: 10696 mW', 'GPU Power: 230 mW', 'CPU Power: 4742 mW', 'GPU Power: 53 mW', 'CPU Power: 4398 mW', 'GPU Power: 102 mW', 'CPU Power: 4109 mW', 'GPU Power: 94 mW', 'CPU Power: 4820 mW', 'GPU Power: 57 mW', 'CPU Power: 3639 mW', 'GPU Power: 56 mW', 'CPU Power: 5382 mW', 'GPU Power: 61 mW', 'CPU Power: 3982 mW', 'GPU Power: 123 mW', 'CPU Power: 9695 mW', 'GPU Power: 52 mW', 'CPU Power: 11560 mW', 'GPU Power: 106 mW', 'CPU Power: 11062 mW', 'GPU Power: 332 mW', 'CPU Power: 9407 mW', 'GPU Power: 45 mW', 'CPU Power: 10160 mW', 'GPU Power: 49 mW', 'CPU Power: 5598 mW', 'GPU Power: 52 mW', 'CPU Power: 3734 mW', 'GPU Power: 60 mW', 'CPU Power: 5456 mW', 'GPU Power: 62 mW', 'CPU Power: 5087 mW', 'GPU Power: 55 mW', 'CPU Power: 3487 mW', 'GPU Power: 94 mW', 'CPU Power: 4949 mW', 'GPU Power: 47 mW', 'CPU Power: 5864 mW', 'GPU Power: 44 mW', 'CPU Power: 10837 mW', 'GPU Power: 89 mW', 'CPU Power: 13889 mW', 'GPU Power: 133 mW', 'CPU Power: 12082 mW', 'GPU Power: 132 mW', 'CPU Power: 10409 mW', 'GPU Power: 191 mW', 'CPU Power: 5578 mW', 'GPU Power: 45 mW', 'CPU Power: 3821 mW', 'GPU Power: 41 mW', 'CPU Power: 3840 mW', 'GPU Power: 93 mW', 'CPU Power: 5444 mW', 'GPU Power: 43 mW', 'CPU Power: 3536 mW', 'GPU Power: 96 mW', 'CPU Power: 4138 mW', 'GPU Power: 57 mW', 'CPU Power: 6481 mW', 'GPU Power: 65 mW', 'CPU Power: 8544 mW', 'GPU Power: 91 mW', 'CPU Power: 10067 mW', 'GPU Power: 89 mW', 'CPU Power: 10315 mW', 'GPU Power: 196 mW', 'CPU Power: 3657 mW', 'GPU Power: 98 mW', 'CPU Power: 4206 mW', 'GPU Power: 52 mW', 'CPU Power: 4880 mW', 'GPU Power: 62 mW', 'CPU Power: 5997 mW', 'GPU Power: 122 mW', 'CPU Power: 3918 mW', 'GPU Power: 50 mW', 'CPU Power: 4279 mW', 'GPU Power: 95 mW', 'CPU Power: 4272 mW', 'GPU Power: 44 mW', 'CPU Power: 9288 mW', 'GPU Power: 88 mW', 'CPU Power: 10147 mW', 'GPU Power: 85 mW', 'CPU Power: 9486 mW', 'GPU Power: 258 mW', 'CPU Power: 3198 mW', 'GPU Power: 43 mW', 'CPU Power: 4342 mW', 'GPU Power: 45 mW', 'CPU Power: 3454 mW', 'GPU Power: 45 mW', 'CPU Power: 4984 mW', 'GPU Power: 108 mW', 'CPU Power: 5383 mW', 'GPU Power: 0 mW', 'CPU Power: 4337 mW', 'GPU Power: 186 mW', 'CPU Power: 7115 mW', 'GPU Power: 53 mW', 'CPU Power: 9646 mW', 'GPU Power: 47 mW', 'CPU Power: 10923 mW', 'GPU Power: 52 mW', 'CPU Power: 9770 mW', 'GPU Power: 193 mW', 'CPU Power: 8635 mW', 'GPU Power: 230 mW', 'CPU Power: 6865 mW', 'GPU Power: 51 mW', 'CPU Power: 4088 mW', 'GPU Power: 64 mW', 'CPU Power: 6457 mW', 'GPU Power: 62 mW', 'CPU Power: 4385 mW', 'GPU Power: 49 mW', 'CPU Power: 4748 mW', 'GPU Power: 92 mW', 'CPU Power: 4540 mW', 'GPU Power: 88 mW', 'CPU Power: 4209 mW', 'GPU Power: 43 mW', 'CPU Power: 9113 mW', 'GPU Power: 45 mW', 'CPU Power: 12118 mW', 'GPU Power: 202 mW', 'CPU Power: 10178 mW', 'GPU Power: 179 mW']\n",
      "Total energy consumption: 20541.44 mV\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subprocess finished.Subprocess finished.\n",
      "\n",
      "Subprocess finished.\n"
     ]
    }
   ],
   "source": [
    "thread_output = {}\n",
    "duration = 0\n",
    "inference_duration = 0  # 添加用于存储推理时间的变量\n",
    "\n",
    "# 定义监控保存模型时的资源使用率的线程函数\n",
    "def monitor_resources_during_save(stop_event):\n",
    "    cpu_usage = []\n",
    "    gpu_usage = []\n",
    "\n",
    "    while not stop_event.is_set():\n",
    "        cpu_usage.append(psutil.cpu_percent(interval=0.1))\n",
    "        try:\n",
    "            gpu_output = subprocess.check_output(['nvidia-smi', \n",
    "                                                  '--query-gpu=utilization.gpu', '--format=csv,noheader,nounits'])\n",
    "            gpu_usage.append(int(gpu_output.strip()))\n",
    "        except Exception as e:\n",
    "            gpu_usage.append(None)  # 如果没有GPU或nvidia-smi命令失败，则记录None\n",
    "\n",
    "    # 保存监测结果\n",
    "    thread_output['cpu_usage'] = cpu_usage\n",
    "    thread_output['gpu_usage'] = gpu_usage\n",
    "    print(\"Resource monitoring finished.\")\n",
    "\n",
    "# 定义保存模型的函数\n",
    "def save_model(stop_event, model):\n",
    "    global duration\n",
    "    global inference_duration\n",
    "    print(\"Saving model...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    # 保存整个模型为 PyTorch 格式\n",
    "    pytorch_model = save_keras_lstm_to_pytorch(\"./models_train/imdb_lstm_model.h5\")\n",
    "    for i in range(1):\n",
    "        torch.save(pytorch_model, 'imdb_lstm_model.pth')\n",
    "\n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    print(f'Time taken to save PyTorch model: {duration:.2f} seconds')\n",
    "\n",
    "    \n",
    "    # 准备输入数据\n",
    "    X_test_sample = X_test[:10].astype(np.float32)  # 选择前10个样本进行推理\n",
    "    X_test_sample_torch = torch.from_numpy(X_test_sample).long()  # 转换为整数张量\n",
    "\n",
    "    # 测量推理时间\n",
    "    start_time_inference = time.time()\n",
    "    # 加载模型\n",
    "    pytorch_model.eval()\n",
    "\n",
    "    # 假设 X_test 是你要用于推理的输入数据\n",
    "    for i in range(1):\n",
    "        with torch.no_grad():\n",
    "            output = pytorch_model(X_test_sample_torch)\n",
    "            print(\"Inference output:\", output)\n",
    "\n",
    "    end_time_inference = time.time()\n",
    "    inference_duration = end_time_inference - start_time_inference\n",
    "    print(f'Time taken for inference on {len(X_test_sample)} samples: {inference_duration:.4f} seconds')\n",
    "\n",
    "\n",
    "    stop_event.set()  # 触发停止其他线程\n",
    "\n",
    "# 运行外部脚本并捕获输出\n",
    "def run_script(stop_event):\n",
    "    try:\n",
    "        # 使用临时文件来存储子进程输出\n",
    "        with tempfile.NamedTemporaryFile(delete=False) as tmp_file:\n",
    "            process = subprocess.Popen(['/Users/anelloyi/Desktop/run_powermetrics.sh'], \n",
    "                                       stdout=tmp_file, stderr=subprocess.STDOUT, text=True)\n",
    "            print(\"Subprocess started.\")\n",
    "            while not stop_event.is_set():\n",
    "                if process.poll() is not None:  # 检查进程是否已经结束\n",
    "                    break\n",
    "            \n",
    "            if process.poll() is None:\n",
    "                # 尝试终止进程\n",
    "                process.terminate()\n",
    "                try:\n",
    "                    process.wait(timeout=0.1)\n",
    "                except subprocess.TimeoutExpired:\n",
    "                    process.kill()  # 如果超时，则强制终止\n",
    "                    process.wait()\n",
    "        \n",
    "        # 读取临时文件的内容\n",
    "        with open(tmp_file.name, 'r') as f:\n",
    "            thread_output['powermetrics'] = f.read()\n",
    "        \n",
    "        os.remove(tmp_file.name)  # 删除临时文件\n",
    "        print(\"Subprocess finished.\")\n",
    "    except Exception as e:\n",
    "        thread_output['powermetrics'] = str(e)\n",
    "        print(\"Exception in subprocess:\", str(e))\n",
    "\n",
    "\n",
    "stop_event = threading.Event()\n",
    "model = tf.keras.models.load_model(\"./models_train/imdb_lstm_model.h5\") \n",
    "\n",
    "# 创建和启动线程\n",
    "thread1 = threading.Thread(target=save_model, args=(stop_event, model))\n",
    "thread2 = threading.Thread(target=monitor_resources_during_save, args=(stop_event,))\n",
    "thread3 = threading.Thread(target=run_script, args=(stop_event,))\n",
    "\n",
    "\n",
    "thread1.start()\n",
    "thread2.start()\n",
    "thread3.start()\n",
    "\n",
    "# 等待线程完成\n",
    "thread1.join()\n",
    "thread2.join()\n",
    "thread3.join()\n",
    "\n",
    "# 输出从线程收集的数据\n",
    "content = thread_output.get('powermetrics', 'No output captured')\n",
    "lines = content.split('\\n')\n",
    "filtered_lines = [line for line in lines if line.startswith('GPU Power:') or \n",
    "                  line.startswith('CPU Power:')]\n",
    "\n",
    "filtered_content = '\\n'.join(filtered_lines)\n",
    "\n",
    "output_file_name = './imbd_models/output_lstm/output-imdb-lstm-pth.txt'\n",
    "with open(output_file_name, 'w') as file:\n",
    "    file.write(filtered_content)\n",
    "    file.write(f'\\nTotal Duration(s): {duration:.2f}')\n",
    "    file.write(f'\\nInference Duration(s): {inference_duration:.4f}')\n",
    "print(f\"Content saved to {output_file_name}\")\n",
    "\n",
    "filtered_lines_count = len(filtered_lines)\n",
    "#print(filtered_lines_count)\n",
    "#print(filtered_lines)\n",
    "\n",
    "duration = inference_duration\n",
    "# 确保 filtered_lines_count 不为零\n",
    "if filtered_lines_count > 0:\n",
    "    # 提取每一个采样点的数字，即CPU和GPU的具体mV\n",
    "    numbers = []\n",
    "    for line in filtered_lines:\n",
    "        match = re.search(r'[\\d.]+', line)\n",
    "        if match:\n",
    "            numbers.append(float(match.group()))\n",
    "\n",
    "    delta_time = duration * 2 / filtered_lines_count\n",
    "    numbers_scaled = [num * delta_time for num in numbers]\n",
    "    total_energy_consumption = sum(numbers_scaled)\n",
    "    print(f\"Total energy consumption: {total_energy_consumption:.2f} mV\")\n",
    "else:\n",
    "    print(\"No filtered lines to process.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model...\n",
      "Subprocess started.\n",
      "Time taken to save PyTorch model: 2.04 seconds\n",
      "Time taken for inference on 1000 samples: 0.0003 seconds\n",
      "Subprocess finished.\n",
      "Resource monitoring finished.\n",
      "Content saved to ./imbd_models/output_lstm/output-imdb-lstm-pth.txt\n",
      "Total energy consumption: 1.84 mV\n"
     ]
    }
   ],
   "source": [
    "thread_output = {}\n",
    "duration = 0\n",
    "inference_duration = 0  # 添加用于存储推理时间的变量\n",
    "\n",
    "# 定义监控保存模型时的资源使用率的线程函数\n",
    "def monitor_resources_during_save(stop_event):\n",
    "    cpu_usage = []\n",
    "    gpu_usage = []\n",
    "\n",
    "    while not stop_event.is_set():\n",
    "        cpu_usage.append(psutil.cpu_percent(interval=0.1))\n",
    "        try:\n",
    "            gpu_output = subprocess.check_output(['nvidia-smi', \n",
    "                                                  '--query-gpu=utilization.gpu', '--format=csv,noheader,nounits'])\n",
    "            gpu_usage.append(int(gpu_output.strip()))\n",
    "        except Exception as e:\n",
    "            gpu_usage.append(None)  # 如果没有GPU或nvidia-smi命令失败，则记录None\n",
    "\n",
    "    # 保存监测结果\n",
    "    thread_output['cpu_usage'] = cpu_usage\n",
    "    thread_output['gpu_usage'] = gpu_usage\n",
    "    print(\"Resource monitoring finished.\")\n",
    "\n",
    "def save_model(stop_event, model):\n",
    "    global duration\n",
    "    global inference_duration\n",
    "    global X_test  # 确保引用全局变量\n",
    "\n",
    "    print(\"Saving model...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    # 假设此函数正确将Keras模型转换为PyTorch模型\n",
    "    pytorch_model = save_keras_lstm_to_pytorch(\"./models_train/imdb_lstm_model.h5\")\n",
    "    torch.save(pytorch_model, 'imdb_lstm_model.pth')\n",
    "\n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    print(f'Time taken to save PyTorch model: {duration:.2f} seconds')\n",
    "\n",
    "    # 准备输入数据\n",
    "    X_test_sample = X_test[:1000].astype(np.float32)  # 选择前10个样本进行推理\n",
    "    X_test_sample_torch = torch.from_numpy(X_test_sample).long()  # 转换为整数张量\n",
    "\n",
    "    # 测量推理时间\n",
    "    start_time_inference = time.time()\n",
    "    pytorch_model.eval()  # 设置模型为评估模式\n",
    "\n",
    "    # 推理，批处理逻辑\n",
    "    batch_size = 1  # 设置模拟的批大小为128\n",
    "    predictions = []\n",
    "    # 通过多次执行推理来模拟大批量处理\n",
    "    for _ in range(batch_size // len(X_test_sample)):  # 这里使用实际样本数量来计算需要模拟批处理的次数\n",
    "        with torch.no_grad():\n",
    "            output = pytorch_model(X_test_sample_torch)\n",
    "            predictions.append(output)\n",
    "            print(\"Inference output:\", output)\n",
    "\n",
    "    end_time_inference = time.time()\n",
    "    inference_duration = end_time_inference - start_time_inference\n",
    "    print(f'Time taken for inference on {len(X_test_sample)} samples: {inference_duration:.4f} seconds')\n",
    "\n",
    "    stop_event.set()  # 触发停止其他线程\n",
    "\n",
    "\n",
    "\n",
    "# 运行外部脚本并捕获输出\n",
    "def run_script(stop_event):\n",
    "    try:\n",
    "        # 使用临时文件来存储子进程输出\n",
    "        with tempfile.NamedTemporaryFile(delete=False) as tmp_file:\n",
    "            process = subprocess.Popen(['/Users/anelloyi/Desktop/run_powermetrics.sh'], \n",
    "                                       stdout=tmp_file, stderr=subprocess.STDOUT, text=True)\n",
    "            print(\"Subprocess started.\")\n",
    "            while not stop_event.is_set():\n",
    "                if process.poll() is not None:  # 检查进程是否已经结束\n",
    "                    break\n",
    "            \n",
    "            if process.poll() is None:\n",
    "                # 尝试终止进程\n",
    "                process.terminate()\n",
    "                try:\n",
    "                    process.wait(timeout=0.1)\n",
    "                except subprocess.TimeoutExpired:\n",
    "                    process.kill()  # 如果超时，则强制终止\n",
    "                    process.wait()\n",
    "        \n",
    "        # 读取临时文件的内容\n",
    "        with open(tmp_file.name, 'r') as f:\n",
    "            thread_output['powermetrics'] = f.read()\n",
    "        \n",
    "        os.remove(tmp_file.name)  # 删除临时文件\n",
    "        print(\"Subprocess finished.\")\n",
    "    except Exception as e:\n",
    "        thread_output['powermetrics'] = str(e)\n",
    "        print(\"Exception in subprocess:\", str(e))\n",
    "\n",
    "\n",
    "stop_event = threading.Event()\n",
    "model = tf.keras.models.load_model(\"./models_train/imdb_lstm_model.h5\") \n",
    "\n",
    "# 创建和启动线程\n",
    "thread1 = threading.Thread(target=save_model, args=(stop_event, model))\n",
    "thread2 = threading.Thread(target=monitor_resources_during_save, args=(stop_event,))\n",
    "thread3 = threading.Thread(target=run_script, args=(stop_event,))\n",
    "\n",
    "\n",
    "thread1.start()\n",
    "thread2.start()\n",
    "thread3.start()\n",
    "\n",
    "# 等待线程完成\n",
    "thread1.join()\n",
    "thread2.join()\n",
    "thread3.join()\n",
    "\n",
    "# 输出从线程收集的数据\n",
    "content = thread_output.get('powermetrics', 'No output captured')\n",
    "lines = content.split('\\n')\n",
    "filtered_lines = [line for line in lines if line.startswith('GPU Power:') or \n",
    "                  line.startswith('CPU Power:')]\n",
    "\n",
    "filtered_content = '\\n'.join(filtered_lines)\n",
    "\n",
    "output_file_name = './imbd_models/output_lstm/output-imdb-lstm-pth.txt'\n",
    "with open(output_file_name, 'w') as file:\n",
    "    file.write(filtered_content)\n",
    "    file.write(f'\\nTotal Duration(s): {duration:.2f}')\n",
    "    file.write(f'\\nInference Duration(s): {inference_duration:.4f}')\n",
    "print(f\"Content saved to {output_file_name}\")\n",
    "\n",
    "filtered_lines_count = len(filtered_lines)\n",
    "\n",
    "duration = inference_duration\n",
    "# 确保 filtered_lines_count 不为零\n",
    "if filtered_lines_count > 0:\n",
    "    # 提取每一个采样点的数字，即CPU和GPU的具体mV\n",
    "    numbers = []\n",
    "    for line in filtered_lines:\n",
    "        match = re.search(r'[\\d.]+', line)\n",
    "        if match:\n",
    "            numbers.append(float(match.group()))\n",
    "\n",
    "    delta_time = duration * 2 / filtered_lines_count\n",
    "    numbers_scaled = [num * delta_time for num in numbers]\n",
    "    total_energy_consumption = sum(numbers_scaled)\n",
    "    print(f\"Total energy consumption: {total_energy_consumption:.2f} mV\")\n",
    "else:\n",
    "    print(\"No filtered lines to process.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model...\n",
      "Subprocess started.\n",
      "Time taken to save PyTorch model: 0.95 seconds\n",
      "Time taken for inference on 10000 samples: 27.1826 seconds\n",
      "Subprocess finished.\n",
      "Resource monitoring finished.\n",
      "Content saved to ./imbd_models/output_lstm/output-imdb-lstm-pth.txt\n",
      "Total energy consumption: 193130.05 mV\n"
     ]
    }
   ],
   "source": [
    "thread_output = {}\n",
    "duration = 0\n",
    "inference_duration = 0  # 添加用于存储推理时间的变量\n",
    "\n",
    "# 定义监控保存模型时的资源使用率的线程函数\n",
    "def monitor_resources_during_save(stop_event):\n",
    "    cpu_usage = []\n",
    "    gpu_usage = []\n",
    "\n",
    "    while not stop_event.is_set():\n",
    "        cpu_usage.append(psutil.cpu_percent(interval=0.1))\n",
    "        try:\n",
    "            gpu_output = subprocess.check_output(['nvidia-smi', \n",
    "                                                  '--query-gpu=utilization.gpu', '--format=csv,noheader,nounits'])\n",
    "            gpu_usage.append(int(gpu_output.strip()))\n",
    "        except Exception as e:\n",
    "            gpu_usage.append(None)  # 如果没有GPU或nvidia-smi命令失败，则记录None\n",
    "\n",
    "    # 保存监测结果\n",
    "    thread_output['cpu_usage'] = cpu_usage\n",
    "    thread_output['gpu_usage'] = gpu_usage\n",
    "    print(\"Resource monitoring finished.\")\n",
    "\n",
    "def save_model(stop_event, model):\n",
    "    global duration\n",
    "    global inference_duration\n",
    "    global X_test  # 确保引用全局变量\n",
    "\n",
    "    print(\"Saving model...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    # 假设此函数正确将Keras模型转换为PyTorch模型\n",
    "    pytorch_model = save_keras_lstm_to_pytorch(\"./models_train/imdb_lstm_model.h5\")\n",
    "    torch.save(pytorch_model, 'imdb_lstm_model.pth')\n",
    "\n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    print(f'Time taken to save PyTorch model: {duration:.2f} seconds')\n",
    "\n",
    "    # 准备输入数据\n",
    "    X_test_sample = X_test[:10000]  # 选择前1000个样本进行推理\n",
    "    X_test_sample_torch = torch.from_numpy(X_test_sample).long()  # 正确转换为整数张量\n",
    "\n",
    "    # 测量推理时间\n",
    "    start_time_inference = time.time()\n",
    "    pytorch_model.eval()  # 设置模型为评估模式\n",
    "\n",
    "    # 推理，批处理逻辑\n",
    "    batch_size = 128  # 实际设置批量大小为128\n",
    "    predictions = []\n",
    "    for i in range(0, len(X_test_sample_torch), batch_size):\n",
    "        batch = X_test_sample_torch[i:i + batch_size]\n",
    "        with torch.no_grad():\n",
    "            output = pytorch_model(batch)\n",
    "            predictions.append(output)\n",
    "            #print(\"Inference output:\", output)\n",
    "\n",
    "    end_time_inference = time.time()\n",
    "    inference_duration = end_time_inference - start_time_inference\n",
    "    print(f'Time taken for inference on {len(X_test_sample)} samples: {inference_duration:.4f} seconds')\n",
    "\n",
    "    stop_event.set()  # 触发停止其他线程\n",
    "\n",
    "\n",
    "# 运行外部脚本并捕获输出\n",
    "def run_script(stop_event):\n",
    "    try:\n",
    "        # 使用临时文件来存储子进程输出\n",
    "        with tempfile.NamedTemporaryFile(delete=False) as tmp_file:\n",
    "            process = subprocess.Popen(['/Users/anelloyi/Desktop/run_powermetrics.sh'], \n",
    "                                       stdout=tmp_file, stderr=subprocess.STDOUT, text=True)\n",
    "            print(\"Subprocess started.\")\n",
    "            while not stop_event.is_set():\n",
    "                if process.poll() is not None:  # 检查进程是否已经结束\n",
    "                    break\n",
    "            \n",
    "            if process.poll() is None:\n",
    "                # 尝试终止进程\n",
    "                process.terminate()\n",
    "                try:\n",
    "                    process.wait(timeout=0.1)\n",
    "                except subprocess.TimeoutExpired:\n",
    "                    process.kill()  # 如果超时，则强制终止\n",
    "                    process.wait()\n",
    "        \n",
    "        # 读取临时文件的内容\n",
    "        with open(tmp_file.name, 'r') as f:\n",
    "            thread_output['powermetrics'] = f.read()\n",
    "        \n",
    "        os.remove(tmp_file.name)  # 删除临时文件\n",
    "        print(\"Subprocess finished.\")\n",
    "    except Exception as e:\n",
    "        thread_output['powermetrics'] = str(e)\n",
    "        print(\"Exception in subprocess:\", str(e))\n",
    "\n",
    "\n",
    "stop_event = threading.Event()\n",
    "model = tf.keras.models.load_model(\"./models_train/imdb_lstm_model.h5\") \n",
    "\n",
    "# 创建和启动线程\n",
    "thread1 = threading.Thread(target=save_model, args=(stop_event, model))\n",
    "thread2 = threading.Thread(target=monitor_resources_during_save, args=(stop_event,))\n",
    "thread3 = threading.Thread(target=run_script, args=(stop_event,))\n",
    "\n",
    "\n",
    "thread1.start()\n",
    "thread2.start()\n",
    "thread3.start()\n",
    "\n",
    "# 等待线程完成\n",
    "thread1.join()\n",
    "thread2.join()\n",
    "thread3.join()\n",
    "\n",
    "# 输出从线程收集的数据\n",
    "content = thread_output.get('powermetrics', 'No output captured')\n",
    "lines = content.split('\\n')\n",
    "filtered_lines = [line for line in lines if line.startswith('GPU Power:') or \n",
    "                  line.startswith('CPU Power:')]\n",
    "\n",
    "filtered_content = '\\n'.join(filtered_lines)\n",
    "\n",
    "output_file_name = './imbd_models/output_lstm/output-imdb-lstm-pth.txt'\n",
    "with open(output_file_name, 'w') as file:\n",
    "    file.write(filtered_content)\n",
    "    file.write(f'\\nTotal Duration(s): {duration:.2f}')\n",
    "    file.write(f'\\nInference Duration(s): {inference_duration:.4f}')\n",
    "print(f\"Content saved to {output_file_name}\")\n",
    "\n",
    "filtered_lines_count = len(filtered_lines)\n",
    "\n",
    "duration = inference_duration\n",
    "# 确保 filtered_lines_count 不为零\n",
    "if filtered_lines_count > 0:\n",
    "    # 提取每一个采样点的数字，即CPU和GPU的具体mV\n",
    "    numbers = []\n",
    "    for line in filtered_lines:\n",
    "        match = re.search(r'[\\d.]+', line)\n",
    "        if match:\n",
    "            numbers.append(float(match.group()))\n",
    "\n",
    "    delta_time = duration * 2 / filtered_lines_count\n",
    "    numbers_scaled = [num * delta_time for num in numbers]\n",
    "    total_energy_consumption = sum(numbers_scaled)\n",
    "    print(f\"Total energy consumption: {total_energy_consumption:.2f} mV\")\n",
    "else:\n",
    "    print(\"No filtered lines to process.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.18 ('env_name')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "37cc1b05901c8571bea9fc4b42e3f528ca4394d6e6719f1b5d30d208c4a3cfc1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
