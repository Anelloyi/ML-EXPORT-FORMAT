{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-08 18:07:13.059422: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import numpy as np\n",
    "import time\n",
    "import subprocess\n",
    "import os\n",
    "import psutil\n",
    "import tensorflow as tf\n",
    "import tempfile\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.callbacks import LambdaCallback\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "import onnxmltools\n",
    "import onnxruntime as ort\n",
    "from skl2onnx.common.data_types import FloatTensorType\n",
    "from scipy.sparse import issparse\n",
    "import hummingbird.ml\n",
    "import torch\n",
    "import pickle\n",
    "import joblib\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from joblib import dump\n",
    "import onnx\n",
    "from skl2onnx import convert_sklearn\n",
    "import onnxruntime as rt\n",
    "from scipy.sparse import hstack\n",
    "from nyoka import PMML44 as pml\n",
    "from pypmml import Model\n",
    "import joblib\n",
    "from nyoka import skl_to_pmml\n",
    "from sklearn.pipeline import Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载 IMDB 数据集\n",
    "max_features = 20000  # 使用的单词数量\n",
    "maxlen = 100  # 每条评论的最大长度\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "\n",
    "# 将序列转换为文本\n",
    "word_index = imdb.get_word_index()\n",
    "index_word = {v: k for k, v in word_index.items()}\n",
    "\n",
    "def sequences_to_texts(sequences):\n",
    "    return [' '.join([index_word.get(i - 3, '?') for i in seq]) for seq in sequences]\n",
    "\n",
    "X_train_text = sequences_to_texts(X_train)\n",
    "X_test_text = sequences_to_texts(X_test)\n",
    "\n",
    "# 使用TF-IDF向量化文本数据\n",
    "vectorizer = TfidfVectorizer(max_features=max_features)\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train_text)\n",
    "X_test_tfidf = vectorizer.transform(X_test_text)\n",
    "X_test_tfidf_1 = vectorizer.transform(X_test_text[:1])\n",
    "X_test_tfidf_200 = vectorizer.transform(X_test_text[:200])\n",
    "X_test_tfidf_500 = vectorizer.transform(X_test_text[:500])\n",
    "X_test_tfidf_1000 = vectorizer.transform(X_test_text[:1000])\n",
    "X_test_tfidf_10000 = vectorizer.transform(X_test_text[:10000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 83.77%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 创建随机森林分类器\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# 训练模型\n",
    "clf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# 预测测试集\n",
    "y_pred = clf.predict(X_test_tfidf)\n",
    "\n",
    "# 计算准确率\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Test accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to ./models_train/imdb_random_forest_model.joblib\n"
     ]
    }
   ],
   "source": [
    "# 保存训练好的模型到文件\n",
    "model_filename = ('./models_train/imdb_random_forest_model.joblib')\n",
    "dump(clf, model_filename)\n",
    "vectorizer_filename = './models_train/imdb_random_forest_tfidf_vectorizer.joblib'\n",
    "dump(vectorizer, vectorizer_filename)\n",
    "print(f\"Model saved to {model_filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMBD RF joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subprocess started.\n",
      "Time taken to save model: 0.9118 seconds\n",
      "Time taken for inference on 10000 samples: 1.7299 seconds\n",
      "Subprocess finished.\n",
      "Resource monitoring finished.\n",
      "Content saved to ./imbd_models/output_random_forest/output-imdb-random_forest-joblib.txt\n",
      "Total energy consumption: 8141.84 mV\n"
     ]
    }
   ],
   "source": [
    "# 初始化全局变量\n",
    "thread_output = {}\n",
    "duration = 0\n",
    "inference_duration = 0\n",
    "\n",
    "# 定义保存模型的函数\n",
    "def save_model(stop_event, model, X_test_tfidf):\n",
    "    global duration\n",
    "    global inference_duration\n",
    "\n",
    "    # model = joblib.load('./models_train/imdb_random_forest_model.joblib')\n",
    "\n",
    "    start_time = time.time()\n",
    "    for i in range(10):\n",
    "        joblib.dump(model, 'imdb_random_forest_model.joblib')\n",
    "    end_time = time.time()\n",
    "\n",
    "    duration = end_time - start_time\n",
    "    print(f'Time taken to save model: {duration:.4f} seconds')\n",
    "\n",
    "    X_test_tfidf_array = X_test_tfidf.astype(np.float32)\n",
    "\n",
    "    batch_size = 128\n",
    "    num_samples = X_test_tfidf_array.shape[0]\n",
    "    num_batches = (num_samples + batch_size - 1) // batch_size  # 计算批次数量\n",
    "\n",
    "    start_time_inference = time.time()\n",
    "    for _ in range(1):  # 推理循环次数为 1\n",
    "        for batch_idx in range(num_batches):\n",
    "            batch_start = batch_idx * batch_size\n",
    "            batch_end = min(batch_start + batch_size, num_samples)\n",
    "            batch = X_test_tfidf_array[batch_start:batch_end]\n",
    "            predictions = model.predict(batch)\n",
    "            # 在此处处理预测结果，例如保存或输出\n",
    "\n",
    "    end_time_inference = time.time()\n",
    "\n",
    "    inference_duration = end_time_inference - start_time_inference\n",
    "    print(f'Time taken for inference on {num_samples} samples: {inference_duration:.4f} seconds')\n",
    "    stop_event.set()  # 触发停止其他线程\n",
    "\n",
    "\n",
    "# 监控保存模型时的资源使用率的线程函数\n",
    "def monitor_resources_during_save(stop_event):\n",
    "    cpu_usage = []\n",
    "    gpu_usage = []\n",
    "\n",
    "    while not stop_event.is_set():\n",
    "        cpu_usage.append(psutil.cpu_percent(interval=0.1))\n",
    "        try:\n",
    "            gpu_output = subprocess.check_output(['nvidia-smi', '--query-gpu=utilization.gpu', '--format=csv,noheader,nounits'])\n",
    "            gpu_usage.append(int(gpu_output.strip()))\n",
    "        except Exception as e:\n",
    "            gpu_usage.append(None)  # 如果没有GPU或nvidia-smi命令失败，则记录None\n",
    "\n",
    "    # 保存监测结果\n",
    "    thread_output['cpu_usage'] = cpu_usage\n",
    "    thread_output['gpu_usage'] = gpu_usage\n",
    "    print(\"Resource monitoring finished.\")\n",
    "\n",
    "# 运行外部脚本并捕获输出\n",
    "def run_script(stop_event):\n",
    "    try:\n",
    "        with tempfile.NamedTemporaryFile(delete=False) as tmp_file:\n",
    "            process = subprocess.Popen(['/Users/anelloyi/Desktop/run_powermetrics.sh'], stdout=tmp_file, stderr=subprocess.STDOUT, text=True)\n",
    "            print(\"Subprocess started.\")\n",
    "            while not stop_event.is_set():\n",
    "                if process.poll() is not None:  # 检查进程是否已经结束\n",
    "                    break\n",
    "            \n",
    "            if process.poll() is None:\n",
    "                process.terminate()\n",
    "                try:\n",
    "                    process.wait(timeout=0.1)\n",
    "                except subprocess.TimeoutExpired:\n",
    "                    process.kill()\n",
    "                    process.wait()\n",
    "        \n",
    "        with open(tmp_file.name, 'r') as f:\n",
    "            thread_output['powermetrics'] = f.read()\n",
    "        \n",
    "        os.remove(tmp_file.name)  # 删除临时文件\n",
    "        print(\"Subprocess finished.\")\n",
    "    except Exception as e:\n",
    "        thread_output['powermetrics'] = str(e)\n",
    "        print(\"Exception in subprocess:\", str(e))\n",
    "\n",
    "# 创建和启动线程\n",
    "stop_event = threading.Event()\n",
    "\n",
    "# 加载保存的模型\n",
    "model_filename = './models_train/imdb_random_forest_model.joblib'\n",
    "clf = joblib.load(model_filename)\n",
    "\n",
    "#thread1 = threading.Thread(target=save_model, args=(stop_event, clf, X_test_tfidf))\n",
    "thread1 = threading.Thread(target=save_model, args=(stop_event, clf, X_test_tfidf_10000))\n",
    "thread2 = threading.Thread(target=monitor_resources_during_save, args=(stop_event,))\n",
    "thread3 = threading.Thread(target=run_script, args=(stop_event,))\n",
    "thread1.start()\n",
    "thread2.start()\n",
    "thread3.start()\n",
    "\n",
    "# 等待线程完成\n",
    "thread1.join()\n",
    "thread2.join()\n",
    "thread3.join()\n",
    "\n",
    "# 输出从线程收集的数据\n",
    "content = thread_output.get('powermetrics', 'No output captured')\n",
    "# 将内容按行拆分\n",
    "lines = content.split('\\n')\n",
    "# 筛选出以 \"CPU consume\" 和 \"GPU Power\" 开头的行\n",
    "filtered_lines = [line for line in lines if line.startswith('GPU Power:') or line.startswith('CPU Power:')]\n",
    "# 将筛选后的行合并为一个字符串，每行之间用换行符分隔\n",
    "filtered_content = '\\n'.join(filtered_lines)\n",
    "\n",
    "output_file_name = './imbd_models/output_random_forest/output-imdb-random_forest-joblib.txt'\n",
    "with open(output_file_name, 'w') as file:\n",
    "    file.write(filtered_content)\n",
    "    file.write(f'\\nTotal Duration(s): {duration:.2f}')\n",
    "    file.write(f'\\nInference Duration(s): {inference_duration:.4f}')\n",
    "print(f\"Content saved to {output_file_name}\")\n",
    "\n",
    "filtered_lines_count = len(filtered_lines)\n",
    "#print(filtered_lines_count)\n",
    "#print(filtered_lines)\n",
    "\n",
    "\n",
    "duration = inference_duration\n",
    "# 确保 filtered_lines_count 不为零\n",
    "if filtered_lines_count > 0:\n",
    "    # 提取每一个采样点的数字，即CPU和GPU的具体mV\n",
    "    numbers = []\n",
    "    for line in filtered_lines:\n",
    "        match = re.search(r'[\\d.]+', line)\n",
    "        if match:\n",
    "            numbers.append(float(match.group()))\n",
    "\n",
    "    delta_time = duration * 2 / filtered_lines_count\n",
    "    numbers_scaled = [num * delta_time for num in numbers]\n",
    "    total_energy_consumption = sum(numbers_scaled)\n",
    "    print(f\"Total energy consumption: {total_energy_consumption:.2f} mV\")\n",
    "else:\n",
    "    print(\"No filtered lines to process.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### imdb RF ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to save model: 0.0000 seconds\n",
      "Subprocess started.\n",
      "Time taken for inference on 10000 samples: 3.1971 seconds\n",
      "Subprocess finished.\n",
      "Resource monitoring finished.\n",
      "Content saved to ./imbd_models/output_random_forest/output-imdb-random_forest-onnx.txt\n",
      "Total energy consumption: 17335.55 mV\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import joblib\n",
    "import threading\n",
    "import psutil\n",
    "import subprocess\n",
    "import tempfile\n",
    "import os\n",
    "import re\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from skl2onnx import convert_sklearn\n",
    "from skl2onnx.common.data_types import FloatTensorType\n",
    "import onnxruntime as rt\n",
    "from tensorflow.keras.datasets import imdb\n",
    "\n",
    "# 初始化全局变量\n",
    "thread_output = {}\n",
    "duration = 0\n",
    "inference_duration = 0\n",
    "\n",
    "# 定义保存模型的函数\n",
    "def save_model(stop_event, model, X_test_tfidf):\n",
    "    global duration\n",
    "    global inference_duration\n",
    "\n",
    "    # 将random_forest模型转换为onnx模型\n",
    "    start_time = time.time()\n",
    "    for i in range(5):\n",
    "        # 转换为ONNX格式\n",
    "        initial_type = [('float_input', FloatTensorType([None, max_features]))]\n",
    "        #onnx_model = convert_sklearn(model, initial_types=initial_type, target_opset=9)\n",
    "\n",
    "        # 保存ONNX模型\n",
    "        onnx_model_filename = 'imdb_random_forest_model.onnx'\n",
    "        # with open(onnx_model_filename, \"wb\") as f:\n",
    "        #     f.write(onnx_model.SerializeToString())\n",
    "        # print(f\"ONNX model saved to {onnx_model_filename}\")\n",
    "    end_time = time.time()\n",
    "\n",
    "    duration = end_time - start_time\n",
    "    print(f'Time taken to save model: {duration:.4f} seconds')\n",
    "\n",
    "    start_time_inference = time.time()\n",
    "    # 准备输入数据\n",
    "    # 如果特征数量少于max_features，则补充空的特征\n",
    "    if X_test_tfidf.shape[1] < max_features:\n",
    "        padding = np.zeros((X_test_tfidf.shape[0], max_features - X_test_tfidf.shape[1]))\n",
    "        X_test_tfidf = hstack([X_test_tfidf, padding])\n",
    "    X_test_tfidf_array = X_test_tfidf.toarray().astype(np.float32)\n",
    "\n",
    "    # 使用ONNX runtime进行推理\n",
    "    sess = rt.InferenceSession(onnx_model_filename)\n",
    "    # sess = rt.InferenceSession(onnx_model.SerializeToString())\n",
    "    input_name = sess.get_inputs()[0].name\n",
    "    label_name = sess.get_outputs()[0].name\n",
    "\n",
    "    # 批量推理部分\n",
    "    batch_size = 128\n",
    "    num_samples = X_test_tfidf_array.shape[0]\n",
    "    num_batches = (num_samples + batch_size - 1) // batch_size  # 计算批次数量\n",
    "\n",
    "    for _ in range(1):  # 推理循环次数为 1\n",
    "        for batch_idx in range(num_batches):\n",
    "            batch_start = batch_idx * batch_size\n",
    "            batch_end = min(batch_start + batch_size, num_samples)\n",
    "            batch = X_test_tfidf_array[batch_start:batch_end]\n",
    "            predictions = sess.run([label_name], {input_name: batch})[0]\n",
    "            # 在此处处理预测结果，例如保存或输出\n",
    "\n",
    "    end_time_inference = time.time()\n",
    "\n",
    "    inference_duration = end_time_inference - start_time_inference\n",
    "    print(f'Time taken for inference on {num_samples} samples: {inference_duration:.4f} seconds')\n",
    "\n",
    "    stop_event.set()  # 触发停止其他线程\n",
    "\n",
    "# 监控保存模型时的资源使用率的线程函数\n",
    "def monitor_resources_during_save(stop_event):\n",
    "    cpu_usage = []\n",
    "    gpu_usage = []\n",
    "\n",
    "    while not stop_event.is_set():\n",
    "        cpu_usage.append(psutil.cpu_percent(interval=0.1))\n",
    "        try:\n",
    "            gpu_output = subprocess.check_output(['nvidia-smi', '--query-gpu=utilization.gpu', '--format=csv,noheader,nounits'])\n",
    "            gpu_usage.append(int(gpu_output.strip()))\n",
    "        except Exception as e:\n",
    "            gpu_usage.append(None)  # 如果没有GPU或nvidia-smi命令失败，则记录None\n",
    "\n",
    "    # 保存监测结果\n",
    "    thread_output['cpu_usage'] = cpu_usage\n",
    "    thread_output['gpu_usage'] = gpu_usage\n",
    "    print(\"Resource monitoring finished.\")\n",
    "\n",
    "# 运行外部脚本并捕获输出\n",
    "def run_script(stop_event):\n",
    "    try:\n",
    "        with tempfile.NamedTemporaryFile(delete=False) as tmp_file:\n",
    "            process = subprocess.Popen(['/Users/anelloyi/Desktop/run_powermetrics.sh'], stdout=tmp_file, stderr=subprocess.STDOUT, text=True)\n",
    "            print(\"Subprocess started.\")\n",
    "            while not stop_event.is_set():\n",
    "                if process.poll() is not None:  # 检查进程是否已经结束\n",
    "                    break\n",
    "            \n",
    "            if process.poll() is None:\n",
    "                process.terminate()\n",
    "                try:\n",
    "                    process.wait(timeout=0.1)\n",
    "                except subprocess.TimeoutExpired:\n",
    "                    process.kill()\n",
    "                    process.wait()\n",
    "        \n",
    "        with open(tmp_file.name, 'r') as f:\n",
    "            thread_output['powermetrics'] = f.read()\n",
    "        \n",
    "        os.remove(tmp_file.name)  # 删除临时文件\n",
    "        print(\"Subprocess finished.\")\n",
    "    except Exception as e:\n",
    "        thread_output['powermetrics'] = str(e)\n",
    "        print(\"Exception in subprocess:\", str(e))\n",
    "\n",
    "# 创建和启动线程\n",
    "stop_event = threading.Event()\n",
    "\n",
    "\n",
    "# 加载保存的模型\n",
    "model_filename = './models_train/imdb_random_forest_model.joblib'\n",
    "clf = joblib.load(model_filename)\n",
    "\n",
    "# 创建并启动线程\n",
    "#thread1 = threading.Thread(target=save_model, args=(stop_event, clf, X_test_tfidf))\n",
    "thread1 = threading.Thread(target=save_model, args=(stop_event, clf, X_test_tfidf_10000))\n",
    "thread2 = threading.Thread(target=monitor_resources_during_save, args=(stop_event,))\n",
    "thread3 = threading.Thread(target=run_script, args=(stop_event,))\n",
    "thread1.start()\n",
    "thread2.start()\n",
    "thread3.start()\n",
    "\n",
    "# 等待线程完成\n",
    "thread1.join()\n",
    "thread2.join()\n",
    "thread3.join()\n",
    "\n",
    "# 输出从线程收集的数据\n",
    "content = thread_output.get('powermetrics', 'No output captured')\n",
    "# 将内容按行拆分\n",
    "lines = content.split('\\n')\n",
    "# 筛选出以 \"CPU consume\" 和 \"GPU Power\" 开头的行\n",
    "filtered_lines = [line for line in lines if line.startswith('GPU Power:') or line.startswith('CPU Power:')]\n",
    "# 将筛选后的行合并为一个字符串，每行之间用换行符分隔\n",
    "filtered_content = '\\n'.join(filtered_lines)\n",
    "\n",
    "output_file_name = './imbd_models/output_random_forest/output-imdb-random_forest-onnx.txt'\n",
    "with open(output_file_name, 'w') as file:\n",
    "    file.write(filtered_content)\n",
    "    file.write(f'\\nTotal Duration(s): {duration:.2f}')\n",
    "    file.write(f'\\nInference Duration(s): {inference_duration:.4f}')\n",
    "print(f\"Content saved to {output_file_name}\")\n",
    "\n",
    "filtered_lines_count = len(filtered_lines)\n",
    "#print(filtered_lines_count)\n",
    "#print(filtered_lines)\n",
    "\n",
    "duration = inference_duration\n",
    "\n",
    "# 确保 filtered_lines_count 不为零\n",
    "if filtered_lines_count > 0:\n",
    "    # 提取每一个采样点的数字，即CPU和GPU的具体mV\n",
    "    numbers = []\n",
    "    for line in filtered_lines:\n",
    "        match = re.search(r'[\\d.]+', line)\n",
    "        if match:\n",
    "            numbers.append(float(match.group()))\n",
    "\n",
    "    delta_time = duration * 2 / filtered_lines_count\n",
    "    numbers_scaled = [num * delta_time for num in numbers]\n",
    "    total_energy_consumption = sum(numbers_scaled)\n",
    "    print(f\"Total energy consumption: {total_energy_consumption:.2f} mV\")\n",
    "else:\n",
    "    print(\"No filtered lines to process.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RF IMDB pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subprocess started.\n",
      "Time taken to save model: 18.4107 seconds\n",
      "Time taken for inference on 10000 samples: 24.2019 seconds\n",
      "Resource monitoring finished.\n",
      "Subprocess finished.\n",
      "Content saved to ./imbd_models/output_random_forest/output-imdb-random_forest-pth.txt\n",
      "Total energy consumption: 111257.33 mV\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import time\n",
    "import subprocess\n",
    "import tempfile\n",
    "import os\n",
    "import re\n",
    "import psutil\n",
    "import numpy as np\n",
    "import joblib\n",
    "import torch\n",
    "from hummingbird.ml import convert\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Global variables\n",
    "thread_output = {}\n",
    "duration = 0\n",
    "inference_duration = 0\n",
    "\n",
    "# Function to save the model and perform inference\n",
    "def save_model(stop_event, model, X_test_tfidf):\n",
    "    global duration\n",
    "    global inference_duration\n",
    "\n",
    "    start_time = time.time()\n",
    "    # Convert the sklearn model to a PyTorch model using Hummingbird\n",
    "    pytorch_model = convert(model, 'pytorch')\n",
    "    torch.save(pytorch_model, './imdb_random_forest_model.pth')  # Save the model\n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    print(f'Time taken to save model: {duration:.4f} seconds')\n",
    "\n",
    "    # Perform inference\n",
    "    start_time_inference = time.time()\n",
    "    X_test_torch = torch.from_numpy(X_test_tfidf.toarray().astype(np.float32))  # Convert to tensor\n",
    "    batch_size = 128\n",
    "    num_samples = X_test_torch.shape[0]\n",
    "    num_batches = (num_samples + batch_size - 1) // batch_size\n",
    "\n",
    "    # No need to switch to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for batch_idx in range(num_batches):\n",
    "            batch_start = batch_idx * batch_size\n",
    "            batch_end = min(batch_start + batch_size, num_samples)\n",
    "            batch = X_test_torch[batch_start:batch_end]\n",
    "            predictions = pytorch_model.predict(batch)  # Use predict directly\n",
    "\n",
    "    end_time_inference = time.time()\n",
    "    inference_duration = end_time_inference - start_time_inference\n",
    "    print(f'Time taken for inference on {num_samples} samples: {inference_duration:.4f} seconds')\n",
    "    stop_event.set()\n",
    "\n",
    "# Function to monitor resource usage\n",
    "def monitor_resources_during_save(stop_event):\n",
    "    cpu_usage = []\n",
    "    gpu_usage = []\n",
    "    while not stop_event.is_set():\n",
    "        cpu_usage.append(psutil.cpu_percent(interval=0.1))\n",
    "        try:\n",
    "            gpu_output = subprocess.check_output(['nvidia-smi', '--query-gpu=utilization.gpu', '--format=csv,noheader,nounits'])\n",
    "            gpu_usage.append(int(gpu_output.strip()))\n",
    "        except Exception as e:\n",
    "            gpu_usage.append(None)\n",
    "\n",
    "    thread_output['cpu_usage'] = cpu_usage\n",
    "    thread_output['gpu_usage'] = gpu_usage\n",
    "    print(\"Resource monitoring finished.\")\n",
    "\n",
    "# Function to run external script and capture output\n",
    "def run_script(stop_event):\n",
    "    try:\n",
    "        with tempfile.NamedTemporaryFile(delete=False) as tmp_file:\n",
    "            process = subprocess.Popen(['/Users/anelloyi/Desktop/run_powermetrics.sh'], stdout=tmp_file, stderr=subprocess.STDOUT, text=True)\n",
    "            print(\"Subprocess started.\")\n",
    "            while not stop_event.is_set():\n",
    "                if process.poll() is not None:\n",
    "                    break\n",
    "            if process.poll() is None:\n",
    "                process.terminate()\n",
    "                try:\n",
    "                    process.wait(timeout=0.1)\n",
    "                except subprocess.TimeoutExpired:\n",
    "                    process.kill()\n",
    "                    process.wait()\n",
    "\n",
    "        with open(tmp_file.name, 'r') as f:\n",
    "            thread_output['powermetrics'] = f.read()\n",
    "        os.remove(tmp_file.name)\n",
    "        print(\"Subprocess finished.\")\n",
    "    except Exception as e:\n",
    "        thread_output['powermetrics'] = str(e)\n",
    "        print(\"Exception in subprocess:\", str(e))\n",
    "\n",
    "# Load the model and data\n",
    "model_filename = './models_train/imdb_random_forest_model.joblib'\n",
    "clf = joblib.load(model_filename)\n",
    "\n",
    "# Start threads\n",
    "stop_event = threading.Event()\n",
    "\n",
    "\n",
    "thread1 = threading.Thread(target=save_model, args=(stop_event, clf, X_test_tfidf_10000))\n",
    "#thread1 = threading.Thread(target=save_model, args=(stop_event, clf, X_test_tfidf_1000))\n",
    "thread2 = threading.Thread(target=monitor_resources_during_save, args=(stop_event,))\n",
    "thread3 = threading.Thread(target=run_script, args=(stop_event,))\n",
    "thread1.start()\n",
    "thread2.start()\n",
    "thread3.start()\n",
    "\n",
    "# Wait for threads to complete\n",
    "thread1.join()\n",
    "thread2.join()\n",
    "thread3.join()\n",
    "\n",
    "# 输出从线程收集的数据\n",
    "content = thread_output.get('powermetrics', 'No output captured')\n",
    "# 将内容按行拆分\n",
    "lines = content.split('\\n')\n",
    "# 筛选出以 \"CPU consume\" 和 \"GPU Power\" 开头的行\n",
    "filtered_lines = [line for line in lines if line.startswith('GPU Power:') or line.startswith('CPU Power:')]\n",
    "# 将筛选后的行合并为一个字符串，每行之间用换行符分隔\n",
    "filtered_content = '\\n'.join(filtered_lines)\n",
    "\n",
    "output_file_name = './imbd_models/output_random_forest/output-imdb-random_forest-pth.txt'\n",
    "with open(output_file_name, 'w') as file:\n",
    "    file.write(filtered_content)\n",
    "    file.write(f'\\nTotal Duration(s): {duration:.2f}')\n",
    "    file.write(f'\\nInference Duration(s): {inference_duration:.4f}')\n",
    "print(f\"Content saved to {output_file_name}\")\n",
    "\n",
    "filtered_lines_count = len(filtered_lines)\n",
    "#print(filtered_lines_count)\n",
    "#print(filtered_lines)\n",
    "\n",
    "duration = inference_duration\n",
    "\n",
    "# 确保 filtered_lines_count 不为零\n",
    "if filtered_lines_count > 0:\n",
    "    # 提取每一个采样点的数字，即CPU和GPU的具体mV\n",
    "    numbers = []\n",
    "    for line in filtered_lines:\n",
    "        match = re.search(r'[\\d.]+', line)\n",
    "        if match:\n",
    "            numbers.append(float(match.group()))\n",
    "\n",
    "    delta_time = duration * 2 / filtered_lines_count\n",
    "    numbers_scaled = [num * delta_time for num in numbers]\n",
    "    total_energy_consumption = sum(numbers_scaled)\n",
    "    print(f\"Total energy consumption: {total_energy_consumption:.2f} mV\")\n",
    "else:\n",
    "    print(\"No filtered lines to process.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### imdb RF PMML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nyoka import PMML44 as pml\n",
    "from pypmml import Model\n",
    "import joblib\n",
    "from nyoka import skl_to_pmml\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn2pmml import PMMLPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subprocess started.\n",
      "Model exported to imdb_random_forest_model.pmml\n",
      "Time taken to save model: 51.3694 seconds\n",
      "Time taken for inference on 1000 samples: 281.7404 seconds\n",
      "Resource monitoring finished.\n",
      "Subprocess finished.\n",
      "Content saved to ./mnist_models/output_rf/output-imdb-rf-pmml.txt\n",
      "Total energy consumption: 1577398.50 mV\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "import time\n",
    "import numpy as np\n",
    "import joblib\n",
    "import psutil\n",
    "import subprocess\n",
    "import tempfile\n",
    "import os\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn2pmml import sklearn2pmml\n",
    "from pypmml import Model\n",
    "\n",
    "# Initialize global variables\n",
    "thread_output = {}\n",
    "duration = 0\n",
    "inference_duration = 0\n",
    "\n",
    "def save_model(stop_event, model, X_test_2d):\n",
    "    global duration\n",
    "    global inference_duration\n",
    "\n",
    "    start_time = time.time()\n",
    "    # Create a PMMLPipeline with the classifier\n",
    "    pipeline = PMMLPipeline([(\"classifier\", model)])\n",
    "    \n",
    "    # Export to PMML\n",
    "    pmml_filename = 'imdb_random_forest_model.pmml'\n",
    "    sklearn2pmml(pipeline, pmml_filename)\n",
    "    print(f\"Model exported to {pmml_filename}\")\n",
    "    \n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    print(f'Time taken to save model: {duration:.4f} seconds')\n",
    "\n",
    "    # Load PMML model\n",
    "    model = Model.load(pmml_filename)\n",
    "\n",
    "    # Prepare input data\n",
    "    X_test_2d_array = X_test_2d.astype(np.float32)\n",
    "\n",
    "    # Batch inference\n",
    "    batch_size = 128\n",
    "    num_samples = len(X_test_2d_array)\n",
    "    num_batches = (num_samples + batch_size - 1) // batch_size\n",
    "\n",
    "    start_time_inference = time.time()\n",
    "    predictions = []\n",
    "    for _ in range(1):  # Inference loop count as 1\n",
    "        for batch_idx in range(num_batches):\n",
    "            batch_start = batch_idx * batch_size\n",
    "            batch_end = min(batch_start + batch_size, num_samples)\n",
    "            batch = X_test_2d_array[batch_start:batch_end]\n",
    "            predictions.extend(model.predict(batch))\n",
    "\n",
    "    end_time_inference = time.time()\n",
    "    inference_duration = end_time_inference - start_time_inference\n",
    "    print(f'Time taken for inference on {num_samples} samples: {inference_duration:.4f} seconds')\n",
    "\n",
    "    stop_event.set()  # Signal to stop other threads\n",
    "\n",
    "def monitor_resources_during_save(stop_event):\n",
    "    cpu_usage = []\n",
    "    gpu_usage = []\n",
    "    while not stop_event.is_set():\n",
    "        cpu_usage.append(psutil.cpu_percent(interval=0.1))\n",
    "        try:\n",
    "            gpu_output = subprocess.check_output(['nvidia-smi', '--query-gpu=utilization.gpu', '--format=csv,noheader,nounits'])\n",
    "            gpu_usage.append(int(gpu_output.strip()))\n",
    "        except Exception as e:\n",
    "            gpu_usage.append(None)\n",
    "\n",
    "    thread_output['cpu_usage'] = cpu_usage\n",
    "    thread_output['gpu_usage'] = gpu_usage\n",
    "    print(\"Resource monitoring finished.\")\n",
    "\n",
    "def run_script(stop_event):\n",
    "    try:\n",
    "        with tempfile.NamedTemporaryFile(delete=False) as tmp_file:\n",
    "            process = subprocess.Popen(['/Users/anelloyi/Desktop/run_powermetrics.sh'], stdout=tmp_file, stderr=subprocess.STDOUT, text=True)\n",
    "            print(\"Subprocess started.\")\n",
    "            while not stop_event.is_set():\n",
    "                if process.poll() is not None:\n",
    "                    break\n",
    "            if process.poll() is None:\n",
    "                process.terminate()\n",
    "                process.wait(timeout=0.1)\n",
    "\n",
    "        with open(tmp_file.name, 'r') as f:\n",
    "            thread_output['powermetrics'] = f.read()\n",
    "\n",
    "        os.remove(tmp_file.name)\n",
    "        print(\"Subprocess finished.\")\n",
    "    except Exception as e:\n",
    "        thread_output['powermetrics'] = str(e)\n",
    "        print(\"Exception in subprocess:\", str(e))\n",
    "\n",
    "# Create and start threads\n",
    "stop_event = threading.Event()\n",
    "loaded_model = joblib.load('./models_train/imdb_random_forest_model.joblib')\n",
    "X_test_2d = np.random.rand(1000, 28*28)  # Example data\n",
    "\n",
    "thread1 = threading.Thread(target=save_model, args=(stop_event, loaded_model, X_test_2d))\n",
    "thread2 = threading.Thread(target=monitor_resources_during_save, args=(stop_event,))\n",
    "thread3 = threading.Thread(target=run_script, args=(stop_event,))\n",
    "thread1.start()\n",
    "thread2.start()\n",
    "thread3.start()\n",
    "\n",
    "# Wait for threads to complete\n",
    "thread1.join()\n",
    "thread2.join()\n",
    "thread3.join()\n",
    "\n",
    "# Output collected data\n",
    "content = thread_output.get('powermetrics', 'No output captured')\n",
    "lines = content.split('\\n')\n",
    "filtered_lines = [line for line in lines if line.startswith('GPU Power:') or line.startswith('CPU Power:')]\n",
    "filtered_content = '\\n'.join(filtered_lines)\n",
    "\n",
    "output_file_name = './mnist_models/output_rf/output-imdb-rf-pmml.txt'\n",
    "with open(output_file_name, 'w') as file:\n",
    "    file.write(filtered_content)\n",
    "    file.write(f'\\nTotal Duration(s): {duration:.2f}')\n",
    "    file.write(f'\\nInference Duration(s): {inference_duration:.4f}')\n",
    "print(f\"Content saved to {output_file_name}\")\n",
    "\n",
    "filtered_lines_count = len(filtered_lines)\n",
    "\n",
    "duration = inference_duration\n",
    "\n",
    "# 确保 filtered_lines_count 不为零\n",
    "if filtered_lines_count > 0:\n",
    "    # 提取每一个采样点的数字，即CPU和GPU的具体mV\n",
    "    numbers = []\n",
    "    for line in filtered_lines:\n",
    "        match = re.search(r'[\\d.]+', line)\n",
    "        if match:\n",
    "            numbers.append(float(match.group()))\n",
    "\n",
    "    delta_time = duration * 2 / filtered_lines_count\n",
    "    numbers_scaled = [num * delta_time for num in numbers]\n",
    "    total_energy_consumption = sum(numbers_scaled)\n",
    "    print(f\"Total energy consumption: {total_energy_consumption:.2f} mV\")\n",
    "else:\n",
    "    print(\"No filtered lines to process.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.18 ('env_name')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "37cc1b05901c8571bea9fc4b42e3f528ca4394d6e6719f1b5d30d208c4a3cfc1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
